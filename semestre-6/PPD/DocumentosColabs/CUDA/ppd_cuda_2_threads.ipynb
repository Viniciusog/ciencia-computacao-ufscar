{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF76EjM3ZQ7r"
      },
      "source": [
        "# PPD: Programação com CUDA\n",
        "\n",
        "Hélio - DC/UFSCar - 2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJBnbvxIMzze"
      },
      "source": [
        "https://developer.nvidia.com/blog/easy-introduction-cuda-c-and-c/\n",
        "\n",
        "https://developer.nvidia.com/blog/even-easier-introduction-cuda/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi -L\n",
        "# ! echo && nvidia-smi"
      ],
      "metadata": {
        "id": "XweVoN6o_dLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uwygBAU_IsI"
      },
      "source": [
        "# Blocos e Threads\n",
        "\n",
        "A execução de instruções em GPU com CUDA é feita no modelo **SIMT**, criando *threads* a partir de funções definidas como *kernels*.  O **número** e a **organização das *threads*** que vão executar instâncias do *kernel* simultaneamnte são determinados como parâmetros na ativação do *kernel*.\n",
        "\n",
        "* As *threads* são organizadas em **blocos**. A estrutura dos blocos é definida por uma variável do tipo ***dim3***, que possibilita arranjos em 1, 2 ou 3 dimensões.\n",
        "\n",
        "* Além disso, é possível definir uma grade (***grid***) de blocos, que podem ser organizados em estruturas lógicas de 1, 2 ou 3 dimensões.\n",
        "\n",
        "Assim, há 2 parâmetros que são passados na ativação de um *kernel*. O primeiro indica a estruturação do ***grid***, ou seja, quantos blocos de *threads* serão usados e como esses blocos estarão organizados logicamente. O segundo parâmetro indica como as *threads* estarão organizadas dentro de cada bloco.\n",
        "\n",
        "```c\n",
        "kernel_function << grid, bloco >>\n",
        "```\n",
        "ou seja,\n",
        "```c\n",
        "kernel_function << #blocos, #threads_por_bloco >>\n",
        "```\n",
        "\n",
        "Durante a execução das *threads*, há funções que permitem a cada uma saber quais são as dimensões do bloco a que essa *thread* pertence e qual é o posicionamento desta *thread* dentro deste bloco, nas dimensões X, Y e Z.\n",
        "\n",
        "Também é possível saber qual é a organização do *grid* e qual é o posicionamento do bloco desta *thread* em relação ao *grid*.\n",
        "\n",
        "\n",
        "* ***Thread*** dentro do bloco: **threadIdx.x**, **threadIdx.y**, **threadIdx.z**\n",
        "\n",
        "* **Bloco** dentro do *grid*: **blockIdx.x**, **blockIdx.y**, **blockIdx.z**\n",
        "\n",
        "* **Dimensões** do bloco: **blockDim.x**, **blockDim.y**, **blockDim.z**\n",
        "\n",
        "* **Dimensões** do grid em blocos: **gridDim.x**, **gridDim.y**, **gridDim.z**\n",
        "\n",
        "<br>\n",
        "\n",
        "Há 2 fatores associados a essa forma de organização lógica das *threads*. Um aspecto é poder mapear o identificador da *thread* com a organização de estruturas de dados multidimensionais, como vetores, matrizes e volumes.\n",
        "\n",
        "Outro aspecto da organização das *threads* em blocos é o mapeamento dos blocos aos elementos de processamento das GPUs.\n",
        "\n",
        "<br>\n",
        "\n",
        "No exemplo a seguir, vê-se a ativação do *kernel* e a configuração do *grid* e das *threads* dentro dos blocos.\n",
        "\n",
        "As variáveis **threadIdx** e **blockIdx** indicam a posição lógica de cada *thread* na estrutura.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FLqgPrdjgch",
        "outputId": "b5b6ed09-cded-4b4f-cdc0-d3f8727f0c95"
      },
      "source": [
        "%%writefile id.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "/*\n",
        "typedef struct {\n",
        "   unsigned int x;\n",
        "   unsigned int y;\n",
        "   unsigned int z;\n",
        "} dim3;\n",
        "*/\n",
        "\n",
        "\n",
        "__global__\n",
        "void checkIndex(void)\n",
        "{\n",
        "  printf(\"threadIdx:(%d, %d, %d) blockIdx:(%d, %d, %d) blockDim:(%d, %d, %d) \"\n",
        "      \"gridDim:(%d, %d, %d)\\n\", threadIdx.x, threadIdx.y, threadIdx.z,\n",
        "      blockIdx.x, blockIdx.y, blockIdx.z, blockDim.x, blockDim.y,\n",
        "      blockDim.z, gridDim.x, gridDim.y, gridDim.z);\n",
        "}\n",
        "\n",
        "int\n",
        "main(int argc, char **argv)\n",
        "{\n",
        "  dim3 grid = {1,1,3};    // organização dos blocos de threads\n",
        "  dim3 block = {2,2,2};   // organização das threads em cada bloco\n",
        "\n",
        "  checkIndex <<< grid, block >>>();\n",
        "\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  return (0);\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing id.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQ6NLwlIjoTF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ebba741-146d-48f4-dbfe-a79d4fbc1da6"
      },
      "source": [
        "# ! if [ ! id -nt id.c ]; then nvcc id.cu -o id  -Wno-deprecated-gpu-targets -gencode=arch=compute_37,code=sm_37; fi\n",
        "! nvcc id.c -o id -O3\n",
        "! ./id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "threadIdx:(0, 0, 0) blockIdx:(0, 0, 0) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(1, 0, 0) blockIdx:(0, 0, 0) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(0, 1, 0) blockIdx:(0, 0, 0) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(1, 1, 0) blockIdx:(0, 0, 0) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(0, 0, 1) blockIdx:(0, 0, 0) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(1, 0, 1) blockIdx:(0, 0, 0) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(0, 1, 1) blockIdx:(0, 0, 0) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(1, 1, 1) blockIdx:(0, 0, 0) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(0, 0, 0) blockIdx:(0, 0, 1) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(1, 0, 0) blockIdx:(0, 0, 1) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(0, 1, 0) blockIdx:(0, 0, 1) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(1, 1, 0) blockIdx:(0, 0, 1) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(0, 0, 1) blockIdx:(0, 0, 1) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(1, 0, 1) blockIdx:(0, 0, 1) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(0, 1, 1) blockIdx:(0, 0, 1) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(1, 1, 1) blockIdx:(0, 0, 1) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(0, 0, 0) blockIdx:(0, 0, 2) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(1, 0, 0) blockIdx:(0, 0, 2) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(0, 1, 0) blockIdx:(0, 0, 2) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(1, 1, 0) blockIdx:(0, 0, 2) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(0, 0, 1) blockIdx:(0, 0, 2) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(1, 0, 1) blockIdx:(0, 0, 2) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(0, 1, 1) blockIdx:(0, 0, 2) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(1, 1, 1) blockIdx:(0, 0, 2) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgv3mSTzv7Bf"
      },
      "source": [
        "# Escolhendo as dimensões do kernel\n",
        "\n",
        "Devido à forma em que é organizada a arquitetura da GPU, há limitações sobre as dimensões máximas de *grids* e de blocos, como se vê nas informações obtidas com [cudaGetDeviceProperties](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html).\n",
        "\n",
        "```c\n",
        "__host__ cudaError_t cudaGetDeviceProperties ( cudaDeviceProp* prop, int  device )\n",
        "\n",
        "// Returns information about the compute-device.\n",
        "\n",
        "// prop: Properties for the specified device\n",
        "// device: Device number to get properties for\n",
        "\n",
        "// Returns in *prop the properties of device dev. The cudaDeviceProp structure is defined as:\n",
        "\n",
        "struct cudaDeviceProp {\n",
        "  char name[256];\n",
        "  cudaUUID_t uuid;\n",
        "  ...\n",
        "  int cudaDeviceProp::maxThreadsPerBlock; // Maximum number of threads per block\n",
        "  int cudaDeviceProp::maxThreadsDim[3]; // Maximum size of each dimension of a block\n",
        "  int cudaDeviceProp::maxGridSize[3];  // Maximum size of each dimension of a grid\n",
        "  ...\n",
        "  int cudaDeviceProp::maxThreadsPerMultiProcessor; // Maximum resident threads per multiprocessor\n",
        "  ...\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTketNVRwXlH",
        "outputId": "ea6d4a94-94e5-420a-dd65-a3604c7962b7"
      },
      "source": [
        "%%writefile max-dims.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "\tcudaSetDevice(0);\n",
        "\tcudaDeviceProp prop;\n",
        "\tcudaGetDeviceProperties(&prop,0);\n",
        "\n",
        " \tprintf(\"Modelo do Device: %s\\n\",prop.name);\n",
        "  printf(\"Número de SMs: %d\\n\",prop.multiProcessorCount);\n",
        "  printf(\"\\nmaxGridSize: %d,%d,%d\\n\",\n",
        "\t \t\t\tprop.maxGridSize[0], prop.maxGridSize[1],prop.maxGridSize[2]);\n",
        "\tprintf(\"maxThreadsDim: %d,%d,%d\\n\",\n",
        "\t \t\t\tprop.maxThreadsDim[0], prop.maxThreadsDim[1],prop.maxThreadsDim[2]);\n",
        "  printf(\"maxThreadsPerBlock: %d\\n\",prop.maxThreadsPerBlock);\n",
        "  printf(\"maxThreadsPerMultiProcessor: %d\\n\",prop.maxThreadsPerMultiProcessor);\n",
        "\n",
        "\treturn 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing max-dims.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JltG8EGZwlO0",
        "outputId": "928b9a4b-21c0-4928-e616-aaed38f96e3b"
      },
      "source": [
        "! nvcc max-dims.cu -o max-dims && ./max-dims"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo do Device: Tesla T4\n",
            "Número de SMs: 40\n",
            "\n",
            "maxGridSize: 2147483647,65535,65535\n",
            "maxThreadsDim: 1024,1024,64\n",
            "maxThreadsPerBlock: 1024\n",
            "maxThreadsPerMultiProcessor: 1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W6wFw5wy9P9"
      },
      "source": [
        "\n",
        "Como se vê pelos dados obtidos com a chamada *cudaGetDeviceProperties*, as dimensões **máximas de cada bloco** estão limitadas a \\< 1024, 1024, 64 \\>. Porém, a soma do número de *threads* nessas 3 dimensões não pode ultrapassar **1024**, como retornado em **maxThredsPerBlock**.\n",
        "\n",
        "Ou seja, pode-se escolher qualquer valor entre 1..1024 para a primeira dimensão do bloco, qualquer valor entre 1..1024 para a segunda dimensão, e qualqer valor entre 1..64 para a 3a dimensão, desde que **d1\\*d2\\*d3 <= 1024**.\n",
        "\n",
        "Já o número máximo de blocos é MUITO grande, com dimensões máximas \\< 2147483647, 65535, 65535 \\>.\n",
        "\n",
        "Deste modo, o número total de *threads* que uma aplicação pode ter é 2147483647 \\* 65535 \\* 65535 \\* 1024.\n",
        "\n",
        "São muitas *threads*, cabendo ao programador escolher a melhor organização lógica para isso.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Escolhendo a organização das *threads*\n",
        "\n",
        "Como visto, é possível utilizar difentes formas de organizar um número fixo de *threads* em grades com diferentes números de blocos, sendo que cada bloco pode ter diferentes números de *threads*, organizadas em 1, 2 ou 3 dimensões.\n",
        "\n",
        "O ponto aqui é considerarmos que podemos definir uma *thread* para o cálculo de cada elemento de uma estrutura vetorial a ser manipulada. Independentemente do número de *cores* (núcleos de processamento da GPU), podemos pensar como se todas as *threads* fossem executadas ao mesmo tempo.  \n",
        "\n",
        "Sabendo que a organização das *threads* ocorre na forma de uma **grade de blocos**, a conta que precisa ser feita é:\n",
        "\n",
        "* definir o número de *threads* do bloco\n",
        "* dividir o número de elementos a manipular pelo tamanho do bloco, resultando no tamanho da grade\n",
        "\n",
        "É claro, né?! Número de threads = núm_blocos * tam_bloco :-)\n",
        "\n",
        "<br>\n",
        "\n",
        "Para que todos os elementos sejam calculados, ou seja, para que haja ao menos uma *thread* por elemento a calcular, é preciso **arredondar para cima** o **resultado desta divisão**.\n",
        "\n",
        "Caso as *threads* do bloco sejam organizadas em apenas 1 dimensão (x), isso pode ser feito de duas formas:\n",
        "\n",
        "* somando o tamanho do boco menos um elemento (block.x -1) ao número de elementos, se quisermos fazer esta conta usando valores do tipo inteiro (int), que descarta o resto;\n",
        "\n",
        "* fazendo a divisão de valores em ponto flutuante, e considerando o próximo valor inteiro, se o resultado for fracionado, usando a função ceil().\n",
        "\n",
        "\n",
        "```c\n",
        "dim3 block, grid;\n",
        "\n",
        "block.x = 1024;\n",
        "grid.x = (nElem + block.x -1) / block.x;    // divisão com valores inteiros\n",
        "// ou\n",
        "grid.x = ceil ( (float)nElem / (float)block.x );  // cálculo com float, pegando inteiro seguinte quando resultado é fracionado\n",
        "```\n",
        "\n",
        "Em geral, a primeira opção é usada, dado que o cálculo de valores inteiros é menos custoso do que operações em ponto flutuante.\n"
      ],
      "metadata": {
        "id": "pKIOi2M0sXp_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Organização de *threads* numa soma de matrizes\n",
        "\n",
        "Consideremos uma soma de 2 matrizes com 1000 x 1000 elementos.\n",
        "\n",
        "Pensando na decomposição máxima que esse problema pode sofrer, há 1.000.000 somas a serem feitas, todas independentes e passíveis de serem realizadas simultaneamente.\n",
        "\n",
        "Como vimos, contudo, não dá para criar todas essas *threads* usando apenas as **dimensões do bloco** de *threads*, que está limitado a 1024.\n",
        "\n",
        "Pensando apenas em resolver a conta, uma possível estratégia seria então considerar um bloco com 1024 *threads* e criar um número de blocos necessários para acomodar todas as *threads*. Assim, a composição do *grid* poderia ser algo como {1000000/1024, 1, 1} .\n",
        "\n",
        "Vale observar que, como estamos tratando de divisão inteira, pode ser preciso usar o **teto** (*ceil*()) desta divisão. Para tanto, é comum fazer uma conta como (1000000+1023) / 1024. Neste caso, a parte inteira do resultado é 977.\n",
        "\n",
        "<br>\n",
        "\n",
        "Assim, uma possível ativação do *kernel* poderia dimensionar as *threads* da seguinte maneira:\n",
        "\n",
        "```c\n",
        "  int threadsPerBlock = 1024;\n",
        "  int blocksPerGrid = ( 1000000 + threadsPerBlock - 1) / threadsPerBlock;\n",
        "  // ou int blocksPerGrid = (int) ceil (1000000. / (double)threadsPerBlock );\n",
        "\n",
        "  MatAdd <<< blocksPerGrid, threadsPerBlock >>> ();\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "Ah, neste caso, temos uma *thread* para cada soma, mas falta definir qual *thread* vai manipular qual elemento da matriz (bi-dimensional)...\n",
        "\n",
        "<br>\n",
        "\n",
        "Vale lembrar que tanto a organização das *threads* de um bloco quanto a composição dos blocos do *grid* são estruturas que podem ter 1, 2 ou 3 dimensões. Assim, será que há alguma outra forma mais vantajosa para organizarmos as *threads*?\n",
        "\n",
        "Para responder essa pergunta, é preciso conhecer alguns aspectos adicionais sore a arquitetura CUDA e a organização física dos processadores."
      ],
      "metadata": {
        "id": "1WhlXy3SI2pd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72y2XxCpQx7x"
      },
      "source": [
        "# Cuda SMs\n",
        "\n",
        "Ao longo do tempo, diferentes modelos de GPU foram desenvolvidos, com capacidades variadas. Entre os aspectos distintos nas diferentes versões está o número de multiprocessadores (**SMs: *Stream Multiprocessors***).\n",
        "\n",
        "Esse valor e o número de elementos de processamento dentro de cada SM mudam de acordo com a [*capability*](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities) da placa. Essas informações podem ser obtidas com a chamada [cudaGetDeviceProperties](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g1bf9d625a931d657e08db2b4391170f0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRslkcw_Q02i",
        "outputId": "d362d4f3-46d2-4966-ea23-fddb263243f2"
      },
      "source": [
        "%%writefile dev-cap.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "\tcudaSetDevice(0);\n",
        "\tcudaDeviceProp prop;\n",
        "\tcudaGetDeviceProperties(&prop,0);\n",
        "\n",
        " \tprintf(\"Modelo do Device: %s\\n\\n\",prop.name);\n",
        "  printf(\"Computing capability: %d.%d\\n\",prop.major, prop.minor);\n",
        "  printf(\"Número de SMs (multiProcessorCounr): %d\\n\",prop.multiProcessorCount);\n",
        "  printf(\"warpSize: %d\\n\\n\", prop.warpSize);\n",
        "\n",
        "\treturn 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dev-cap.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ammAd6nYQ2jG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85cc2372-e21f-4b19-f9e3-66aa8b33e3da"
      },
      "source": [
        "! nvcc dev-cap.cu -o dev-cap && ./dev-cap"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo do Device: Tesla T4\n",
            "\n",
            "Computing capability: 7.5\n",
            "Número de SMs (multiProcessorCounr): 40\n",
            "warpSize: 32\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW6BmQ0YW0b4"
      },
      "source": [
        "# Execução de *threads*\n",
        "\n",
        "https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation\n",
        "\n",
        "Quando um programa CUDA executado no *host* (CPU) ativa a execução de um *kernel*, é especificada uma grade (*grid*) de blocos, cada um contendo uma organização de *threads*. Cabe ao programador decidir como os dados a manipular serão divididos entre essas *threads*, ou seja, quantas *threads* são necessárias para manipular os dados.\n",
        "\n",
        "Para isso, o programador pode considerar a GPU de forma abstrata, como se ela tivesse tantos núcleos (*cores*) que fosse possível executar todas essas *threads* ao mesmo tempo, cada uma manipulando um dado em paralelo, numa única etapa.\n",
        "\n",
        "Como vimos, contudo, o número de processadores (*CUDA cores*) em uma GPU pode [variar](https://www.nvidia.com/pt-br/geforce/graphics-cards/compare/), estando comumente limitados a alguns poucos milhares [1].\n",
        "\n",
        "Além disso, a organização desses processadores numa GPU também pode variar. A arquitetura das GPUs Nvidia é construída na forma de um conjunto escalável de blocos multiprocessadores, chamados de ***Streaming Multiprocessors***, ou **SMs**. Cada um desses SMs tem tipicamente 32 processadores (*CUDA cores*).\n",
        "\n",
        "<br>\n",
        "\n",
        "Contudo, como é que as *threads* especificadas na ativação de um *kernel* são atribuídas aos processadores da GPU para execução?\n",
        "\n",
        "<br>\n",
        "\n",
        "As atribuições para execução ocorrem em blocos de *threads*. Os blocos de *threads* do *grid* especificado são atribuídos aos multiprocessadores (SMs), aparentemente em rodadas (*round-robin*), e não migram. Assim, múltiplos blocos de *threads* podem ser executados de forma paralela nos vários SMs, sendo que as *threads* de um bloco executam de maneira paralela dentro de um SM.\n",
        "\n",
        "É claro, contudo, que o número de blocos pode ser superior ao número de SMs e o número de *threads* pode ser superior ao número de *CUDA cores* em um SM. Assim, pode haver uma **concorrência** tanto na execução das *threads* de um bloco quanto na execução dos blocos.\n",
        "\n",
        "À medida que a execução dos blocos de *thread* é concluída, novos blocos são ativados (*launched*) nos multiprocessadores que ficaram vazios.\n",
        "\n",
        "Esse gerenciamento é feito internamente pela GPU, sem intereferência do programador.\n",
        "\n",
        "<br>\n",
        "\n",
        "Cada multiprocessador é projetado para executar centenas de *threads* concorrentemente, no modelo SIMT (*Single-Instruction, Multiple-Thread*).\n",
        "\n",
        "As *threads* em um multiprocessador (SM) são gerenciadas e **executadas de forma paralela em grupos de 32**, chamados ***warps***. As *threads* de um *warp* são colocadas em execução a partir do mesmo endereço do programa, mas cada *thread* tem seu ponteiro de instruções e estado dos registradores próprios, de forma que podem sofrer desvios e executar de forma independente.\n",
        "\n",
        "Quando um multiprocessador recebe um bloco de *threads* para executar, ele o particiona em *warps*, que serão escalonadas para execução por um escalonador de *warps*. O particionamento das *threads* em *warps* ocorre sempre da mesma maneira, atribuindo a cada *warp* as *threads* com identificadores consecutivos.\n",
        "\n",
        "Uma *warp* executa uma instrução comum de cada vez, de forma que a eficiência máxima é obtida quando todas as 32 *threads* de uma *warp* seguem no mesmo fluxo de execução. Se as *threads* numa *warp* divergem em desvios condicionais, a *warp* executa cada caminho seguido, desabilitando as *threads* cujo fluxo não está naquele caminho. *Warps* distintas, contudo, têm fluxos de execução próprios, de maneira disjunta das demais *warps*.\n",
        "\n",
        "Embora nas primeiras versões de arquiteturas de GPUs um único ponteiro de instruções era mantido para todas as *threads* de uma *warp*, em versões mais recentes, os fluxos podem ser distintos nas *threads*. Assim, do ponto de vista da execução do código, o comportamento das *threads* vai ser correto mesmo que o programador ignore o modelo de execução SIMT. De todo modo, melhores desempenhos podem ser obtidos se os fluxos de execução nas *threads* não divergirem.\n",
        "\n",
        "### **Em suma**:\n",
        "\n",
        "* Todos os SMs de uma GPU trabalham em paralelo, de forma independente\n",
        "\n",
        "* O particionamento do *grid* de *threads* de um *kernel* é feito atribuindo blocos inteiros aos SMs.\n",
        "\n",
        "* Dentro de um SM, as *threads* de cada bloco são divididas em grupos de 32 *threads* para execução, chamados *warps*.\n",
        "\n",
        "* A execução de um *warp* ocorre no modelo SIMT, com todas as *threads* executando a mesma sequência de instruções.\n",
        "  * Desvios condicionais no fluxo de instruções das *threads* de uma *warp* podem fazer com que algumas *threads*, que não seguiram o mesmo caminho no código, fiquem temporariamente paradas.\n",
        "\n",
        "* *Threads* em um SM compartilham a memória deste SM, que pode ser usada para comunicação eficiente entre elas.\n",
        "\n",
        "* Cooperações e sincronizações só são possíveis entre *threads* do mesmo bloco.\n",
        "\n",
        "<!--\n",
        "### Algumas conclusões    (minhas, requerem mais investigação... *hélio*)\n",
        "\n",
        "* O paralelismo efetivo máximo corresponde a 32 threads de um *warp* vezes o número de SMs da GPU\n",
        "* Dentro de uma SM, até 32 *threads* de uma *warp* vão estar em execução de uma vez (pois esse é o número de *processing elements*). Na verdade, depende do número de CUDA cores em cada SM... Esse valor pode ser 64....\n",
        "*\n",
        "-->\n",
        "\n",
        "<br>\n",
        "\n",
        "[1] [Nvidia GPUs sorted by CUDA cores](https://gist.github.com/cavinsmith/ed92fee35d44ef91e09eaa8775e3284e)<br>\n",
        "[2] https://forums.developer.nvidia.com/t/what-is-cores-per-sm/29997"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hierarquia de *threads*\n",
        "\n",
        "https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#thread-hierarchy\n",
        "\n",
        "***Thread Hierarchy***\n",
        "\n",
        "*For convenience, threadIdx is a 3-component vector, so that threads can be identified using a one-dimensional, two-dimensional, or three-dimensional thread index, forming a one-dimensional, two-dimensional, or three-dimensional block of threads, called a thread block. This provides a natural way to invoke computation across the elements in a domain such as a vector, matrix, or volume.*\n",
        "\n",
        "*The index of a thread and its thread ID relate to each other in a straightforward way:*\n",
        "\n",
        "* For a one-dimensional block, they are the same;\n",
        "* for a two-dimensional block of size (Dx, Dy), the thread ID of a thread of index (x, y) is (x + y Dx);\n",
        "* for a three-dimensional block of size (Dx, Dy, Dz), the thread ID of a thread of index (x, y, z) is (x + y Dx + z Dx Dy).\n"
      ],
      "metadata": {
        "id": "HFkgowmZFqfF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkaLUkOCwzO-"
      },
      "source": [
        "# Linearização dos índices\n",
        "\n",
        "É claro que GPUs e seus processadores que operam no modo SIMT têm tudo a ver com o processamento simultâneo de dados de estruturas como matrizes e vetores. Basta determinar uma forma para que cada *thread* na GPU saiba qual dado irá manipular.\n",
        "\n",
        "O caminho natural para este mapeamento é usar os **índices das *threads* e dos blocos** a que pertencem nessa identificação.\n",
        "\n",
        "<br>\n",
        "\n",
        "O programa a seguir visa testar o mapeamento de índices de um vetor manipulado por diferentes blocos de *threads*.\n",
        "\n",
        "Do ponto de vista da alocação de memória para estruturas bi ou tridimensionais, uma estratégia é alocar os elementos em sequência, como se fosse uma estrutura unidimensional.\n",
        "\n",
        "Para mapear as posições corretas da estrutura, contudo, é preciso  **linearizar** os índices.\n",
        "\n",
        "Isso é o que é feito quando uma matriz é tratada como uma sequência de linhas, cada uma composta da dimensão completa das colunas:\n",
        "\n",
        "M \\[ i, j \\] = M \\[ i \\* ncol +j \\]\n",
        "\n",
        "O exemplo a seguir procura ilustrar a linearização dos índices das *threads*, considerando a organização do grid como uma estrutura de blocos, cada um composto de uma estrutura de *threads*.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nN_ObJK2w0hC",
        "outputId": "082e0b2c-c0ab-45d5-d45d-d3031b763557"
      },
      "source": [
        "%%writefile ind.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "/* https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#thread-hierarchy\n",
        " *\n",
        " * The index of a thread and its thread ID relate to each other in a straightforward way:\n",
        " *\n",
        " * For a one-dimensional block, they are the same;\n",
        " * for a two-dimensional block of size (Dx, Dy), the thread ID of a thread of index (x, y) is (x + y Dx);\n",
        " * for a three-dimensional block of size (Dx, Dy, Dz), the thread ID of a thread of index (x, y, z) is (x + y Dx + z Dx Dy).\n",
        " */\n",
        "\n",
        "\n",
        "__global__\n",
        "void mark(int *vet, int dim)\n",
        "{\n",
        "  //  for a three-dimensional block of size (Dx, Dy, Dz), the thread ID of a thread of index (x, y, z) is (x + y Dx + z Dx Dy).\n",
        "  // int block = blockIdx.x + blockIdx.y * gridDim.x + blockIdx.z * gridDim.x * gridDim.y;\n",
        "  int block = blockIdx.x * gridDim.y * gridDim.z + blockIdx.y * gridDim.z + blockIdx.z;\n",
        "\n",
        "  // int thread = threadIdx.x + threadIdx.y * blockDim.x + threadIdx.z * blockDim.x * blockDim.y;\n",
        "  int thread = threadIdx.x * blockDim.y * blockDim.z + threadIdx.y * blockDim.z + threadIdx.z;\n",
        "\n",
        "  int ind = block * blockDim.x * blockDim.y * blockDim.z + thread;\n",
        "\n",
        "  printf(\"%d,%d,%d / %d,%d,%d : %d\\n\", blockIdx.x, blockIdx.y,blockIdx.z,\n",
        "        threadIdx.x, threadIdx.y, threadIdx.z, ind);\n",
        "\n",
        "  // Testa se índice resultante está dentro do limite do vetor!\n",
        "  if (ind < dim)\n",
        "    vet[ind] = ind;\n",
        "}\n",
        "\n",
        "int\n",
        "main(int argc, char **argv)\n",
        "{\n",
        "  int i, *vet, *d_vet;\n",
        "  dim3 grid = {2,2,2};    // organização dos blocos de threads\n",
        "  dim3 block = {2,2,2};   // organização das threads em cada bloco\n",
        "\n",
        "  int dim = grid.x * grid.y * grid.z * block.x * block.y * block.z;\n",
        "\n",
        "  vet = (int *)malloc(dim * sizeof(int));\n",
        "\n",
        "  // alocação de memória na GPU\n",
        "  cudaMalloc(&d_vet, dim * sizeof(int));\n",
        "\n",
        "  // Neste caso, não precisa copiar para a memória da GPU; inicialização na GPU\n",
        "  // cudaMemcpy(d_vet, vet, dim * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "  mark <<< grid, block >>>(d_vet, dim);\n",
        "\n",
        "  // Esta é a forma de testar se houve erro na ativação do kernel\n",
        "  cudaError_t err = cudaGetLastError();\n",
        "  if (err != cudaSuccess) {\n",
        "    printf(\"CUDA Error: %s\\n\",cudaGetErrorString(err));\n",
        "    cudaDeviceReset();\n",
        "    exit(0);\n",
        "  }\n",
        "\n",
        "  // Cópia dos dados da memória da GPU para a memória RAM\n",
        "  cudaMemcpy(vet, d_vet, dim * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "  // Verifica se índices foram preenchidos corretamente\n",
        "  for (i=0; i < dim; i++)\n",
        "    if(vet[i]!=i)\n",
        "      printf(\"Erro em %d\\n\",i);\n",
        "\n",
        "  cudaFree(d_vet);\n",
        "\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  return (0);\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ind.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VH3NYYZxE0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a27ca7ed-8b97-4af0-99f6-110b2b228836"
      },
      "source": [
        "# Se ordenarmos o resultado, fica fácil conferir para quem sabe contar em binário :-)\n",
        "# ! nvcc -arch=sm_37 -gencode=arch=compute_37,code=sm_37 ind.cu -o ind && ./ind | sort\n",
        "! nvcc ind.cu -o ind && ./ind | sort"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0,0,0 / 0,0,0 : 0\n",
            "0,0,0 / 0,0,1 : 1\n",
            "0,0,0 / 0,1,0 : 2\n",
            "0,0,0 / 0,1,1 : 3\n",
            "0,0,0 / 1,0,0 : 4\n",
            "0,0,0 / 1,0,1 : 5\n",
            "0,0,0 / 1,1,0 : 6\n",
            "0,0,0 / 1,1,1 : 7\n",
            "0,0,1 / 0,0,0 : 8\n",
            "0,0,1 / 0,0,1 : 9\n",
            "0,0,1 / 0,1,0 : 10\n",
            "0,0,1 / 0,1,1 : 11\n",
            "0,0,1 / 1,0,0 : 12\n",
            "0,0,1 / 1,0,1 : 13\n",
            "0,0,1 / 1,1,0 : 14\n",
            "0,0,1 / 1,1,1 : 15\n",
            "0,1,0 / 0,0,0 : 16\n",
            "0,1,0 / 0,0,1 : 17\n",
            "0,1,0 / 0,1,0 : 18\n",
            "0,1,0 / 0,1,1 : 19\n",
            "0,1,0 / 1,0,0 : 20\n",
            "0,1,0 / 1,0,1 : 21\n",
            "0,1,0 / 1,1,0 : 22\n",
            "0,1,0 / 1,1,1 : 23\n",
            "0,1,1 / 0,0,0 : 24\n",
            "0,1,1 / 0,0,1 : 25\n",
            "0,1,1 / 0,1,0 : 26\n",
            "0,1,1 / 0,1,1 : 27\n",
            "0,1,1 / 1,0,0 : 28\n",
            "0,1,1 / 1,0,1 : 29\n",
            "0,1,1 / 1,1,0 : 30\n",
            "0,1,1 / 1,1,1 : 31\n",
            "1,0,0 / 0,0,0 : 32\n",
            "1,0,0 / 0,0,1 : 33\n",
            "1,0,0 / 0,1,0 : 34\n",
            "1,0,0 / 0,1,1 : 35\n",
            "1,0,0 / 1,0,0 : 36\n",
            "1,0,0 / 1,0,1 : 37\n",
            "1,0,0 / 1,1,0 : 38\n",
            "1,0,0 / 1,1,1 : 39\n",
            "1,0,1 / 0,0,0 : 40\n",
            "1,0,1 / 0,0,1 : 41\n",
            "1,0,1 / 0,1,0 : 42\n",
            "1,0,1 / 0,1,1 : 43\n",
            "1,0,1 / 1,0,0 : 44\n",
            "1,0,1 / 1,0,1 : 45\n",
            "1,0,1 / 1,1,0 : 46\n",
            "1,0,1 / 1,1,1 : 47\n",
            "1,1,0 / 0,0,0 : 48\n",
            "1,1,0 / 0,0,1 : 49\n",
            "1,1,0 / 0,1,0 : 50\n",
            "1,1,0 / 0,1,1 : 51\n",
            "1,1,0 / 1,0,0 : 52\n",
            "1,1,0 / 1,0,1 : 53\n",
            "1,1,0 / 1,1,0 : 54\n",
            "1,1,0 / 1,1,1 : 55\n",
            "1,1,1 / 0,0,0 : 56\n",
            "1,1,1 / 0,0,1 : 57\n",
            "1,1,1 / 0,1,0 : 58\n",
            "1,1,1 / 0,1,1 : 59\n",
            "1,1,1 / 1,0,0 : 60\n",
            "1,1,1 / 1,0,1 : 61\n",
            "1,1,1 / 1,1,0 : 62\n",
            "1,1,1 / 1,1,1 : 63\n"
          ]
        }
      ]
    }
  ]
}