{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rF76EjM3ZQ7r"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF76EjM3ZQ7r"
      },
      "source": [
        "# PPD: Programação com CUDA\n",
        "\n",
        "Hélio - DC/UFSCar - 2023"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paralelização\n",
        "\n",
        "Como visto, o modelo de paralelização em GPUs usando CUDA consiste em criar *kernels* (funções) cujo código é executado simultaneamente por múltiplas *threads*. Trata-se de um caso típico de decomposição de domnínio (ou de dados).\n",
        "\n",
        "Na paralelização de um *loop for*, a estratégia é direta: ao invés de iterar N vezes, e se houver **1 *thead* para executar o código de cada iteração**?\n",
        "\n",
        "\n",
        "```c\n",
        "int N = 1<<10;\n",
        "\n",
        "for (int i = 0; i < N; ++i) {\n",
        "  a[i] = 2 * a[i];\n",
        "}\n",
        "```\n",
        "\n",
        "Para isso, é preciso criar um *kernel* que execute o código de 1 iteração do *loop*. Basta agora ativar a execução deste *kernel* com um arranjo de *threads* que corresponda ao número de iterações.\n",
        "\n",
        "É claro, contudo, que o código deste *kernel* deve ter algum mecanismo para que cada *thread* saiba qual é a iteração que deve executar; ou seja, quais elementos deve manipular.\n",
        "\n",
        "```c\n",
        "int N = 1<<10;\n",
        "\n",
        "__global__ void loop() {\n",
        "  // Em geral, a função do kernel corresponde ao trabalho de 1 iteração do loop.\n",
        "  // Como determinar qual iteração?\n",
        "  // Neste caso, há apenas 1 dimensão na organização das threads e o índice é direto\n",
        "\n",
        "  int i = threadIdx.x;\n",
        "\n",
        "  a[i] = 2 * a[i];\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  // ...\n",
        "\n",
        "  loop <<<1, N>>>();\n",
        "\n",
        "  // ...\n",
        "  return 0;\n",
        "}\n",
        "```\n"
      ],
      "metadata": {
        "id": "FReRbllgEiIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como há um limite para o número de *threads* em cada bloco (1024), para conseguir ter mais *threads* (1 para cada iteração), é preciso usar múltiplos blocos na ativação do *kernel*.\n",
        "\n",
        "Uma estratégia é dividir o número de elementos (e iterações) pelo tamanho do bloco.\n",
        "\n",
        "```c\n",
        "int N = 1<<20;\n",
        "\n",
        "for (int i = 0; i < N; ++i) {\n",
        "  a[i] = 2 * a[i];\n",
        "}\n",
        "```\n",
        "\n",
        "Considerando que cada bloco terá o número máximo de *threads* permitido, uma estratégia é fazer a divisão do número de iterações pelo tamanho dos blocos, o que vai resultar no número de blocos necessários.\n",
        "\n",
        "O cálculo do índice que cada *thread* irá manipular, contudo, passa a ser calculado em função do número do bloco e desta *thread* dentro do bloco em que está.\n",
        "\n",
        "```c\n",
        "int N = 1<<20;\n",
        "\n",
        "__global__ void loop() {\n",
        "  // Em geral, a função do kernel corresponde ao trabalho de 1 iteração do loop.\n",
        "  // Como determinar qual iteração?\n",
        "  // Como há vários blocos, o índice pode ser obtido em função do\n",
        "  // número do bloco vezes o tamanho de cada bloco, mais o índice da thread neste bloco\n",
        "  \n",
        "  int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "\n",
        "  a[i] = 2 * a[i];\n",
        "\n",
        "  // Opa! É claro que 'a' deve ser uma variável em memória acessível pelo código na GPU...\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  // ...\n",
        "\n",
        "  // falta a declaração de a e ajustes para que este vetor seja acessível\n",
        "  // pelo código da GPU...\n",
        "  ...\n",
        "\n",
        "  int nthreads, nblocks;\n",
        "  \n",
        "  // supondo blocos com o tamanho máximo, organizados apenas na dimensão x\n",
        "  nthreads = 1024;\n",
        "  nblocks = (N + nthreads-1) / nthreads;\n",
        "\n",
        "  loop <<< nblocks, nthreads >>>();\n",
        "\n",
        "  // ... outras organizações de blocos poderiam ser usadas...\n",
        "\n",
        "  // ...\n",
        "  return 0;\n",
        "}\n",
        "```\n"
      ],
      "metadata": {
        "id": "bJN8juIAa7YM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No cálculo do número de blocos de *threads*, é possível que a divisão do número de elementos (iterações) pelo tamanho do bloco resulte num número de *threads* (blocos * tam blocos) maior que o número de iterações que precisam ser realizadas.\n",
        "\n",
        "Assim, usando os mecanismos de cálculo do índice a manipular dentro da função do *kernel* executada pelas *threads*, pode ser que o índice resultante seja maior do que o total de elementos.\n",
        "\n",
        "Deste modo, é preciso que no início de sua execução, cada *thread* determine o índice da iteração que corresponde aos seus identificadores (blockIdx.[x,y,z] e threadIdx.[x,y,z]) e verifique se este índice corresponde a uma iteração que precisa ser calculada. Caso contrário, esta *thread* não executa o código previsto, já que estaria manipulando elementos inexistentes ou iterações não previstas.\n",
        "\n",
        "\n",
        "```c\n",
        "int N = 1>>2000;\n",
        "\n",
        "size_t threads_per_block = 128;\n",
        "\n",
        "// ...\n",
        "\n",
        "__global__ k_function() {\n",
        "\n",
        "  int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  if (i < N) { // Verifica se `i` corresponde a uma iteração válida (0..N-1)\n",
        "    // executa código\n",
        "  }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  // ...\n",
        "\n",
        "  // É preciso haver ao menos N threads na grade, com apenas 1 bloco excedente\n",
        "  size_t number_of_blocks = (N + threads_per_block - 1) / threads_per_block;\n",
        "\n",
        "  k_function <<< number_of_blocks, threads_per_block>>> ();\n",
        "\n",
        "  // ...\n",
        "  return 0;\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "1HSNJKryiSat"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSFT46XDx5MR"
      },
      "source": [
        "# Estudo de Caso: saxpy\n",
        "\n",
        "O exemplo a seguir, extraído de https://developer.nvidia.com/blog/easy-introduction-cuda-c-and-c, ilustra uma implementação do programa SAXPY (Single-precision A * X Plus Y), onde se pode ver a manipulação de memória com alocação, cópias de e para a GPU e liberação do espaço."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile saxpy.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__\n",
        "void saxpy(int n, float a, float *x, float *y)\n",
        "{\n",
        "  int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (i < n)\n",
        "    y[i] = a*x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{blockSize\n",
        "  int N = 1<<20;\n",
        "  float *x, *y, *d_x, *d_y;\n",
        "\n",
        "  x = (float*)malloc(N*sizeof(float));\n",
        "  y = (float*)malloc(N*sizeof(float));\n",
        "\n",
        "  // alocação de memória na GPU\n",
        "  cudaMalloc(&d_x, N*sizeof(float));\n",
        "  cudaMalloc(&d_y, N*sizeof(float));\n",
        "\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Cópia dos dados da memória RAM para a memória do dispositivo\n",
        "  cudaMemcpy(d_x, x, N*sizeof(float), cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_y, y, N*sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "  // Perform SAXPY on 1M elements\n",
        "  saxpy<<<(N+255)/256, 256>>>(N, 2.0f, d_x, d_y);\n",
        "\n",
        "  // Cópia dos dados da memória da GPU para a memória RAM\n",
        "  cudaMemcpy(y, d_y, N*sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = max(maxError, abs(y[i]-4.0f));\n",
        "  printf(\"Max error: %f\\n\", maxError);\n",
        "\n",
        "  // Liberação das áreas de memória alocadas da GPU\n",
        "  cudaFree(d_x);\n",
        "  cudaFree(d_y);\n",
        "  free(x);\n",
        "  free(y);\n",
        "}"
      ],
      "metadata": {
        "id": "n8DiZP9RJ8IK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e48af70-9490-4d49-c5a9-0cd9189394a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing saxpy.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aci8vuUFTpX_"
      },
      "source": [
        "# Calculando o tempo decorrido de processamento na GPU\n",
        "\n",
        "Na programação com GPUs, uma estratégia para a paralelização é criar uma *thread* para executar cada uma das iterações dos laços contendo atividades independentes. A execução dessas *threads* de forma paralela, em rodadas executadas por até milhares de núcleos de processamento das GPUs, deve reduzir o tempo de execução.\n",
        "\n",
        "Uma forma de medir o tempo total de execução da aplicação é usar o utilitário ***time*** na ativação do programa paralelo em linha de comando.\n",
        "\n",
        "```\n",
        "$ time ./prog-par\n",
        "\n",
        "real\t0m12.345s\n",
        "user\t0m0.369s\n",
        "sys\t 0m1.149s\n",
        "```\n",
        "\n",
        "Como há várias etapas e atividades para a execução de programas em GPU, contudo, pode ser interessante realizar medidas de tempo dentro do código, de forma a identificar tempos gastos com diferentes atividades, como transferências de memória e processamento, por exemplo.\n",
        "\n",
        "Assim, uma outra opção para medir tempos associados às atividades é usar o mecanismo provido pelo SO para verificar o instante atual antes e após realizar as operações desejadas, como ilustrado a seguir:\n",
        "\n",
        "```c\n",
        "#include <sys/time.h>\n",
        "...\n",
        "struct timeval inic, fim;\n",
        "// struct timespec inic, fim;\n",
        "double etime;\n",
        "...\n",
        "// verifica instante atual antes do bloco de código a medir\n",
        "gettimeofday(&inic, 0);    \n",
        "// clock_gettime(CLOCK_REALTIME, &inic);\n",
        "\n",
        "  // ativação do kernel\n",
        "  kernel_call<<<dimGrid, dimBlock>>>();\n",
        "  // mesmo retornando da função, não significa que o kernel já foi executado...\n",
        "\n",
        "  // função que aguarda pela conclusão da última atividade submetida à execução na GPU\n",
        "  cudaDeviceSynchronize();)\n",
        "  // pronto, se retornou da chamada de sincronização, significa que a última operação na GPU (execução, neste caso) foi concluída\n",
        "\n",
        "// verifica instante atual após a conclusão do bloco de código\n",
        "gettimeofday(&fim, 0);\n",
        "// clock_gettime(CLOCK_REALTIME, &fim);\n",
        "\n",
        "// tempo decorrido: elapsed time\n",
        "etime = (fim.tv_sec + fim.tv_usec/1e6) - (inic.tv_sec + inic.tv_usec/1e6),\n",
        "// etime = (fim.tv_sec + fim.tv_nsec/1e9) - (inic.tv_sec + inic.tv_nsec/1e9);\n",
        "\n",
        "```\n",
        "\n",
        "Um aspecto a observar neste caso é que a execução da função de um *kernel* é assíncrona em relação ao processamento em CPU. Isso significa que para saber o instante em que a execução do *kernel* é concluída é preciso realizar uma operação de sincronização, como provido pela chamada [cudaDeviceSynchronize](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g10e20b05a95f638a4071a655503df25d)().\n",
        "\n",
        "Assim, esta técnica talvez não seja a mais adequada se quisermos medir tempos de atividades distintas na GPU sem realizar um bloqueio entre elas.\n",
        "\n",
        "Há, contudo, outra forma de medir tempos gastos por atividades em GPU usando [eventos](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html).\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "## Medição de tempo usando eventos CUDA (cudaEvent)\n",
        "\n",
        "Uma restrição à medição de tempo usando temporizadores e as sincronizações necessárias é que elas paralizam o *pipeline* da GPU. Como alternativa para as medições de tempo, CUDA oferece uma estrutura de [eventos](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html).  \n",
        "\n",
        "Esta API inclui chamadas para criar eventos ([cudaEventCreate](http://docs.nvidia.com/cuda/cuda-runtime-api/index.html#group__CUDART__EVENT_1g320ab51604f3a7a082795202e7eaf774)), para registrar eventos ([cudaEventRecord](http://docs.nvidia.com/cuda/cuda-runtime-api/index.html#group__CUDART__EVENT_1ge31b1b1558db52579c9e23c5782af93e)) e para computar o tempo decorrido entre 2 eventos registrados, podendo ser usada como ilustrado a seguir.\n",
        "\n",
        "```c\n",
        "// declaração e iniciação das estruturas\n",
        "cudaEvent_t start, stop;\n",
        "cudaEventCreate(&start);\n",
        "cudaEventCreate(&stop);\n",
        "...\n",
        "cudaMemcpy(d_x, x, N*sizeof(float), cudaMemcpyHostToDevice);\n",
        "cudaMemcpy(d_y, y, N*sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "// antes de ativar a ação em GPU que se deseja medir, registra-se o evento\n",
        "cudaEventRecord(start);\n",
        "\n",
        "saxpy<<<(N+255)/256, 256>>>(N, 2.0f, d_x, d_y);\n",
        "\n",
        "// registra-se o evento após a submissão de execução, neste caso.\n",
        "cudaEventRecord(stop);\n",
        "...\n",
        "cudaMemcpy(y, d_y, N*sizeof(float), cudaMemcpyDeviceToHost);\n",
        "...\n",
        "// Espera-se pela conclusão do evento registrado após a execução do kernel\n",
        "cudaEventSynchronize(stop);\n",
        "float milliseconds = 0;\n",
        "cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "```\n",
        "\n",
        "A chamada [cudaEventRecord](http://docs.nvidia.com/cuda/cuda-runtime-api/index.html#group__CUDART__EVENT_1ge31b1b1558db52579c9e23c5782af93e) tem o papel de inserir o evento indicado no fluxo de ações padrão da GPU. Assim, a GPU irá registrar o instante associado a um evento **assim que processá-lo**.\n",
        "\n",
        "Neste caso, o evento associado a ***start*** será processado antes de ativar a execução do kernel e o evento ***stop*** será processado depois que a execução do *kernel* for concluída, de modo que não é preciso que o código em CPU fique bloqueado à espera do evento. Assim, a função [cudaEventSynchronize](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html#group__CUDART__EVENT_1g949aa42b30ae9e622f6ba0787129ff22)() pode ser usada somente quando houver mais eventos a ativar no *kernel* e for relevante esperar pelo registro do tempo.\n",
        "\n",
        "Já a função [cudaEventElapsedtime](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html#group__CUDART__EVENT_1g40159125411db92c835edb46a0989cd6)() retorna o número de milissegundos (com precisão de 0.5 microssegundos) decorridos entre os eventos indicados.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyXrx6VbTqlg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32372d49-84cc-4946-b372-640bce85aba4"
      },
      "source": [
        "%%writefile saxpy-t.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__\n",
        "void saxpy(int n, float a, float *x, float *y)\n",
        "{\n",
        "  int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (i < n)\n",
        "    y[i] = a*x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20;\n",
        "  float *x, *y, *d_x, *d_y;\n",
        "\n",
        "\tcudaEvent_t start, smanagedMemorytop;  // eventos para registro do tempo\n",
        "\n",
        "\t// cria eventos\n",
        "\tcudaEventCreate(&start);\n",
        "\tcudaEventCreate(&stop);\n",
        "\n",
        "\t// alocação dos vetores na memória do host (RAM)\n",
        "  x = (float*)malloc(N*sizeof(float));\n",
        "  y = (float*)malloc(N*sizeof(float));\n",
        "\n",
        "  // alocação dos vetors na memória na GPU\n",
        "  cudaMalloc(&d_x, N*sizeof(float));\n",
        "  cudaMalloc(&d_y, N*sizeof(float));\n",
        "\n",
        "\t// iniciação dos elementos dos vetores na memória RAM\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Cópia dos dados da memória RAM para a memória do dispositivo\n",
        "  cudaMemcpy(d_x, x, N*sizeof(float), cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_y, y, N*sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "\t// registra evento antes da execução do kernel\n",
        "\tcudaEventRecord(start);\n",
        "\n",
        "    // Perform SAXPY on 1M elements\n",
        "    saxpy<<<(N+255)/256, 256>>>(N, 2.0f, d_x, d_y);\n",
        "\n",
        "\t// registra o evento após a execução do kernel\n",
        "\tcudaEventRecord(stop);\n",
        "\n",
        "  // Cópia dos dados da memória da GPU para a memória RAM\n",
        "  cudaMemcpy(y, d_y, N*sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = max(maxError, abs(y[i]-4.0f));\n",
        "  printf(\"Max error: %f\\n\", maxError);\n",
        "\n",
        "  // Liberação das áreas de memória alocadas da GPU\n",
        "  cudaFree(d_x);\n",
        "  cudaFree(d_y);\n",
        "  free(x);\n",
        "  free(y);\n",
        "\n",
        "\t// garante que o evento stop já ocorreu\n",
        "\tcudaEventSynchronize(stop);\n",
        "\n",
        "\tfloat milliseconds = 0;\n",
        "\tcudaEventElapsedTime(&milliseconds, start, stop);\n",
        "\tprintf(\"Tempo de Execução na GPU: %.4f ms\", milliseconds);\n",
        "\n",
        "\treturn 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing saxpy-t.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc saxpy-t.cu -o saxpy-t -O3\n",
        "! time ./saxpy-t"
      ],
      "metadata": {
        "id": "DdfBR1Tort7i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0be5bbf5-77bb-49bb-ec9a-e433195c9a11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max error: 0.000000\n",
            "Tempo de Execução na GPU: 0.0562 ms\n",
            "real\t0m0.938s\n",
            "user\t0m0.023s\n",
            "sys\t0m0.845s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile saxpy-t2.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__\n",
        "void saxpy(int n, float a, float *x, float *y)\n",
        "{\n",
        "  int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (i < n)\n",
        "    y[i] = a*x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20;\n",
        "  float *x, *y, *d_x, *d_y;\n",
        "\n",
        "\tcudaEvent_t memcopy_i, memcopy_f, code_i, code_f;  // eventos para registro do tempo apresenta\n",
        "\t// cria eventos\n",
        "\tcudaEventCreate(&memcopy_i);\n",
        "\tcudaEventCreate(&memcopy_f);\n",
        "\tcudaEventCreate(&code_i);\n",
        "\tcudaEventCreate(&code_f);\n",
        "\n",
        "\t// alocação dos vetores na memória do host (RAM)\n",
        "  x = (float*)malloc(N*sizeof(float));\n",
        "  y = (float*)malloc(N*sizeof(float));\n",
        "\n",
        "  // alocação dos vetors na memória na GPU\n",
        "  cudaMalloc(&d_x, N*sizeof(float));\n",
        "  cudaMalloc(&d_y, N*sizeof(float));\n",
        "\n",
        "\t// iniciação dos elementos dos vetores na memória RAM\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // registra evento antes da cópia de memória\n",
        "  cudaEventRecord(memcopy_i);\n",
        "\n",
        "  // Cópia dos dados da memória RAM para a memória do dispositivo\n",
        "  cudaMemcpy(d_x, x, N*sizeof(float), cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_y, y, N*sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "  // registra evento após a cópia de memória\n",
        "  cudaEventRecord(memcopy_f);\n",
        "\n",
        "\t// registra evento antes da execução do kernel\n",
        "\tcudaEventRecord(code_i);\n",
        "\n",
        "    // Perform SAXPY on 1M elements\n",
        "    saxpy<<<(N+255)/256, 256>>>(N, 2.0f, d_x, d_y);\n",
        "\n",
        "\t// registra o evento após a execução do kernel\n",
        "\tcudaEventRecord(code_f);\n",
        "\n",
        "  // Cópia dos dados da memória da GPU para a memória RAM\n",
        "  cudaMemcpy(y, d_y, N*sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = max(maxError, abs(y[i]-4.0f));\n",
        "  printf(\"Max error: %f\\n\", maxError);\n",
        "\n",
        "  // Liberação das áreas de memória alocadas da GPU\n",
        "  cudaFree(d_x);\n",
        "  cudaFree(d_y);\n",
        "  free(x);\n",
        "  free(y);\n",
        "\n",
        "\t// garante que o evento code_f já ocorreu\n",
        "\tcudaEventSynchronize(code_f);\n",
        "\n",
        "\tfloat milliseconds = 0;\n",
        "\tcudaEventElapsedTime(&milliseconds, memcopy_i, memcopy_f);\n",
        "\tprintf(\"Tempo para transferência de memória host->device: %.4f ms\\n\", milliseconds);\n",
        "\tcudaEventElapsedTime(&milliseconds, code_i, code_f);\n",
        "\tprintf(\"Tempo de Execução na GPU: %.4f ms\\n\", milliseconds);\n",
        "\n",
        "\treturn 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svVb5nnM2uQq",
        "outputId": "1243ea93-fd4b-4367-9833-0d8e9e2e1533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing saxpy-t2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! if [ ! saxpy-t2 -nt saxpy-t2.cu ]; then nvcc saxpy-t2.cu -o saxpy-t2 -O3; fi\n",
        "! ./saxpy-t2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKWb4nPR39RB",
        "outputId": "b645c1bf-f858-408f-8a0d-14fe35649e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max error: 0.000000\n",
            "Tempo para transferência de memória host->device: 2.9311 ms\n",
            "Tempo de Execução na GPU: 128.3095 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Medindo outros valores associados à execução do programa\n",
        "\n",
        "***Obs***: Vou procurar criar uma seção específica sobre *profiling* mas, por ora, que tal examinar outros valores associados aos tempos de execução de um programa em GPU?"
      ],
      "metadata": {
        "id": "M-tnvfAKsaot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! nvprof ./saxpy-t2"
      ],
      "metadata": {
        "id": "saUWiD1JtQ9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c00a0589-aa4c-48ff-e005-ee6dc2e4e72e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==1954== NVPROF is profiling process 1954, command: ./saxpy-t2\n",
            "Max error: 0.000000\n",
            "Tempo para transferência de memória host->device: 2.1941 ms\n",
            "Tempo de Execução na GPU: 0.3047 ms\n",
            "==1954== Profiling application: ./saxpy-t2\n",
            "==1954== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   73.78%  1.6676ms         2  833.79us  813.91us  853.66us  [CUDA memcpy HtoD]\n",
            "                   24.16%  546.17us         1  546.17us  546.17us  546.17us  [CUDA memcpy DtoH]\n",
            "                    2.06%  46.495us         1  46.495us  46.495us  46.495us  saxpy(int, float, float*, float*)\n",
            "      API calls:   98.17%  226.62ms         4  56.655ms     811ns  226.62ms  cudaEventCreate\n",
            "                    1.27%  2.9407ms         3  980.24us  831.23us  1.1199ms  cudaMemcpy\n",
            "                    0.16%  372.90us         2  186.45us  163.84us  209.06us  cudaFree\n",
            "                    0.14%  330.68us         1  330.68us  330.68us  330.68us  cudaLaunchKernel\n",
            "                    0.12%  268.01us         2  134.01us  106.23us  161.79us  cudaMalloc\n",
            "                    0.09%  208.06us       114  1.8250us     292ns  74.423us  cuDeviceGetAttribute\n",
            "                    0.02%  42.060us         4  10.515us  2.2610us  20.441us  cudaEventRecord\n",
            "                    0.01%  13.799us         1  13.799us  13.799us  13.799us  cuDeviceGetName\n",
            "                    0.01%  12.182us         1  12.182us  12.182us  12.182us  cuDeviceTotalMem\n",
            "                    0.00%  8.7330us         1  8.7330us  8.7330us  8.7330us  cudaEventSynchronize\n",
            "                    0.00%  7.4670us         1  7.4670us  7.4670us  7.4670us  cuDeviceGetPCIBusId\n",
            "                    0.00%  5.2480us         2  2.6240us  1.6940us  3.5540us  cudaEventElapsedTime\n",
            "                    0.00%  2.5630us         3     854ns     358ns  1.8470us  cuDeviceGetCount\n",
            "                    0.00%  1.1630us         2     581ns     328ns     835ns  cuDeviceGet\n",
            "                    0.00%     520ns         1     520ns     520ns     520ns  cuModuleGetLoadingMode\n",
            "                    0.00%     430ns         1     430ns     430ns     430ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observem a diferença de tempo entre as operações de cópia de memória e processamento!!!!"
      ],
      "metadata": {
        "id": "RJ8QAIso5oEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saxpy com cudaMallocManaged\n",
        "\n",
        "Que tal experimentar o programa saxpy usando memória unificada?\n",
        "\n",
        "O que muda? cudaMalloc -> cudaMallocManaged\n"
      ],
      "metadata": {
        "id": "XkWmxW7WI2rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile saxpy.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__\n",
        "void saxpy(int n, float a, float *x, float *y)\n",
        "{\n",
        "  int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (i < n)\n",
        "    y[i] = a*x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20;\n",
        "  float *x, *y, *d_x, *d_y;\n",
        "\n",
        "  x = (float*)malloc(N*sizeof(float));\n",
        "  y = (float*)malloc(N*sizeof(float));\n",
        "\n",
        "  // alocação de memória na GPU\n",
        "  cudaMalloc(&d_x, N*sizeof(float));\n",
        "  cudaMalloc(&d_y, N*sizeof(float));\n",
        "\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Cópia dos dados da memória RAM para a memória do dispositivo\n",
        "  cudaMemcpy(d_x, x, N*sizeof(float), cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_y, y, N*sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "  // Perform SAXPY on 1M elements3.0153ms\n",
        "  saxpy<<<(N+255)/256, 256>>>(N, 2.0f, d_x, d_y);\n",
        "\n",
        "  // Cópia dos dados da memória da GPU para a memória RAM\n",
        "  cudaMemcpy(y, d_y, N*sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = max(maxError, abs(y[i]-4.0f));\n",
        "  printf(\"Max error: %f\\n\", maxError);\n",
        "\n",
        "  // Liberação das áreas de memória alocadas da GPU\n",
        "  cudaFree(d_x);\n",
        "  cudaFree(d_y);\n",
        "  free(x);\n",
        "  free(y);\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaPMf1HuJFXi",
        "outputId": "28d19ae0-b580-4afd-b698-2e0317a744cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing saxpy.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc saxpy.cu -o saxpy -O3\n",
        "! nvprof ./saxpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ua9tpFeuJ-Jy",
        "outputId": "3118ae1b-b421-4b69-9a97-85e035442fa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==1209== NVPROF is profiling process 1209, command: ./saxpy\n",
            "Max error: 0.000000\n",
            "==1209== Profiling application: ./saxpy\n",
            "==1209== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   68.28%  1.5931ms         2  796.57us  786.46us  806.68us  [CUDA memcpy HtoD]\n",
            "                   29.72%  693.47us         1  693.47us  693.47us  693.47us  [CUDA memcpy DtoH]\n",
            "                    2.00%  46.624us         1  46.624us  46.624us  46.624us  saxpy(int, float, float*, float*)\n",
            "      API calls:   65.26%  243.79ms         2  121.89ms  122.91us  243.66ms  cudaMalloc\n",
            "                   33.66%  125.73ms         1  125.73ms  125.73ms  125.73ms  cudaLaunchKernel\n",
            "                    0.83%  3.1177ms         3  1.0392ms  984.12us  1.1224ms  cudaMemcpy\n",
            "                    0.18%  686.42us         2  343.21us  257.13us  429.29us  cudaFree\n",
            "                    0.06%  211.98us       114  1.8590us     179ns  77.730us  cuDeviceGetAttribute\n",
            "                    0.01%  19.525us         1  19.525us  19.525us  19.525us  cuDeviceGetPCIBusId\n",
            "                    0.00%  13.115us         1  13.115us  13.115us  13.115us  cuDeviceGetName\n",
            "                    0.00%  5.4680us         1  5.4680us  5.4680us  5.4680us  cuDeviceTotalMem\n",
            "                    0.00%  2.2200us         3     740ns     295ns  1.5600us  cuDeviceGetCount\n",
            "                    0.00%  1.3130us         2     656ns     307ns  1.0060us  cuDeviceGet\n",
            "                    0.00%     785ns         1     785ns     785ns     785ns  cuModuleGetLoadingMode\n",
            "                    0.00%     378ns         1     378ns     378ns     378ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nos resultados acima, somar os tempos para as operações de cópia de memória e o tempo para a execução da função do *kernel*.\n",
        "\n",
        "E.g.: 1.4568 + 0.4896 + 0.049151 = 1.995551 ms"
      ],
      "metadata": {
        "id": "4KFlNYZ7o1RN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile saxpy-uni.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__\n",
        "void saxpy(int n, float a, float *x, float *y)\n",
        "{\n",
        "  int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (i < n)\n",
        "    y[i] = a*x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20;\n",
        "  float *x, *y;\n",
        "\n",
        "  // alocação de memória na GPU\n",
        "  cudaMallocManaged(&x, N*sizeof(float));\n",
        "  cudaMallocManaged(&y, N*sizeof(float));\n",
        "\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Perform SAXPY on 1M elements\n",
        "  saxpy<<<(N+255)/256, 256>>>(N, 2.0f, x, y);\n",
        "\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = max(maxError, abs(y[i]-4.0f));\n",
        "  printf(\"Max error: %f\\n\", maxError);\n",
        "\n",
        "  // Liberação das áreas de memória alocadas da GPU\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNY6mUaOnUUB",
        "outputId": "32820ea0-d993-4e4e-f8ad-523af6d250bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting saxpy-uni.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc saxpy-uni.cu -o saxpy-uni -O3\n",
        "! nvprof ./saxpy-uni"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw7z5Gw7nfAS",
        "outputId": "72bbac8a-7b6a-40c2-da00-dcfececb5001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==2034== NVPROF is profiling process 2034, command: ./saxpy-uni\n",
            "Max error: 0.000000\n",
            "==2034== Profiling application: ./saxpy-uni\n",
            "==2034== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  2.8983ms         1  2.8983ms  2.8983ms  2.8983ms  saxpy(int, float, float*, float*)\n",
            "      API calls:   98.23%  211.08ms         2  105.54ms  39.164us  211.04ms  cudaMallocManaged\n",
            "                    1.36%  2.9218ms         1  2.9218ms  2.9218ms  2.9218ms  cudaDeviceSynchronize\n",
            "                    0.23%  486.53us         2  243.26us  214.53us  272.00us  cudaFree\n",
            "                    0.11%  229.43us         1  229.43us  229.43us  229.43us  cudaLaunchKernel\n",
            "                    0.07%  140.10us       114  1.2280us     136ns  61.471us  cuDeviceGetAttribute\n",
            "                    0.01%  11.880us         1  11.880us  11.880us  11.880us  cuDeviceGetName\n",
            "                    0.00%  5.3500us         1  5.3500us  5.3500us  5.3500us  cuDeviceGetPCIBusId\n",
            "                    0.00%  4.6400us         1  4.6400us  4.6400us  4.6400us  cuDeviceTotalMem\n",
            "                    0.00%  1.2720us         3     424ns     172ns     856ns  cuDeviceGetCount\n",
            "                    0.00%  1.0840us         2     542ns     225ns     859ns  cuDeviceGet\n",
            "                    0.00%     571ns         1     571ns     571ns     571ns  cuModuleGetLoadingMode\n",
            "                    0.00%     232ns         1     232ns     232ns     232ns  cuDeviceGetUuid\n",
            "\n",
            "==2034== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "     138  59.362KB  4.0000KB  952.00KB  8.000000MB  1.121912ms  Host To Device\n",
            "      24  170.67KB  4.0000KB  0.9961MB  4.000000MB  361.6310us  Device To Host\n",
            "      11         -         -         -           -  2.842922ms  Gpu page fault groups\n",
            "Total CPU Page faults: 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No caso de uso de memória unificada, os tempos de cópia de memória ficam embutidos no tempo de execução do *kernel*.\n",
        "\n",
        "Assim, no caso acima, T = 3.0153\n",
        "\n",
        "Resumidamente, o tempo foi um pouco pior para memória unificada. Para casos em que há um maior uso de dados dentro da GPU, contudo, essa diferença tende a desaparecer."
      ],
      "metadata": {
        "id": "S5BWxykRpqiT"
      }
    }
  ]
}