{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF76EjM3ZQ7r"
      },
      "source": [
        "# PPD: Programação com CUDA\n",
        "\n",
        "Hélio - DC/UFSCar - 2023"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-hierarchy"
      ],
      "metadata": {
        "id": "ib8YlJAMxwvM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4l3q9nxVrlW"
      },
      "source": [
        "# Gerenciamento de memória na GPU\n",
        "\n",
        "Um dos aspectos fundamentais na programação usando GPUs é a **comunicação CPU / GPU**. Mais especificamente, a cópia de dados da memória RAM para a memória do dispositivo, para que sejam acessados pelas *threads* dos *kernels*, e vice-versa, trazendo para a RAM os dados resultados das manipulações em GPU.\n",
        "\n",
        "Como veremos, essa transferência pode ser feita de maneira explícita, controlada pelo programa executando em CPU, ou pode ser feita de forma forma transparente, apoiada pelos mecanismos de endereçamento de E/S mapeada em memória. De todo modo, cabe ao programador especificar quais são as áreas de memória utilizadas na GPU.\n",
        "\n",
        "Mas por que é preciso preocupar-se com áreas de memória em GPU?\n",
        "\n",
        "A obtenção de dados de entrada a partir de arquivos, ou da rede, por exemplo, é feita pelo programa executando em CPU, colocando-os na memória RAM. Para processamento em GPU, contudo, esses dados precisam ser transferidos para a memória deste dispositivo.\n",
        "\n",
        "Antes de fazer isso, é preciso **reservar espaço** na memória da GPU.\n",
        "\n",
        "Tendo alocado os espaços, é possível **copiar** dados da memória RAM, comumente referida como memória do *host*, para a memória do dispositivo (*device*).\n",
        "\n",
        "Uma vez que novos dados tenham sido gerados pelo processamento em GPU, é preciso **copiar** os dados de interesse da memória deste dispositivo para a memória RAM, de forma que possam ser salvos em arquivos, transmitidos ou tratados de alguma outra forma.\n",
        "\n",
        "Na programação com CUDA, esse gerenciamento de memória pode ser feito de forma simplificada, com funções semelhantes à alocação e cópia em memória em C.\n",
        "\n",
        "\n",
        "1.   **Alocar espaço de memória na GPU**\n",
        "\n",
        "      cudaError_t [cudaMalloc](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g37d37965bfb4803b6d4e59ff26856356) ( void\\*\\* devPtr , size_t size)\n",
        "\n",
        "2.   **Transferir os dados entre CPU (RAM) e (memória da) GPU**\n",
        "\n",
        "     cudaError_t [cudaMemcpy](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1gc263dbe6574220cc776b45438fc351e8) ( void\\* **dst**, const void\\* **src**, size_t count, cudaMemcpyKind kind)\n",
        "\n",
        "3.   **Retornar os dados para a memória da CPU (RAM)**\n",
        "\n",
        "     cudaError_t **cudaMemcpy**( void\\* dst, const void\\* src, size_t count, cudaMemcpyKind kind)\n",
        "\n",
        "4.   **Liberar espaço de memória alocado na GPU**\n",
        "\n",
        "     cudaError_t **cudaFree**( void\\* ptr)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W0aAsCqHgDT"
      },
      "source": [
        "O exemplo a seguir, extraído de https://developer.nvidia.com/blog/easy-introduction-cuda-c-and-c, ilustra uma implementação do programa SAXPY\n",
        "(*Single-precision A * X Plus Y*), onde se pode ver a manipulação de memória com alocação, cópias de e para a GPU e liberação do espaço.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXhqwNlyyUJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3af00e2-523b-4f3b-da01-dcbba3575b5a"
      },
      "source": [
        "%%writefile saxpy.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__\n",
        "void saxpy(int n, float a, float *x, float *y)\n",
        "{\n",
        "  int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (i < n)\n",
        "    y[i] = a*x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20;\n",
        "  float *x, *y, *d_x, *d_y;\n",
        "\n",
        "  x = (float*)malloc(N*sizeof(float));\n",
        "  y = (float*)malloc(N*sizeof(float));\n",
        "\n",
        "  // alocação de memória na GPU\n",
        "  cudaMalloc(&d_x, N*sizeof(float));\n",
        "  cudaMalloc(&d_y, N*sizeof(float));\n",
        "\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Cópia dos dados da memória RAM para a memória do dispositivo\n",
        "  cudaMemcpy(d_x, x, N*sizeof(float), cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_y, y, N*sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "  // Perform SAXPY on 1M elements\n",
        "  saxpy<<<(N+255)/256, 256>>>(N, 2.0f, d_x, d_y);\n",
        "\n",
        "  // Cópia dos dados da memória da GPU para a memória RAM\n",
        "  cudaMemcpy(y, d_y, N*sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = max(maxError, abs(y[i]-4.0f));\n",
        "  printf(\"Max error: %f\\n\", maxError);\n",
        "\n",
        "  // Liberação das áreas de memória alocadas da GPU\n",
        "  cudaFree(d_x);\n",
        "  cudaFree(d_y);\n",
        "  free(x);\n",
        "  free(y);\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting saxpy.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc saxpy.cu -o saxpy -O3\n",
        "! ./saxpy"
      ],
      "metadata": {
        "id": "qTnOBc9Hff_E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5707456d-bcd6-4be0-e9a1-b9e830c2e507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max error: 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwhHv5cd1Ykz"
      },
      "source": [
        "# Alocação de memória na GPU\n",
        "\n",
        "Há vários aspectos a notar no programa acima. Primeiro, com relação à **alocação** e à **liberação** de área de memória no dispositivo, vê-se as funções [cudaMalloc](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g37d37965bfb4803b6d4e59ff26856356) (22 e 23) e, ao final do programa (46 e 47) chamadas a **cudaFree**, cuja utilização é bem semelhante ao que ocorre com *malloc* e *free* na alocação dinâmica em C.\n",
        "\n",
        "<br>\n",
        "\n",
        "# Nomeação de variáveis\n",
        "\n",
        "A **nomeação** das variáveis também merece um comentário. Por questões de organização do código, é comum que as variáveis que vão referir-se a espaços de **endereçamento dentro do dispositivo** (GPU) sejam nomeadas com o prefixo **d_**. Isso não é obrigatório, mas é bastante comum na programação com aceleradores. Nessa mesma lógica, alguns programadores costumam nomear as variáveis correspondentes do programa em CPU com o prefixo **h_**.\n",
        "\n",
        "<br>\n",
        "\n",
        "# Cópia de dados CPU / GPU\n",
        "\n",
        "Passando à atribuição de valores às variáveis e estruturas que serão manipuladas em GPU, nas linhas 31, 32 e 38, vê-se exemplos de uso da função [cudaMemcpy](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1gc263dbe6574220cc776b45438fc351e8).\n",
        "\n",
        "```c\n",
        "__host__ ​cudaError_t cudaMemcpy ( void* dst, const void* src, size_t count, cudaMemcpyKind kind )\n",
        "```\n",
        "* **dst**: endereço de memória destino\n",
        "* **src**: endereço de memória de origem\n",
        "* **count**: número de bytes a copiar\n",
        "* **kind**: tipo (sentido) da transferência (comumente ***cudaMemcpyHostToDevice*** ou ***cudaMemcpyDeviceToHost***)\n",
        "\n",
        "Como se pode observar, o primeiro parâmetro é o **destino** da cópia, seguido da **origem** e o número de bytes a copiar. Já o parâmetro *kind* indica o **sentido** da transferência, comumente indicando cópia do *host* (RAM) para o dispositivo (*device*), ou vice-versa.\n",
        "\n",
        "<br>\n",
        "\n",
        "Neste exemplo específico, há inicialmente a transferência (cópia) do conteúdo dos 2 vetores da memória RAM para a área pré-alocada na GPU (31 e 32) e, ao final do processamento na GPU, a cópia apenas do vetor modificado na GPU para a memória RAM (38). É claro que não é preciso copiar de volta o vetor x neste caso, já que ele não foi alterado na GPU.\n",
        "\n",
        "É importante ressaltar que essas operações de transferência entre áreas de memória do *host* e do *device* são [síncronas](https://docs.nvidia.com/cuda/cuda-runtime-api/api-sync-behavior.html#api-sync-behavior__memcpy-sync). Ou seja, o código só passa para a linha seguinte após a transferência ser realizada.\n",
        "\n",
        "Há uma série de outras funções **cuda** para [gerenciamento de memória](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html), incluindo suporte para transfências bi-dimensionais e assíncronas ([cudaMemcpyAsync](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g85073372f776b4c4d5f89f7124b7bf79)).\n",
        "\n",
        "<br>\n",
        "\n",
        "# Passagem de parâmetros para a função do *kernel*\n",
        "\n",
        "Outro aspecto fundamental a observar neste exemplo é a **passagem de parâmetros** para a função do *kernel*.\n",
        "\n",
        "```c\n",
        "   // Declaração da função\n",
        "   __global__ void saxpy(int n, float a, float *x, float *y)\n",
        "  ...\n",
        "  // invocação da função como um kernel\n",
        "  saxpy<<<(N+255)/256, 256>>>(N, 2.0f, d_x, d_y);\n",
        "```\n",
        "Como se vê, a função *saxpy* tem 4 parâmetros, sendo um valor inteiro, um valor em ponto flutuante e dois ponteiros para valores em ponto flutuante, os vetores neste caso.\n",
        "\n",
        "A passagem de **valores** (fixos ou contidos em variáveis) na invocação de um *kernel* é simples e é resolvida pelos mecanismos de CUDA. Já a passagem de ponteiros requer atenção. É claro que não adiantaria passar à função do *kernel* endereços no espaço de endereçamento da memória RAM (\\*).\n",
        "\n",
        "    (*) salvo se tratar-se de espaço de memória do dispositivo mapeado em RAM, o que estudaremos posteriormente.\n",
        "\n",
        "Assim, neste exemplo, os endereços que são passados como parâmetro são os endereços que alocamos anteriormente na memória do dispositivo, com cudaMalloc!\n",
        "\n",
        "# Ativação do *kernel*\n",
        "\n",
        "Ainda na ativação do *kernel*, antes dos parâmetros é definida a organização das *threads* que serão usadas na execução do código.\n",
        "\n",
        "Neste caso, optou-se por blocos com 256 *threads*. Assim, dado que são 1M elementos (1\\<\\<20), o número de blocos pode ser calculado dividindo-se (N+255) por 256.\n",
        "\n",
        "```c\n",
        "  saxpy<<<(N+255)/256, 256>>>( ... );\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i0xp9qEySBb"
      },
      "source": [
        "# CUDA Unified Memory\n",
        "\n",
        "Como uma evolução do modelo de memória e da comunicação *host* / *device*, a versão 6 de CUDA implantou o suporte para o que é chamado de [*Unified Memory*](https://developer.nvidia.com/blog/unified-memory-in-cuda-6/). \\([*](https://developer.nvidia.com/blog/unified-memory-cuda-beginners/)\\)\n",
        "\n",
        "Fisicamente, GPUs são conectadas ao sistema hospedeiro através de algum barramento, tipicamente o PCI-Express. As áreas de memória acessíveis diretamente por CPUs e pelos processadores da GPU são distintas, sendo a RAM para as CPUs e a memória presente na GPU para os processadores deste dispositivo.\n",
        "\n",
        "Assim, uma estratégia comum de programação em GPUs consiste em alocar espaços de memória em GPU, copiar dados necessários na memória deste dispositivo e copiar de volta à RAM dados relevantes que foram produzidos pelo dispositivo.\n",
        "\n",
        "Já o mecanismo de memória unificada (*unified memory*) de CUDA cria um conjunto de áreas de memória gerenciadas que são compartilhadas entre CPU e GPU. Uma área de memória alocada com esse mecanismo é **acessível diretamente**, tanto pelo código em CPU quanto pelo código em GPU, usando **o mesmo ponteiro**.\n",
        "\n",
        "De maneira simplificada, para alocar áreas de memória (variáveis) compartilhadas entre CPU e GPU, basta usar a função [cudaMallocManaged](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1gd228014f19cc0975ebe3e0dd2af6dd1b)().\n",
        "\n",
        "```c\n",
        "char *data;\n",
        "cudaMallocManaged(&data, N);\n",
        "```\n",
        "\n",
        "Os dados alocados em áreas de memória unificada são automaticamente migrados entre a memória RAM e a memória do dispositivo, de forma que cada código (de CPU ou de GPU) veja esses dados como se fossem locais.\n",
        "\n",
        "<br>\n",
        "\n",
        "[1] https://developer.nvidia.com/blog/unified-memory-cuda-beginners/\n",
        "\n",
        "    When code running on a CPU or GPU accesses data allocated this way (often called CUDA managed data),\n",
        "    the CUDA system software and/or the hardware takes care of migrating memory pages to the memory of the\n",
        "    accessing processor.\n",
        "\n",
        "<br>\n",
        "\n",
        "Vejamos uma ilustração dos 2 modelos de acesso à memória:\n",
        "\n",
        " <img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2013/11/unified_memory.png\">\n",
        "\n",
        "Como resultado, simplifica-se o uso de memória e das interações CPU/GPU em aplicações CUDA.\n",
        "\n",
        "A figura a seguir apresenta 2 versões de um código que realiza a leitura de dados de arquivo, chama uma função de ordenação dos dados e os manipula, antes de gravar os dados em arquivo novamente.\n",
        "\n",
        "<img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2013/11/simplified_memory_mananagement_code-e1384437984510.png\">\n",
        "\n",
        "A alocação dos dados é feita via [cudaMallocManaged](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1gd228014f19cc0975ebe3e0dd2af6dd1b)(), de forma que os dados alocados são acessíveis manipulando os mesmos ponteiros tanto no programa em CPU quanto na GPU.\n",
        "\n",
        "Uma **etapa a mais** necessária neste caso é a **sincronização** após a ativação do *kernel*, para garantir que as operações que manipulam os dados já foram concluídas, evitando transferências de dados que ainda estão em uso na GPU.\n",
        "\n",
        "Isso é feito com a chamada [cudaDeviceSynchronize](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g10e20b05a95f638a4071a655503df25d)(), que bloqueia até que o dispositivo tenha concluído todas as operações requisitadas.\n",
        "\n",
        "Toda a complexidade da migração dos dados é tratada por CUDA.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vejamos um exemplo de código, apresentado em \\[[1](https://developer.nvidia.com/blog/unified-memory-cuda-beginners/)\\] que realiza a soma de 2 vetores, declarados como memória unificada.\n",
        "\n",
        "\\[1\\] https://developer.nvidia.com/blog/unified-memory-cuda-beginners/"
      ],
      "metadata": {
        "id": "frTSs82bZF0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    %%writefile uni-sum.cu\n",
        "\n",
        "    #include <iostream>\n",
        "    #include <math.h>\n",
        "\n",
        "    // CUDA kernel to add elements of two arrays\n",
        "    __global__\n",
        "    void add(int n, float *x, float *y)\n",
        "    {\n",
        "      int index = blockIdx.x * blockDim.x + thr:eadIdx.x;\n",
        "      int stride = blockDim.x * gridDim.x;\n",
        "      for (int i = index; i < n; i += stride)\n",
        "        y[i] = x[i] + y[i];\n",
        "    }\n",
        "\n",
        "    int main(void)\n",
        "    {\n",
        "      int N = 1<<20;\n",
        "      float *x, *y;\n",
        "\n",
        "      // Allocate Unified Memory -- accessible from CPU or GPU\n",
        "      cudaMallocManaged(&x, N*sizeof(float));\n",
        "      cudaMallocManaged(&y, N*sizeof(float));\n",
        "\n",
        "      // initialize x and y arrays on the host\n",
        "      for (int i = 0; i < N; i++) {\n",
        "        x[i] = 1.0f;\n",
        "        y[i] = 2.0f;\n",
        "      }\n",
        "\n",
        "      // Launch kernel on 1M elements on the GPU\n",
        "      int blockSize = 256;\n",
        "      int numBlocks = (N + blockSize - 1) / blockSize;\n",
        "      add<<<numBlocks, blockSize>>>(N, x, y);\n",
        "\n",
        "      // Wait for GPU to finish before accessing on host\n",
        "      cudaDeviceSynchronize();\n",
        "\n",
        "      // Check for errors (all values should be 3.0f)\n",
        "      float maxError = 0.0f;\n",
        "      for (int i = 0; i < N; i++)\n",
        "        maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "      std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "      // Free memory\n",
        "      cudaFree(x);\n",
        "      cudaFree(y);\n",
        "\n",
        "      return 0;\n",
        "    }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64zNZeZLZEK6",
        "outputId": "532886f9-e9f6-4606-aed2-e38954896e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing uni-sum.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! if [ ! uni-sum -nt uni-sum.cu ]; then nvcc uni-sum.cu -o uni-sum -O3; fi\n",
        "! ./uni-sum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QA-Lrv5Z7jB",
        "outputId": "ea867d09-12b8-445f-ff2d-51317b65d2a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max error: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aspectos do uso de *unified memory*\n",
        "\n",
        "Há 2 aspectos principais no uso de áreas de memória unificadas. Primeiro, o modelo de programação torna-se mais simples, sem ter que incorporar as cópias de dados entre CPU (RAM) e GPU.\n",
        "\n",
        "Outro aspecto é que a cópia dos dados entre CPU e GPU sob demanda provê a localidade dos acessos e os ganhos de desempenho que isso proporciona, de forma transparente para a aplicação. O uso de memórias de alta velocidade, contudo, é um requisito para que o mecanismo de transferências sob demanda consiga prover todo o desempenho de GPUs modernas.\n",
        "\n",
        "<br>\n",
        "\n",
        "### Memory-mapped I/O\n",
        "\n",
        "Cabe lembrar, contudo, dos mecanismos de acesso direto à memória (DMA) e de mapeamentos de espaços de endereçamentos de memória para entrada e saída ([*memory mapped I/O*](https://en.wikipedia.org/wiki/Memory-mapped_I/O)).\n",
        "\n",
        "*Memory-mapped I/O* usa o **mesmo espaço de endereçamento** que é usado para acessar a memória RAM para acessar também o conteúdo de registradores de controle e de áreas de memória dentro de dispositivos de E/S. Assim, as mesmas instruções que fazem acesso à memória podem ser usadas para o acesso a dispositivos, sem modificações no código.\n",
        "\n",
        "Isso é possível porque mecanismos providos pelo sistema de interligação de CPUs, memória e dispositivos de E/S no computador permitem a **reserva de faixas de endereços** que levariam à memória para uso em transferências para controladores específicos interligados aos barramentos PCI-Express, por exemplo.\n",
        "\n",
        "Feito isso, cabe a cada dispositivo de E/S  monitorar os endereços nos acessos ao barramento que leva à memória e tratar as transferências aos endereços que reservou.\n",
        "\n",
        "Há ainda questões relacionadas ao mapeamento de páginas virtuais associadas às áreas de memória reservadas para as variáveis alocadas com o modo unificado.\n",
        "\n",
        "<br>\n",
        "\n",
        "[1] https://developer.nvidia.com/blog/unified-memory-cuda-beginners/\n",
        "\n",
        "    On Pascal and later GPUs, managed memory may not be physically allocated when cudaMallocManaged() returns;\n",
        "    it may only be populated on access (or prefetching). In other words, pages and page table entries\n",
        "    may not be created until they are accessed by the GPU or the CPU. The pages can migrate to any processor’s\n",
        "    memory at any time, and the driver employs heuristics to maintain data locality and prevent excessive page faults.\n",
        "\n",
        "<br>\n",
        "\n",
        "### Aspectos de desempenho nos modelos de acesso à memória pela GPU\n",
        "\n",
        "Embora possa prover transferências em altas taxas e de maneira transparente para a aplicação, o **modelo de memória unificada talvez perca em desempenho para programas que usam o modelo de alocação original mas que conhecem seus padrões de acesso à memória e que usem estratégias como a sobreposição de operações de transferência de dados com processamento**.\n",
        "\n",
        "Outras considerações sobre desempenho e aspectos do funcionamento do mecanismo de mapeamento de memória podem ser vistas em https://developer.nvidia.com/blog/unified-memory-cuda-beginners/ e em https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/.\n",
        "\n",
        "Entre outros aspectos ressaltados por essas referências está a constatação que para maximizar o desempenho, os dados devem ser mantidos o mais próximo possível da GPU.\n",
        "\n",
        "Em situações em que o volume de dados a manipular é maior do que o espaço de endereçamento da GPU, o uso de memória unificada simplifica os acessos, deixando ao sistema o gerenciamento de quais partes dos dados vão estar efetivamente copiados na memória da GPU. Isso nem sempre pode prover melhor desempenho do que deixar ao programador copiar explicitamente na GPU os dados que serão manipulados.\n"
      ],
      "metadata": {
        "id": "oBAC9qdaZqoJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oBAkRrEO2HP"
      },
      "source": [
        "# Suporte a *managed memory*\n",
        "\n",
        "Para saber se uma GPU tem suporte à alocação de memória no modelo unificado, com cudaMallocManaged, é possível analisar as propriedades da GPU. Essas informações podem ser obtidas com a chamada [cudaGetDeviceProperties](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g1bf9d625a931d657e08db2b4391170f0) e observando o campo managedMemory.\n",
        "\n",
        "```c\n",
        "struct cudaDeviceProp {\n",
        "  char name[256];\n",
        "  ...\n",
        "  int cudaDeviceProp::managedMemory; // Device supports allocating managed memory on this system\n",
        "  ...\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2NoqsXWPaTz",
        "outputId": "4335fdfe-f10e-49fd-b88b-e6c7f5d14810"
      },
      "source": [
        "%%writefile man-mem.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "\tcudaSetDevice(0);\n",
        "\tcudaDeviceProp prop;\n",
        "\tcudaGetDeviceProperties(&prop,0);\n",
        "\n",
        " \tprintf(\"Modelo do Device: %s\\n\",prop.name);\n",
        "  printf(\"Número de SMs: %d\\n\",prop.multiProcessorCount);\n",
        "\n",
        "  printf(\"managedMemory: %d\\n\",prop.managedMemory);\n",
        "\n",
        "\treturn 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing man-mem.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEUa3DjoQL96",
        "outputId": "e8c901a1-5c21-444b-fdf6-07437d5fee70"
      },
      "source": [
        "! if [ ! man-mem -nt man-mem.cu ]; then nvcc man-mem.cu -o man-mem  -Wno-deprecated-gpu-targets -gencode=arch=compute_37,code=sm_37 ; fi\n",
        "! ./man-mem"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo do Device: Tesla T4\n",
            "Número de SMs: 40\n",
            "managedMemory: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exemplo de programa CUDA usando cudaMallocManaged\n"
      ],
      "metadata": {
        "id": "9a1tWngAeoPM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUs3ioGNIL4V"
      },
      "source": [
        "O exemplo a seguir, extraído de https://developer.nvidia.com/blog/even-easier-introduction-cuda, ilustra o funcionamento dos mecanismos de alocação e cópia em memória envolvendo a GPU, com *Unified Memory*.\n",
        "\n",
        "Neste exemplo, é feita a soma sequencial, em CPU, de 1M elementos de 2 vetores arnazenados em memória RAM.\n",
        "\n",
        "A primeira versão do programa é um código sequencial em CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vlour_emVwUS",
        "outputId": "eb9a632e-3334-47d8-dd91-efbdb6f1daed"
      },
      "source": [
        "%%writefile vec-sum.c\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "// function to add the elements of two arrays\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  for (int i = 0; i < n; i++)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20; // 1M elements\n",
        "\n",
        "  float *x = malloc (N * sizeofapresenta(float));\n",
        "  float *y = malloc (N * sizeof(float));\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the CPU\n",
        "  add(N, x, y);\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "\n",
        "  printf(\"Max error: %f\\n\", maxError);\n",
        "\n",
        "  // Free memory\n",
        "  free(x);\n",
        "  free(y);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting vec-sum.c\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRI1zByWWAM1"
      },
      "source": [
        "# ! gcc -Wall vec-sum.c -o vec-sum -lm && ./vec-sum\n",
        "! if [ ! vec-sum -nt vec-sum.c ]; then gcc -Wall vec-sum.c -o vec-sum -lm ; fi\n",
        "! ./vec-sum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf0Ge3fzc6UC"
      },
      "source": [
        "Já dá para supor que, com muitos processadores operando em paralelo na execução do mesmo código, deve ser vantajoso gerar uma versão deste código para GPU.\n",
        "\n",
        "Um primeiro passo para isso é gerar uma versão da função de soma na forma de um *kernel*, incluindo o prefixo **\\_\\_global\\_\\_**.\n",
        "\n",
        "```c\n",
        "// CUDA Kernel para a adição dos elementos de 2 vetores em GPU\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  for (int i = 0; i < n; i++)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "```\n",
        "\n",
        "Outro aspecto necessário é a alocação de memória na GPU para conter os vetores que serão somados.\n",
        "\n",
        "O modelo de memória unificado (*Unified Memory*) em CUDA provê um espaço de endereçamento único que é acessível por todas as GPUs e CPUs do sistema. A alocação de memória nesse espaço é feita com a função [cudaMallocManaged](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1gd228014f19cc0975ebe3e0dd2af6dd1b), que permite que o espaço alocado seja acessível pelo código no *host* e pelo código na GPU. A posterior liberação deste espaço é feita com a chamada cudaFree().\n",
        "\n",
        "Desta forma, basta substituir as funções de alocação e liberação de memória no código:\n",
        "```c\n",
        "  // Aloca memória Unificada, acessível por GPUs e CPUs\n",
        "  float *x, *y;\n",
        "  cudaMallocManaged(&x, N*sizeof(float));\n",
        "  cudaMallocManaged(&y, N*sizeof(float));\n",
        "\n",
        "  ...\n",
        "\n",
        "  // Free memory\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "```\n",
        "\n",
        "Para a execução do código em GPU, a chamada da função dá origem à invocação do *kernel*.\n",
        "\n",
        "Um aspecto relevante na ativação do *kernel* é a seleção do número de *threads* que serão usadas na execução.\n",
        "\n",
        "Do modo em que está implementada a função de soma como um *kernel*, todas as iterações serão feitas pela *thread* que a executar.\n",
        "Assim, no exemplo a seguir, o *kernel* é ativado com apenas 1 *thread*. O número de blocos é especificado em 1, com apenas 1 thread por bloco.\n",
        "\n",
        "```c\n",
        "add<<<1, 1>>>(N, x, y);\n",
        "```\n",
        "É claro, contudo que, deste modo, não estamos explorando o potencial de paralelismo dos múltiplos processadores da GPU. Idealmente, devemos considerar blocos de processadores atuando em paralelo, dividindo as iterações do laço na função do kernel. Havendo *cores* suficientes na GPU, cada umd deles ficaria encarregado de manipular um único elemento daquele laço (*for*). Trataremos disso posteriormente.\n",
        "\n",
        "Outro aspecto a observar é o assincronismo na execução dos *kernels* pela GPU. Uma vez ativado o *kernel*, a execução do código prossegue na CPU. Assim, antes de poder acessar na memória os dados que serão produzidos pelo código da GPU, é preciso executar uma operação de sincronização.\n",
        "\n",
        "Isso pode ser feito com a chamada cudaDeviceSynchronize().\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sznks9RLvHMI",
        "outputId": "6d1fd3f6-0a28-4d81-ca75-d49c4e09b30b"
      },
      "source": [
        "# %%cu\n",
        "%%writefile gpu-sum.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <math.h>\n",
        "\n",
        "// Kernel function to add the elements of two arrays\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  for (int i = 0; i < n; i++)\n",
        "    y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20;\n",
        "  float *x, *y;\n",
        "\n",
        "  // Allocate Unified Memory – accessible from CPU or GPU\n",
        "  cudaMallocManaged(&x, N*sizeof(float));\n",
        "  cudaMallocManaged(&y, N*sizeof(float));\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the GPU\n",
        "  add<<<1, 1>>>(N, x, y);\n",
        "\n",
        "  // Wait for GPU to finish before accessing on host\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "\n",
        "  printf(\"Max error: %f\\n\", maxError);\n",
        "\n",
        "  // Free memory\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing gpu-sum.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! if [ ! gpu-sum -nt gpu-sum.cu ]; then nvcc gpu-sum.cu -o gpu-sum  -Wno-deprecated-gpu-targets -gencode=arch=compute_37,code=sm_37 ; fi\n",
        "! ./gpu-sum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOomB0HXnFQo",
        "outputId": "3bc6d202-3ce4-4f5e-b017-53cc37072522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max error: 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sobre acesso aos dados com *unified memory*\n",
        "\n",
        "https://www.cs.ucr.edu/~amazl001/teaching/cs147/S21/slides/11-Pinned_UnifiedMemory.pdf\n",
        "\n",
        "UNIFIED MEMORY ON PRE-PASCAL\n",
        "\n",
        "Code example explained\n",
        "```c\n",
        "cudaMallocManaged(&ptr, . . . ) ;     // Pages are populated in GPU memory\n",
        "*ptr = 1;                             // CPU page fault: data migrates to CPU\n",
        "qsort<<<...>>>(ptr);                  // Kernel launch: data migrates to GPU\n",
        "```\n",
        "* GPU always has address translation during the kernel execution\n",
        "* Pages allocated before they are used – cannot oversubscribe GPU\n",
        "* Pages migrate to GPU only on kernel launch – cannot migrate on-demand\n",
        "\n",
        "**Kernel launch triggers bulk page migrations**\n",
        "\n",
        "<br>\n",
        "\n",
        "UNIFIED MEMORY ON PASCAL\n",
        "\n",
        "Now supports GPU page faults\n",
        "```c\n",
        "cudaMallocManaged(&ptr, . . . ) ;   // Empty, no pages anywhere (similar to malloc)\n",
        "*ptr = 1;                           // CPU page fault: data allocates on CPU\n",
        "qsort<<<...>>>(ptr);                // GPU page fault: data migrates to GPU\n",
        "```\n",
        "* If GPU does not have a VA translation, it issues an interrupt to CPU\n",
        "* Unified Memory driver could decide to map or migrate depending on heuristics\n",
        "* Pages populated and data migrated on first touch\n",
        "\n",
        "**True on-demand page migrations**\n",
        "\n",
        "<br>\n",
        "\n",
        "Pela descrição a seguir, vê-se que GPUs a partir da linha Pascal possuem um mecanismo que detecta falta de páginas na própria GPU e é capaz de parar a execução do kernel até que os dados (páginas de memória) necessários sejam carregados para o dispositivo.\n",
        "\n",
        "Assim, não há uma cópia automática dos dados antes da invocação do *kernel*, mas essa transferência ocorre sob demanda ao ser identificada uma falta de página. Deste modo, o tempo de execução do *kernel* é maior, ao incluir o tempo de transferência.\n",
        "\n",
        "<br>\n",
        "\n",
        "\\[1\\] https://developer.nvidia.com/blog/unified-memory-cuda-beginners/\n",
        "\n",
        "*Unlike the pre-Pascal GPUs, the Tesla P100 supports hardware page faulting and migration. So in this case the runtime doesn’t automatically copy all the pages back to the GPU before running the kernel. The kernel launches without any migration overhead, and when it accesses any absent pages, the GPU stalls execution of the accessing threads, and the Page Migration Engine migrates the pages to the device before resuming the threads.*\n",
        "\n",
        "*This means that the cost of the migrations is included in the kernel run time when I run my program on the Tesla P100 (2.1192 ms). In this kernel, every page in the arrays is written by the CPU, and then accessed by the CUDA kernel on the GPU, causing the kernel to wait on a lot of page migrations. That’s why the kernel time measured by the profiler is longer on a Pascal GPU like Tesla P100.*\n",
        "\n",
        "<br>\n",
        "\n",
        "## O que pode ser feito para melhorar o desempenho ao usar *unified memory*\n",
        "\n",
        "Ainda segundo a referência anterior \\[1\\], algumas estratégias podem ser adotadas para que o mecanismo de transferência dos dados via falta de páginas durante a execução do *kernel* não prejudique seu desempenho:\n",
        "\n",
        "* Realizar a inicialização dos dados dos vetores na GPU, o que pode ser feito por uma outra função de *kernel*. Neste caso, a área de memória (páginas) já seria alocada diretamente na GPU, sem necessidade de transferência no início da execução do *kernel* manipula os dados;\n",
        "\n",
        "```c\n",
        "__global__ void init(int n, float *x, float *y) {\n",
        "  int index = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "  for (int i = index; i < n; i += stride) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "* Realizar múltiplas execuções da função do kernel e calcular os valores médios e mínimo;\n",
        "\n",
        "* Realizar o carregamento prévio dos dados na memória da GPU (*prefetching*) usando uma chamada específica para isso ([cudaMemPrefetchAsync](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1ge8dc9199943d421bc8bc7f473df12e42)).\n",
        "\n",
        "```c\n",
        "  // Prefetch the data to the GPU\n",
        "  int device = -1;\n",
        "  cudaGetDevice(&device);\n",
        "\n",
        "  cudaMemPrefetchAsync(x, N*sizeof(float), device, NULL);\n",
        "  cudaMemPrefetchAsync(y, N*sizeof(float), device, NULL);\n",
        "\n",
        "  // Run kernel on 1M elements on the GPU\n",
        "  int blockSize = 256;\n",
        "  int numBlocks = (N + blockSize - 1) / blockSize;\n",
        "\n",
        "  saxpy<<<numBlocks, blockSize>>>(N, 1.0f, x, y);\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "## Cuidado com concorrência nos acessos aos dados pelo código em CPU e em *kernels* na GPU\n",
        "\n",
        "Apesar do mecanismo eficiente de detecção de faltas de páginas nas GPUs pós-Pascal, com capacidade de transferir páginas de memória com dados sob demanda, é claro que o acesso concorrente às áreas em memória unificada, pelo programa em CPU e por *kernel* em execução na GPU pode gerar problemas.\n",
        "\n",
        "Ao tentar acessar na CPU dados em uso pelo *kernel* em execução, o programa causará uma falha de segmentação (*segmentation fault*).\n",
        "\n",
        "<br>\n",
        "\n",
        "## Sobre os benefícios de usar a memória unificada em GPUs Pascal e posteriores\n",
        "\n",
        "Ainda segundo \\[1], dada a eficiência do mecanismo de paginação na GPU, com transmissões mediante faltas de páginas, é possível que a estratégia de uso de memória unificada sirva para criar programas que requerem áreas de dados maiores do que o espaço disponível na GPU.\n",
        "\n",
        "Além disso, para padrões de acesso a posições esparsas dos dados, o carregamento das páginas sob demanda pode ser mais eficiente do que carregar todo o conjunto de dados para a memória da GPU.\n",
        "\n",
        "<br>\n",
        "\n",
        "*The Benefits of Unified Memory on Pascal and Later GPUs*\n",
        "\n",
        "*Starting with the Pascal GPU architecture, Unified Memory functionality is significantly improved with 49-bit virtual addressing and on-demand page migration. 49-bit virtual addresses are sufficient to enable GPUs to access the entire system memory plus the memory of all GPUs in the system. The Page Migration engine allows GPU threads to fault on non-resident memory accesses so the system can migrate pages on demand from anywhere in the system to the GPU’s memory for efficient processing.*\n",
        "\n",
        "*In other words, Unified Memory transparently enables oversubscribing GPU memory, enabling out-of-core computations for any code that is using Unified Memory for allocations (e.g. cudaMallocManaged()). It “just works” without any modifications to the application, whether running on one GPU or multiple GPUs.*\n",
        "...\n",
        "*Demand paging can be particularly beneficial to applications that access data with a sparse pattern. In some applications, it’s not known ahead of time which specific memory addresses a particular processor will access. Without hardware page faulting, applications can only pre-load whole arrays, or suffer the cost of high-latency off-device accesses (also known as “Zero Copy”). But page faulting means that only the pages the kernel accesses need to be migrated.*\n"
      ],
      "metadata": {
        "id": "5ZCcBGkexauy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ... work in progress... sobre pilha, strack frame e afins..."
      ],
      "metadata": {
        "id": "ygemgGqk5icF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sobre pilha, recursividade e afins...\n",
        "\n",
        "https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#call-stack\n",
        "\n",
        "\n",
        "On devices of compute capability 2.x and higher, the size of the call stack can be queried usingcudaDeviceGetLimit() and set using cudaDeviceSetLimit().\n",
        "\n",
        "When the call stack overflows, the kernel call fails with a stack overflow error if the application is run via a CUDA debugger (CUDA-GDB, Nsight) or an unspecified launch error, otherwise. When the compiler cannot determine the stack size, it issues a warning saying Stack size cannot be statically determined. This is usually the case with recursive functions. Once this warning is issued, user will need to set stack size manually if default stack size is not sufficient.\n",
        "\n",
        "<br>\n",
        "\n",
        "https://docs.nvidia.com/cuda/cuda-c-programming-guide/index,html#configuration-options\n",
        "\n",
        "<br>\n",
        "\n",
        "https://forums.developer.nvidia.com/t/what-is-the-maximum-cuda-stack-frame-size-per-kerenl/31449\n",
        "\n",
        "<br>\n",
        "\n",
        "https://stackoverflow.com/questions/74597338/how-is-stack-frame-managed-within-a-thread-in-cuda\n",
        "\n",
        "<br>\n",
        "\n",
        "How is stack frame managed within a thread in Cuda?\n",
        "\n",
        "Suppose we have a kernel that invokes some functions, for instance:\n",
        "```c\n",
        "__device__ int fib(int n) {\n",
        "    if (n == 0 || n == 1) {\n",
        "        return n;\n",
        "    } else {\n",
        "        int x = fib(n-1);\n",
        "        int y = fib(n-2);\n",
        "        return x + y;\n",
        "    }\n",
        "    return -1;\n",
        "}\n",
        "\n",
        "__global__ void fib_kernel(int* n, int *ret) {\n",
        "    *ret = fib(*n);\n",
        "}\n",
        "```\n",
        "\n",
        "The kernel fib_kernel will invoke the function fib(), which internally will invoke two fib() functions. Suppose the GPU has 80 SMs, we launch exactly 80 threads to do the computation, and pass in n as 10. I am aware that there will be a ton of duplicated computations which violates the idea of data parallelism, but I would like to better understand the stack management of the thread.\n",
        "\n",
        "According to the Documentation of Cuda PTX, it states the following:\n",
        "\n",
        "    the GPU maintains execution state per thread, including a program counter and call stack\n",
        "\n",
        "    The stack locates in local memory. As the threads executing the kernel, do they behave just like the calling convention in CPU? In other words, is it true that for each thread, the corresponding stack will grow and shrink dynamically?\n",
        "\n",
        "    The stack of each thread is private, which is not accessible by other threads. Is there a way that I can manually instrument the compiler/driver, so that the stack is allocated in global memory, no longer in local memory?\n",
        "\n",
        "    Is there a way that allows threads to obtain the current program counter, frame pointer values? I think they are stored in some specific registers, but PTX documentation does not provide a way to access those. May I know what I have to modify (e.g. the driver or the compiler) to be able to obtain those registers?\n",
        "\n",
        "    If we increase the input to fib(n) to be 10000, it is likely to cause stack overflow, is there a way to deal with it? The answer to question 2 might be able to address this. Any other thoughts would be appreciated.\n",
        "\n",
        "    cudagpudriver\n",
        "\n",
        "asked Nov 28, 2022 at 7:33\n",
        "Ethan L.\n",
        "\n",
        "    Local memory is physically located in global memory –\n",
        "    Abator Abetor\n",
        "    Nov 28, 2022 at 8:40\n",
        "    You could just compute fib iteratively, without worrying about stack size –\n",
        "    Abator Abetor\n",
        "    Nov 28, 2022 at 8:41\n",
        "\n",
        "    Yeah, but local memory is not accessible by other threads. Is there a way to copy the stack frame in local memory and place it in global memory? Or we just modify the driver/compiler (not sure what has to be modified) so that the stack frame for each thread is allocated in global memory and is accessible by all other threads. This might lead to security issues but it is not that danger in terms of doing computation. –\n",
        "    Ethan L.\n",
        "    Nov 28, 2022 at 8:45\n",
        "    With a debugger you can read the local memory of each thread, while single-stepping through the CUDA program. –\n",
        "    Sebastian\n",
        "    Nov 28, 2022 at 22:46\n",
        "\n",
        "1 Answer\n",
        "\n",
        "You'll get a somewhat better idea of how these things work if you study the generated SASS code from a few examples.\n",
        "\n",
        "    As the threads executing the kernel, do they behave just like the calling convention in CPU? In other words, is it true that for each thread, the corresponding stack will grow and shrink dynamically?\n",
        "\n",
        "The CUDA compiler will aggressively inline functions when it can. When it can't, it builds a stack-like structure in local memory. However the GPU instructions I'm aware of don't include explicit stack management (e.g. push and pop, for example) so the \"stack\" is \"built by the compiler\" with the use of registers that hold a (local) address and LD/ST instructions to move data to/from the \"stack\" space. In that sense, the actual stack does/can dynamically change in size, however the maximum allowable stack space is limited. Each thread has its own stack, using the definition of \"stack\" given here.\n",
        "\n",
        "    Is there a way that I can manually instrument the compiler/driver, so that the stack is allocated in global memory, no longer in local memory?\n",
        "\n",
        "Practically, no. The NVIDIA compiler that generates instructions has a front-end and a back-end that is closed source. If you want to modify an open-source compiler for the GPUs it might be possible, but at the moment there are no widely recognized tool chains that I am aware of that don't use the closed-source back end (ptxas or its driver equivalent). The GPU driver is also largley closed source. There aren't any exposed controls that would affect the location of the stack, either.\n",
        "\n",
        "    May I know what I have to modify (e.g. the driver or the compiler) to be able to obtain those registers?\n",
        "\n",
        "There is no published register for the instruction pointer/program counter. Therefore its impossible to state what modifications would be needed.\n",
        "\n",
        "    If we increase the input to fib(n) to be 10000, it is likely to cause stack overflow, is there a way to deal with it?\n",
        "\n",
        "As I mentioned, the maximum stack-space per thread is limited, so your observation is correct, eventually a stack could grow to exceed the available space (and this is a possible hazard for recursion in CUDA device code). The provided mechanism to address this is to increase the per-thread local memory size (since the stack exists in the logical local space).\n",
        "\n",
        "answered Nov 28, 2022 at 19:18\n",
        "Robert Crovella\n",
        "\n",
        "\n",
        "\n",
        "    Thanks for your detailed reply. Suppose we have found out a way to convert local memory instructions into global memory instructions in SASS, e.g. LDL -> LDG, can we simply replace those local memory instructions with global memory instructions? Besides, for the memory address in SASS code, are they physical memory addresses? I think it is likely to be the actual physical address since it does not require address translation, but since local memory and global memory are all in DRAM, is the memory layout like \"local mem\": [0x0, 0x2FFFF..), \"global mem\": [0x2FFFF.., 0xFFFFF)? –\n",
        "    Ethan L.\n",
        "    Nov 29, 2022 at 2:56\n",
        "\n"
      ],
      "metadata": {
        "id": "CFXfmXonxkwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mais sobre a pilha...\n",
        "\n",
        "https://stackoverflow.com/questions/7810740/where-does-cuda-allocate-the-stack-frame-for-kernels\n",
        "\n",
        "Where does CUDA allocate the stack frame for kernels?\n",
        "<br>\n",
        "Asked 11 years, 5 months ago\n",
        "Modified 10 years, 8 months ago\n",
        "<br>\n",
        "\n",
        "My kernel call fails with \"out of memory\". It makes significant usage of the stack frame and I was wondering if this is the reason for its failure.\n",
        "\n",
        "When invoking nvcc with --ptxas-options=-v it print the following profile information:\n",
        "\n",
        "    150352 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
        "ptxas info    : Used 59 registers, 40 bytes cmem[0]\n",
        "\n",
        "Hardware: GTX480, sm20, 1.5GB device memory, 48KB shared memory/multiprocessor.\n",
        "\n",
        "My question is where is the stack frame allocated: In shared, global memory, constant memory, ..?\n",
        "\n",
        "I tried with 1 thread per block, as well as with 32 threads per block. Same \"out of memory\".\n",
        "\n",
        "Another issue: One can only enlarge the number of threads resident to one multiprocessor if the total numbers of registers do not exceed the number of available registers at the multiprocessor (32k for my card). Does something similar apply to the stack frame size?\n",
        "\n",
        "    cudastack\n",
        "\n",
        "codetwiddler\n",
        "\n",
        "asked Oct 18, 2011 at 16:41\n",
        "\n",
        "Answers\n",
        "\n",
        "Stack is allocated in local memory. Allocation is per physical thread (GTX480: 15 SM * 1536 threads/SM = 23040 threads). You are requesting 150,352 bytes/thread => ~3.4 GB of stack space. CUDA may reduce the maximum physical threads per launch if the size is that high. The CUDA language is not designed to have a large per thread stack.\n",
        "\n",
        "In terms of registers GTX480 is limited to 63 registers per thread and 32K registers per SM.\n",
        "\n",
        "\n",
        "<br><br>\n",
        "\n",
        "https://forums.developer.nvidia.com/t/what-is-the-maximum-cuda-stack-frame-size-per-kerenl/31449\n",
        "\n",
        "\n",
        "\n",
        " njuffa November 18, 2013, 6:46pm 2\n",
        "\n",
        "The compiler reports stack frame usage on a per-thread basis. The maximum stack frame size per thread for a given GPU is determined by (a) a hard architecture limit on the amount of local memory per thread (b) the amount of available GPU memory.\n",
        "\n",
        "The architectural limit on the amount of local memory per thread is documented in the programming guide section G.1, table 12.\n",
        "[url]http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications[/url]\n",
        "\n",
        "Available stack frame size per thread can then be approximated by\n",
        "\n",
        "stack frame size available per thread =\n",
        "min (amount of local memory per thread as documented in section G.1 table 12,\n",
        "available GPU memory / number of SMs / maximum resident threads per SM)\n",
        "\n",
        "The reason this is approximate is because there are various levels of allocation granularity that, best I know, are not documented and may vary from GPU to GPU. I do not know anything about your use case, but in general massive local memory usage would suggest to me that one might want to re-think the mapping of work to the GPU.\n",
        "\n"
      ],
      "metadata": {
        "id": "WE58mloy0xtW"
      }
    }
  ]
}