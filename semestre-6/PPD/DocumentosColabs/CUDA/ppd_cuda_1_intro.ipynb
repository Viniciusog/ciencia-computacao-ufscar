{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7fbIcLitZX22",
        "pgK-MWiPa4KB",
        "xeYaI-u201sT",
        "M1f42EBBpcmk"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF76EjM3ZQ7r"
      },
      "source": [
        "# PPD: Programação com CUDA\n",
        "\n",
        "Hélio - DC/UFSCar - 2023"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> *Este notebook apresenta aspectos da programação com CUDA usando a plataforma Google Colab. A ideia é usar o mecanismo de documentação e programação, aproveitando-se do fato que esta plataforma permite associar GPUs à execução dos códigos*.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7Jzka0evtWv5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsI-pG7HgG85"
      },
      "source": [
        "https://www.geeksforgeeks.org/how-to-run-cuda-c-c-on-jupyter-notebook-in-google-colaboratory/\n",
        "\n",
        "\n",
        "https://github.com/eegkno/CUDA_by_practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSFT46XDx5MR"
      },
      "source": [
        "# Sobre GPUs e CUDA\n",
        "\n",
        "Do ponto de vista do hardware, as GPUs são aglomerados de processadores organizados de forma hierárquica, tanto na interligação dos processadores quanto no acesso a módulos de memória presentes no adaptador.\n",
        "\n",
        "GPUs foram projetadas inicialmente para o paralelismo de dados, sendo que todos os processadores executam o mesmo conjunto de instruções ao mesmo tempo, comumente sobre partes distintas dos dados.\n",
        "\n",
        "Além de servirem para a geração eficiente de informações que são apresentadas em dispositivos de vídeo, há mecanismos de programação que conseguem utilizar os processadores de GPUs para **operações de propósito geral**, caracterizando-as como GP-GPUs (*General Purpose GPUs*).\n",
        "\n",
        "CUDA é uma arquitetura de software e hardware que permite que [GPUs NVIDIA](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-a-general-purpose-parallel-computing-platform-and-programming-model) executem programas escritos em C, C++, Fortran, OpenCL, DirectCompute, e outras linguagens. Essa arquitetura foi pioneira em prover um [modelo de programação](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#cuda-a-general-purpose-parallel-computing-platform-and-programming-model) de propósito geral usando GPUs.\n",
        "\n",
        "Posteriormente, um padrão aberto para a programação com GPUs, chamado **OpenCL**, passou a ser desenvolvido por um conjunto de empresas, incluindo NVIDIA. Ambos os modelos de programação, CUDA e OpenCL, têm abordagens semelhantes.\n",
        "\n",
        "Mais recentemente, OpenAcc e OpenMP também incorporaram mecanismos para usar GPUs no processamento de tarefas.\n",
        "\n",
        "O modelo de programação para GP-GPUs  é caracterizado como **SIMT**: *Single Instruction Multiple Thread*, uma vez que a mesma *thread*, comumente formada por pequenos conjuntos de instruções, é executada em todos os processadores da GPU ao mesmo tempo.\n",
        "\n",
        "https://docs.nvidia.com/cuda/cuda-c-programming-guide/_images/gpu-computing-applications.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fbIcLitZX22"
      },
      "source": [
        "# Programando com CUDA no Colab\n",
        "\n",
        "Para usar o suporte à programação com CUDA e GPUs no Colab, é preciso, inicialmente, fazer a configuração do Notebook para incluir uma GPU.\n",
        "\n",
        "Assim, no menu superior, selecione **Edit** -> **Notebook Settings** (ou **Runtime** -> **Change runtime type**) e na opção **Hardware Accelerator**, habilitar uma **GPU**.\n",
        "\n",
        "Tendo feito isso, uma estratégia para execução dos códigos é instalar um [plugin](https://github.com/andreinechaev/nvcc4jupyter) que permite executar diretamente programas **CUDA** nas células de código.\n",
        "\n",
        "Também é possível compilar manualmente os códigos, usando o compilador C NVIDIA, [nvcc](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html), como veremos nos exemplos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U-RqHvJgFUm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb50ac94-642c-4d19-ffa1-80287cce15b7"
      },
      "source": [
        "# importa macro %%cu , que permite compilar e executar diretamente o código de uma célula do notebook.\n",
        "! pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
        "\n",
        "# carrega plugin do material importado acima\n",
        "%load_ext nvcc_plugin\n",
        "\n",
        "# exibindo informações sobre as versões de compiladores disponíveis\n",
        "! echo && nvcc --version\n",
        "! echo && gcc --version\n",
        "! echo && g++ --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-7ugg5d8p\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-7ugg5d8p\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 0d2ab99cccbbc682722e708515fe9c4cfc50185a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4716 sha256=a012542cc7c9b3132116ff2327521cf63ba98765b068a21557605833553323c2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-u9hkd0kv/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n",
            "\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "\n",
            "gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Copyright (C) 2021 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n",
            "\n",
            "g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Copyright (C) 2021 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nota sobre os modelos de GPU e compilação:\n",
        "\n",
        "Considerando que a versão CUDA desta plataforma foi recentemente atualizada para a versão 11, que não gera código nativo para as GPUs k80, que podem ser alocadas às sessões do Colab, pode ser preciso incluir um parâmetro a mais na linha de compilação, especificando a arquitetura alvo, caso a GPU atribuíd ao Colab seja a K80.\n",
        "\n",
        "ex:   \n",
        "```\n",
        "nvcc prog.cu -o prog    -Wno-deprecated-gpu-targets -gencode=arch=compute_37,code=sm_37\n",
        "```"
      ],
      "metadata": {
        "id": "5tcqZNaXQM-N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgK-MWiPa4KB"
      },
      "source": [
        "# Obtendo detalhes da GPU disponível\n",
        "\n",
        "Tendo associado uma GPU à sessão, é possível obter informações sobre sua configuração.\n",
        "\n",
        "O comando [nvidia-smi](https://developer.nvidia.com/nvidia-system-management-interface) apresenta informações sobre a GPU disponibilizada pelo Google Colaboratory nesta sessão do Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qDv8PMleGke",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c77681b-ab0d-4793-b672-cc13187d09ac"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jan  7 18:19:34 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NPDmR5NeYai"
      },
      "source": [
        "Informações sobre a GPU também podem ser obtidas via programa, através da função da API CUDA [cudagetDeviceProperties](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g1bf9d625a931d657e08db2b4391170f0), que retorna uma estrutura do tipo [cudaDeviceProp](https://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html#structcudaDeviceProp).\n",
        "\n",
        "Algumas dessas informações são ilustradas a seguir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nT1-Z7Ibwzl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82e6e12d-2653-4b69-ca59-15ed42fb14e8"
      },
      "source": [
        "%%cu\n",
        "// esta macro salva o arquivo desta célula, compila seu código com nvcc e executa o código gerado\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int deviceCount = 0;\n",
        "  cudaGetDeviceCount(&deviceCount);\n",
        "\n",
        "  if(deviceCount == 0) {\n",
        "    printf(\"Não há dispositivos compatíveis com CUDA disponíveis no sistema.\\n\");\n",
        "    return 0;\n",
        "  }\n",
        "  // devine um dispositivo para execução (necessário apenas se houver mais de 1 GPUs)\n",
        "\tcudaSetDevice(0);\n",
        "\tcudaDeviceProp prop;\n",
        "\tcudaGetDeviceProperties(&prop,0);\n",
        "\n",
        " \tprintf(\"Modelo do Device: %s\\n\",prop.name);\n",
        "  printf(\"Número de SMs: %d\\n\",prop.multiProcessorCount);\n",
        "  printf(\"Número de Regs por SM: %d K\\n\",prop.regsPerMultiprocessor >> 10);\n",
        "  printf(\"Número de Regs por Bloco: %d K\\n\",prop.regsPerBlock  >> 10);\n",
        "  printf(\"Memória compartilhada por SM: %d KB\\n\", (int)prop.sharedMemPerMultiprocessor >> 10);\n",
        "  printf(\"Memória compartilhada por Bloco: %d KB\\n\",(int)prop.sharedMemPerBlock  >> 10);\n",
        "  printf(\"Memória Global: %ld GB\\n\",(long int)prop.totalGlobalMem  >> 10  >> 10  >> 10 );\n",
        "  printf(\"Memória Constante: %d KB\\n\",(int)prop.totalConstMem  >> 10);\n",
        "\n",
        "\treturn 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo do Device: Tesla T4\n",
            "Número de SMs: 40\n",
            "Número de Regs por SM: 64 K\n",
            "Número de Regs por Bloco: 64 K\n",
            "Memória compartilhada por SM: 64 KB\n",
            "Memória compartilhada por Bloco: 48 KB\n",
            "Memória Global: 14 GB\n",
            "Memória Constante: 64 KB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# para compilar e executar manualmente o código, seria preciso salvar o arquivo antes...\n",
        "# ! nvcc dev.cu -o dev && ./dev"
      ],
      "metadata": {
        "id": "m21FoxZnY7z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*As of CUDA 12.0, the [cudaInitDevice](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1gac04a5d82168676b20121ca870919419)() and [cudaSetDevice](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g159587909ffa0791bbe4b40187a4c6bb)() calls initialize the runtime and the primary context associated with the specified device. Absent these calls, the runtime will implicitly use device 0 and self-initialize as needed to process other runtime API requests.*\n",
        "\n",
        "*The runtime creates a CUDA context for each device in the system. This context is the primary context for this device and is initialized at the first runtime function which requires an active context on this device. It is shared among all the host threads of the application. As part of this context creation, the device code is just-in-time compiled if necessary and loaded into device memory. This all happens transparently.*\n",
        "\n",
        "*When a host thread calls [cudaDeviceReset](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1gef69dd5c6d0206c2b8d099abac61f217)(), this destroys the primary context of the device the host thread currently operates on (i.e., the current device as defined in Device Selection).*"
      ],
      "metadata": {
        "id": "o1vSonNo87fA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuaVzXKRwwZM"
      },
      "source": [
        "# Criando códigos para GPUs: *kernels*\n",
        "\n",
        "Na programação com CUDA, o fluxo principal do programa continua sendo em CPU, mas é possível transferir parte do processamento para a(s) GPU(s), bem como os dados necessários para esses processamentos.\n",
        "\n",
        "No funcionamento combinado de CPU e GPU, usa-se o termo ***host*** para designar o sistema computacional, e ***device*** para designar a GPU. Assim, fala-se em processamento no *host* e processamento no *device* (dispositivo). Cabe ao código do *host* coordenar as transferência de dados e códigos para o *device*, bem como ativar suas execuções. Os códigos executados no *device* são chamados de ***kernels***.\n",
        "\n",
        "De maneira geral, a programação com CUDA envolve as seguintes etapas:\n",
        "\n",
        "* Declarar e alocar variáveis e espaços de memória para dados na RAM (*host memory*) (para as atividades executando em CPU);\n",
        "* Declarar e alocar variáveis necessárias no espaço de endereçamento da GPU (*device memory*), para os processamentos em GPU;\n",
        "* Inicializar dados no *host*;\n",
        "* Transferir dados do *host* para o *device*;\n",
        "  <br>[ ou Inicializar dados direteramente no *device*; ]\n",
        "* Executar um ou mais *kernels* no *device*;\n",
        "* Transferir os resultados da memória do *device* para a memória do *host*.\n",
        "\n",
        "A declaração e a ativação de códigos para execução em GPU ocorrem dentro do próprio programa. Cada código de GPU (*kernel*) é declarado como uma função que tem o prefixo **\\_\\_global__**. Isso indica ao compilador para gerar código para esta função usando os recursos da arquitetura da GPU.\n",
        "\n",
        "Variáveis declaradas dentro de uma função de *kernel* serão alocadas automaticamente na área de memória do *device*. Essa alocação pode ocorrer em registradores dos processadores da GPU ou em posições de memória, o que é feito pela própria plataforma CUDA.\n",
        "\n",
        "Na ativação da função do *kernel*, especifica-se também a organizacão lógica das *threads* que serão utilizadas na execução deste código. Assim, o **número de instâncias** que irão executar o *kernel* especificado é determinado pelo **número de *threads*** selecionadas na ativação do *kernel*.\n",
        "\n",
        "<!--\n",
        "```\n",
        "// Kernel definition\n",
        "__global__ void VecAdd(float* A, float* B, float* C)\n",
        "{\n",
        "    int i = threadIdx.x;\n",
        "    C[i] = A[i] + B[i];\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    ...\n",
        "    // Kernel invocation with N threads\n",
        "    VecAdd<<<1, N>>>(A, B, C);\n",
        "    ...\n",
        "}\n",
        "```\n",
        "-->\n",
        "\n",
        "<br>\n",
        "\n",
        "O exemplo a seguir ilustra um programa que inclui a definição de um *kernel* simples e uma chamada para execuçao deste *kernel* na GPU, utilizando apenas 1 *thread*.\n",
        "\n",
        "A ativação do código de um *kernel* em GPU é feita de forma semelhante a uma chamada de função no programa da CPU. O uso dos caracteres \"\"<<< , >>>\"\" após o nome da função serve para indicar o número e a organização lógica dos processadores da GPU. Parâmetros da função do *kernel* são especificados depois dessas informações.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVbiZ0RC0uG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b2bd7ce-18ff-4e6e-808f-e97768bed8ca"
      },
      "source": [
        "# macro que compila o código CUDA desta célula e o executa\n",
        "# %%cu\n",
        "\n",
        "# macro que salva o conteúdo da célula como um arquivo\n",
        "%%writefile hello.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "// __global__ indica que esta função é um Kernel para execução em GPU\n",
        "\n",
        "__global__ void helloFromGPU(){\n",
        "    printf(\"Hello from GPU!\\n\");\n",
        "    return;\n",
        "}\n",
        "\n",
        "void helloFromCPU(){\n",
        "    printf(\"Hello from CPU!\\n\");\n",
        "    return;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    // Chamada do Kernel com apenas 1 thread em 1 bloco\n",
        "    helloFromGPU<<<1,1>>>();\n",
        "\n",
        "    // Chamada da função em CPU\n",
        "    helloFromCPU();\n",
        "\n",
        "    // A função a seguir é necessária para garantir que a execução do kernel em GPU foi concluída.\n",
        "    // Experimente comentar a linha a seguir e veja que o programa termina antes que a impressão\n",
        "    // feita no código do kernel seja concluída\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hello.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3WyusTSOufe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adeb7488-23c5-493d-afd9-e4fdc25be2a5"
      },
      "source": [
        "# ! nvcc hello.cu -o hello -Wno-deprecated-gpu-targets -gencode=arch=compute_37,code=sm_37\n",
        "! if [ ! hello -nt hello.cu ]; then nvcc hello.cu -o hello; fi\n",
        "! ./hello"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from CPU!\n",
            "Hello from GPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeYaI-u201sT"
      },
      "source": [
        "# Sincronismo CPU / GPU\n",
        "\n",
        "Após a execução de um *kernel* ser **ativada**, o fluxo de execução de instruções em CPU continua normalmente. Assim, CPU(s) e GPU(s) trabalham em **paralelo**, de forma **assíncrona**.\n",
        "\n",
        "Caso a conclusão do processamento em GPU seja necessária antes de realizar outra operação em CPU, é possível usar a função [cudaDeviceSynchronize](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g10e20b05a95f638a4071a655503df25d)(\\*).\n",
        "\n",
        "Quando o processamento em GPU ocorre em ciclos, esta sincronização ocorre tipicamente antes de cada nova etapa. De maneira geral, também ocorre ao final do programa principal em CPU, para garantir que as operações em GPU foram concluídas antes de o programa ser terminado.\n",
        "\n",
        "<br>\n",
        "\n",
        "(\\*) **cudaDeviceSynchronize** ( void ): *Blocks until the device has completed all preceding requested tasks*.\n",
        "\n",
        "Veja mais em [Cuda Device Management](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tratamento de erros em funções da API CUDA e na ativação do *kernel*\n",
        "\n",
        "A maioria das funções da API CUDA, quando invocadas, tem como retorno um valor do tipo [cudaError_t](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1gf599e5b8b829ce7db0f5216928f6ecb6). Em caso de sucesso na execução, o valor retornado é [cudaSuccess](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1gg3f51e3575c2178246db0a94a430e0038e355f04607d824883b4a50662830d591) e, em caso de falha, é possível detectar a falha ocorrida usando a função [cudaGetErrorString](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__ERROR.html)().\n",
        "\n",
        "```c\n",
        "cudaError_t err;\n",
        "err = cudaX...Y...(...);\n",
        "\n",
        "if (err != cudaSuccess) {\n",
        "  printf(\"Erro: %s\\n\", cudaGetErrorString(err));\n",
        "  ...\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "Já na ativação de um *kernel* em GPU, não há um valor de retorno. Entretanto, erros podem acontecer e é importante identificá-los.\n",
        "\n",
        "Isso pode ser feito usando-se a função [cudaGetLastError](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__ERROR.html)(), ilustrada a seguir:\n",
        "\n",
        "```c\n",
        "// Ativação de kernel\n",
        "k_function <<< 1, 0>>>(); // número inválido de threads: 1 bloco com 0 threads\n",
        "\n",
        "cudaError_t err;\n",
        "err = cudaGetLastError();\n",
        "if (err!= cudaSuccess) {\n",
        "  printf(\"Erro: %s\\n\", cudaGetErrorString(err));\n",
        "  ...\n",
        "}\n",
        "```\n",
        "\n",
        "Também é possível identificar erros que ocorram **durante** a execução de um *kernel* que foi ativado com sucesso. Como a execução de um *kernel* ocorre de forma assíncrona com a execução do código em CPU, este tipo de erro é chamado de erro assíncrono.\n",
        "\n",
        "Neste caso, a análise do valor de retorno da função [cudaDeviceSynchronize](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html)() indicará a ocorrência de erros na execução de um *kernel* ativado anteriormente.\n",
        "\n",
        "```c\n",
        "  cudaError_t syncErr, asyncErr;\n",
        "\n",
        "  k_function <<< number_of_blocks, threads_per_block>>>(...);\n",
        "\n",
        "  // Teste de erros na ativação do kernel\n",
        "  syncErr = cudaGetLastError();\n",
        "  if (syncErr != cudaSuccess) {\n",
        "    printf(\"Error: %s\\n\", cudaGetErrorString(syncErr));\n",
        "    // termina programa (?)\n",
        "  }\n",
        "  ...\n",
        "  // Teste de erros durante a execução do kernel\n",
        "  asyncErr = cudaDeviceSynchronize();\n",
        "  if (asyncErr != cudaSuccess) {\n",
        "    printf(\"Error: %s\\n\", cudaGetErrorString(asyncErr));\n",
        "    ...\n",
        "  }\n",
        "```\n"
      ],
      "metadata": {
        "id": "M1f42EBBpcmk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uwygBAU_IsI"
      },
      "source": [
        "# Blocos e Threads\n",
        "\n",
        "A execução de instruções em GPU com CUDA é feita no modelo **SIMT**, criando *threads* a partir de funções definidas como *kernels*.  O **número** e a **organização das *threads*** que vão executar instâncias do *kernel* simultaneamente são determinados como parâmetros na ativação do *kernel*.\n",
        "\n",
        "* As *threads* são organizadas em **blocos**. A estrutura dos blocos é definida por uma variável do tipo ***dim3***, que possibilita arranjos em 1, 2 ou 3 dimensões.\n",
        "\n",
        "* Além disso, é possível definir uma grade (***grid***) de blocos, que podem ser organizados em estruturas lógicas de 1, 2 ou 3 dimensões.\n",
        "\n",
        "Assim, há 2 parâmetros que são passados na ativação de um *kernel*. O primeiro indica a estruturação do ***grid***, ou seja, quantos blocos de *threads* serão usados e como esses blocos estarão organizados. O segundo parâmetro indica como as *threads* estarão organizadas dentro de cada bloco.\n",
        "\n",
        "```c\n",
        "kernel_function <<< grid, bloco >>>\n",
        "```\n",
        "ou seja,\n",
        "```c\n",
        "kernel_function <<< #blocos, #threads_por_bloco >>>\n",
        "```\n",
        "\n",
        "Durante a execução das *threads*, há variáveis que permitem a cada uma das *threads* saber quais são as dimensões do bloco a que pertence e qual é o posicionamento desta *thread* dentro deste bloco, nas dimensões X, Y e Z.\n",
        "\n",
        "Também é possível saber qual é a organização do *grid* e qual é o posicionamento do bloco desta *thread* em relação ao *grid*.\n",
        "\n",
        "\n",
        "* ***Thread*** dentro do bloco: **threadIdx.x**, **threadIdx.y**, **threadIdx.z**\n",
        "\n",
        "* **Bloco** dentro do *grid*: **blockIdx.x**, **blockIdx.y**, **blockIdx.z**\n",
        "\n",
        "* **Dimensões** do bloco: **blockDim.x**, **blockDim.y**, **blockDim.z**\n",
        "\n",
        "* **Dimensões** do grid em blocos: **gridDim.x**, **gridDim.y**, **gridDim.z**\n",
        "\n",
        "<br>\n",
        "\n",
        "Há 2 fatores importantes associados a essa forma de organização lógica das *threads*. Um aspecto é poder mapear o identificador da *thread* com a organização de estruturas de dados multidimensionais, como vetores, matrizes e volumes.\n",
        "\n",
        "\n",
        "Outro aspecto da organização das *threads* em blocos é o mapeamento dos blocos aos elementos de processamento das GPUs.\n",
        "\n",
        "<!--\n",
        "Um ou mais blocos serão alocados para executar em multiprocessadores (**Stream Multiprocessor ou SM**). Quando um bloco é escalonado em um SM irá permanecer no SM até terminar sua execução.  **Os blocos e seus threads são executados em ordem aleatória**. A quantidade de blocos que executa simultaneamente em um SM depende da quantidade de recursos do SM e da quantidade de recursos requisitados pelo bloco (número de registradores e quantidade de memória compartilhada), além é claro do limite máximo definido para cada arquitetura de GPU.\n",
        "-->\n",
        "\n",
        "<br>\n",
        "\n",
        "No exemplo a seguir, vê-se a ativação do *kernel* e a configuração do *grid* e das *threads* dentro dos blocos.\n",
        "\n",
        "As variáveis ***threadIdx*** e ***blockIdx*** indicam a posição lógica de cada *thread* na estrutura.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGsYommAst4B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cdd91da-d982-40e0-b4ba-5ebf9d1b6e06"
      },
      "source": [
        "%%file index.cu\n",
        "\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "/*\n",
        "  typedef struct {\n",
        "    unsigned int x;\n",
        "    unsigned int y;\n",
        "    unsigned int z;\n",
        "  } dim3;\n",
        "*/\n",
        "\n",
        "\n",
        "__global__\n",
        "void checkIndex(void)\n",
        "{\n",
        "  printf(\"threadIdx:(%d, %d, %d) blockIdx:(%d, %d, %d) blockDim:(%d, %d, %d) \"\n",
        "      \"gridDim:(%d, %d, %d)\\n\", threadIdx.x, threadIdx.y, threadIdx.z,\n",
        "      blockIdx.x, blockIdx.y, blockIdx.z, blockDim.x, blockDim.y,\n",
        "      blockDim.z, gridDim.x, gridDim.y, gridDim.z);\n",
        "}\n",
        "\n",
        "int\n",
        "main(int argc, char **argv)\n",
        "{\n",
        "  dim3 grid = {1,1,3};    // organização dos blocos de threads\n",
        "  dim3 block = {2,2,2};   // organização das threads em cada bloco\n",
        "\n",
        "  checkIndex <<< grid, block >>>();\n",
        "\n",
        "  // espera processamento na GPU ser concluído\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  return (0);\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing index.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! nvcc index.cu -o index   -Wno-deprecated-gpu-targets -gencode=arch=compute_37,code=sm_37\n",
        "! nvcc index.cu -o index\n",
        "! ./index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVTNoQ43qBRQ",
        "outputId": "54ec567f-bd21-4645-eb9e-2d4f4b3872a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "threadIdx:(0, 0, 0) blockIdx:(0, 0, 2) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(1, 0, 0) blockIdx:(0, 0, 2) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(0, 1, 0) blockIdx:(0, 0, 2) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(1, 1, 0) blockIdx:(0, 0, 2) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(0, 0, 1) blockIdx:(0, 0, 2) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(1, 0, 1) blockIdx:(0, 0, 2) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(0, 1, 1) blockIdx:(0, 0, 2) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(1, 1, 1) blockIdx:(0, 0, 2) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(0, 0, 0) blockIdx:(0, 0, 0) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(1, 0, 0) blockIdx:(0, 0, 0) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(0, 1, 0) blockIdx:(0, 0, 0) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(1, 1, 0) blockIdx:(0, 0, 0) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(0, 0, 1) blockIdx:(0, 0, 0) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(1, 0, 1) blockIdx:(0, 0, 0) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(0, 1, 1) blockIdx:(0, 0, 0) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(1, 1, 1) blockIdx:(0, 0, 0) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(0, 0, 0) blockIdx:(0, 0, 1) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(1, 0, 0) blockIdx:(0, 0, 1) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(0, 1, 0) blockIdx:(0, 0, 1) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(1, 1, 0) blockIdx:(0, 0, 1) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(0, 0, 1) blockIdx:(0, 0, 1) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(1, 0, 1) blockIdx:(0, 0, 1) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(0, 1, 1) blockIdx:(0, 0, 1) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n",
            "threadIdx:(1, 1, 1) blockIdx:(0, 0, 1) blockDim:(2, 2, 2) gridDim:(1, 1, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examinando o resultado acima, vê-se que todas as *threads* mostram os mesmos valores para blockDim e gridDim, que indicam as **dimensões** do bloco de *threads* e do *grid* de blocos.\n",
        "\n",
        "Já os índices das *threads* nos blocos a que pertentem e os índices dos blocos na grade variam."
      ],
      "metadata": {
        "id": "HQFV3sKK1zUu"
      }
    }
  ]
}