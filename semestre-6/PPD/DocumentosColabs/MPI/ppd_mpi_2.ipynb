{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oqLeez1BbscH",
        "t4kAk0w6SW1P",
        "LZGqIadihpNp",
        "mJ_6GIUSm0aT"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_vqFo-wFgJn"
      },
      "source": [
        "# PPD: MPI e programação com passagem de mensagem\n",
        "\n",
        "Hélio - DC/UFSCar - 2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqLeez1BbscH"
      },
      "source": [
        "# Aspectos gerais das transmissões\n",
        "\n",
        "Usando o mecanismo de **identificação lógica** (***rank***) dos processos emissores (***senders***) e receptores (***receivers***), MPI oferece suporte tanto para comunicações **diretas** (ponto-a-ponto) entre pares de processos, quanto para operações de comunicação em **grupo** (coletivas).\n",
        "\n",
        "Relembrando, as identificações lógicas levam em consideração a participação de todos os processos no grupo MPI_COMM_WORLD, variando de 0 a N-1.\n",
        "\n",
        "    Também é possível criar outros sub-grupos numa aplicação. Dentro de um sub-grupo,\n",
        "    cada processo também vai ter um identificador lógico, que pode ser usado em comunicações\n",
        "    entre membros deste sub-grupo.\n",
        "\n",
        "Nas transmissões ponto-a-ponto, especifica-se um grupo de processos, que geralmente é o grupo MPI_COMM_WORLD, formado por todos os processos dessa aplicação. Os identificadores lógicos (*ranks*) dos processos **neste grupo** são usados então para identificar o emissor e o receptor de cada operação.\n",
        "\n",
        "Ex:\n",
        "\n",
        "* int MPI_**Send** (void \\*buf, int count, MPI_Datatype dtype,\n",
        "  int **dest**, int tag, MPI_Comm **comm**)\n",
        "\n",
        "* int MPI_**Recv** (void \\*buf, int count, MPI_Datatype dtype,\n",
        "  int **src**, int tag, MPI_Comm **comm**, MPI_Status *stat)\n",
        "\n",
        "<br>\n",
        "\n",
        "Cada mensagem, por sua vez, possui um atributo de identificação (***Tag***), que pode ser usado na seleção de mensagens a receber. Nas operações de recebimento de mensagens, é possível especificar um processo de origem e um identificador de mensagem esperada (*Tag*). Há, contudo, alguns códigos especiais que podem ser usados para identificar **qualquer processo** origem (**MPI_ANY_SOURCE**) e **qualquer mensagem** (**MPI_ANY_TAG**).\n",
        "\n",
        "Ex:\n",
        "\n",
        "* int MPI_**Send** (void \\*buf, int count, MPI_Datatype dtype,\n",
        "  int **dest**, int **tag**, MPI_Comm **comm**)\n",
        "\n",
        "* int MPI_**Recv** (void \\*buf, int count, MPI_Datatype dtype,\n",
        "  int **src**, int **tag**, MPI_Comm **comm**, MPI_Status *stat)\n",
        "\n",
        "<br>\n",
        "\n",
        "Já nas transmissões **coletivas**, identifica-se o **grupo**, ou sub-grupo, envolvido, e o processo que fará o papel de **divulgador** ou **agregador** dos dados, dependendo do tipo de transmissão.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WFEXah-Phba"
      },
      "source": [
        "# Tipos dos dados transmitidos\n",
        "\n",
        "A transmissão de dados usando *sockets* é feita passando-se um ponteiro para uma sequência de bytes. Cabe à aplicação tratar dos detalhes do armazenamento e da leitura dos dados, de acordo com seus tipos.\n",
        "\n",
        "MPI simplifica o envio de sequências de dados de um mesmo tipo. Considerando os dados transmitidos nas mensagens, MPI permite que a aplicação identifique seus **tipos** e **quantidades**, cuidando automaticamente dos empacotamentos apropriados em cada caso. Também é possível fazer empacotamentos e desempacotamentos de conteúdos específicos.\n",
        "\n",
        "\n",
        "Os tipos dos dados transmitidos podem ser pré-definidos ou definidos pelo usuário.\n",
        "\n",
        "* MPI_CHAR: signed char\n",
        "* MPI_SHORT: signed short int\n",
        "* MPI_INT: signed int\n",
        "* MPI_LONG: signed long int\n",
        "* MPI_UNSIGNED_CHAR: unsigned char\n",
        "* MPI_UNSIGNED_SHORT: unsigned short int\n",
        "* MPI_UNSIGNED: unsigned int\n",
        "* MPI_UNSIGNED_LONG: unsigned long int\n",
        "* MPI_FLOAT: float\n",
        "* MPI_DOUBLE: double\n",
        "* MPI_LONG_DOUBLE: long double\n",
        "* MPI_BYTE: 8 binary digits\n",
        "* MPI_PACKED: data packed or unpacked with MPI_Pack()/ MPI_Unpack\n",
        "\n",
        "Ex:\n",
        "\n",
        "* int MPI_**Send** (void \\*buf, int **count**, MPI_Datatype **dtype**,\n",
        "  int dest, int tag, MPI_Comm comm)\n",
        "\n",
        "* int MPI_**Recv** (void \\*buf, int **count**, MPI_Datatype **dtype**,\n",
        "  int src, int tag, MPI_Comm comm, MPI_Status *stat)\n",
        "\n",
        "Nas operações acima, a indicação do tipo dos dados transmitidos e o número de dados serve para que MPI faça o empacotamento e desempacotamento de maneira apropriada. Assim, o conteúdo dos dados transmitidos é preservado nas transmisões, mesmo entre sistemas emissor e receptor executando em SOs e arquiteturas diferentes.\n",
        "\n",
        "<br>\n",
        "\n",
        "Também é possível realizar o envio de dados de tipos variados. Para isso, MPI oferece mecanismos para o empacotamento de dados em um *buffer* para envio e o desempacotamento de dados de uma mensagem recebida.\n",
        "\n",
        "É claro que cabe ao código da aplicação fazer o empacotamento e o desempacotamento dos dados no buffer na ordem apropriada.\n",
        "\n",
        "<br>\n",
        "\n",
        "## Emapacotamento e envio de dados variados\n",
        "\n",
        "https://www.open-mpi.org/doc/v3.1/man3/MPI_Pack.3.php\n",
        "<br>\n",
        "https://www.mpi-forum.org/docs/mpi-3.1/mpi31-report/node92.htm#Node92\n",
        "\n",
        "\n",
        "```c\n",
        "int MPI_Pack (const void *inbuf, int incount, MPI_Datatype datatype,\n",
        "              void *outbuf, int outsize, int *position, MPI_Comm comm)\n",
        "```\n",
        "\n",
        "```c\n",
        " Example: An example using MPI_Pack:\n",
        "\n",
        "    int position, i, j, a[2];\n",
        "    char buff[1000];\n",
        "    ....\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n",
        "    \n",
        "    if (myrank == 0) { /* SENDER CODE */\n",
        "      position = 0;\n",
        "      MPI_Pack(&i, 1, MPI_INT, buff, 1000, &position, MPI_COMM_WORLD);\n",
        "      MPI_Pack(&j, 1, MPI_INT, buff, 1000, &position, MPI_COMM_WORLD);\n",
        "      MPI_Send( buff, position, MPI_PACKED, 1, 0, MPI_COMM_WORLD);\n",
        "    }\n",
        "    else { /* RECEIVER CODE */\n",
        "      MPI_Recv( a, 2, MPI_INT, 0, 0, MPI_COMM_WORLD)\n",
        "    }\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tipos de transmissão ponto-a-ponto\n",
        "\n",
        "Há diferentes tipos de primitivas de envio ponto-a-ponto, que variam em função do **sincronismo** entre emissor e receptor, do **bloqueio** ou não da operação, e do uso de ***buffers*** no transmissor e no receptor.\n",
        "\n",
        "* ***Standard***: nas transmissões padrão, não há sincronismo entre emissor e receptor. Se forem providos buffers, envio pode ser concluído antes do recebimento. No caso de transmissões não-bloqueantes, as operações MPI_Wait( ) e MPI_Test( ) podem ser usadas para saber se foram concluídas.\n",
        "* ***Buffered***: transmissões podem ser *bufferizadas*. Para tanto, as chamadas MPI_Buffer_attach( ) e MPI_Buffer_detach( ) tratam da definição de espaços de *buffer*. Nesse tipo de transmissão bufferizada, o envio pode ser concluído antes do recebimento ser selecionado.\n",
        "* ***Synchronous***: nas transmissões síncronas, as operações envio e recebimento podem ser realizadas em qualquer ordem, mas a transmissão só ocorre quando ambas as operações forem emitidas. Deste modo, além de prover a transmissão, essas chamadas servem para a sincronziação entre as partes envolvidas.\n",
        "* ***Ready***: nesse modo de transmissão, o envio pode ser iniciado apenas quando o recebimento já foi solicitado, ou um erro é resultado.\n",
        "\n",
        "<br>\n",
        "\n",
        "    Nas chamadas providas pela API MPI, mnemônicos nos nomes das funções especificam cada um desses modos de transmissão:\n",
        "    -b: buffered, -s: synchronous, -r: ready\n",
        "\n",
        "Qualquer tipo de envio (padrão, buferizado, síncrono ou *ready*) pode ser associado a qualquer tipo de recepção (padrão, ...).\n",
        "\n",
        "Nas operações não bloqueantes, há formas de verificar posteriormente se mensagens esperadas já foram recebidas.\n",
        "\n",
        "Já as operações que usam *buffers* têm o efeito de permitir que as transmissões ocorram mesmo que emissor e receptor não estejam sincronizados numa operação de transmissão.\n",
        "\n",
        "Uma vez que as chamadas de envio e recebimento comumente podem ser emitidas de forma não sincronizada, **cabe à implementação MPI** tratar do posicionamento dos dados até que eles possam ser efetivamente repassados ao processo receptor.\n",
        "\n",
        "Por exemplo, caso a operação de envio seja realizada antes de o receptor emitir uma chamada de recepção, cabe à implementação MPI decidir onde os dados transmitidos serão armazenados até que possam ser entregues. Para tanto, uma área de *buffer* de recebimento deve ser alocada, seja no nó emissor e/ou no receptor. A decisão de como tratar isso, contudo, não é padronizada, sendo dependente da implementação MPI.\n",
        "\n",
        "Vale ressaltar que as transmissões providas pela biblioteca MPI **são confiáveis**. Ou seja, salvo se ocorra falha nos meios de transmissão, as mensagens enviadas serão sempre recebidas corretamente e a aplicação não precisa preocupar-se com a verificação de erros nos dados recebidos, com limites de tempos para transmissão (*time-outs*), ou com outras condições de erro.\n",
        "\n",
        "MPI garante ainda que a ordem de recebimento das mensagens equivalentes é respeitada nas entregas aos receptores, mas não cabe à implementação MPI garantir que não ocorrerá *starvation* no recebimento de mensagens por processos concorrentes. Ou seja, se vários processos competem pelo recebimento de algum tipo de mensagem, MPI não garante que todos receberão mensagens.\n",
        "\n"
      ],
      "metadata": {
        "id": "uYIqCr2zR-v2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb9WsW6XVBeC"
      },
      "source": [
        "# Comunicação com MPI_Send e MPI_Receive\n",
        "\n",
        "O exemplo a seguir ilustra transmissões usando [MPI_Send](https://www.open-mpi.org/doc/v3.1/man3/MPI_Send.3.php) e [MPI_Recv](https://www.open-mpi.org/doc/v3.1/man3/MPI_Recv.3.php).\n",
        "\n",
        "Uma vez compilado um programa MPI, ele pode ser ativado com diferentes números de processos, alocados sobre diferentes conjuntos de computadores. Além disso, comumente, o mesmo código (arquivo executável) é iniciado em todos os nós, no modelo SPMD.\n",
        "\n",
        "Deste modo, um aspecto comum na maior parte dos programas é determinar quantos processos foram usados na execução corrente e qual é o número lógico (*rank*) de um processo dentro deste conjunto.\n",
        "\n",
        "Isso é feito com as chamadas MPI_Comm_size(MPI_COMM_WORLD, ...) e MPI_Comm_rank(MPI_COMM_WORLD, ...);\n",
        "\n",
        "A diferenciação do papel que cada processo exeutará dentro da aplicação é comumente feita em função de seus *ranks*. Em geral, o processo de *rank* **0** é usado para fazer as atividades de coordenação, mas isso é critério da aplicação.\n",
        "\n",
        "```c\n",
        "// Determina o número de processos e o rank do processo atual no grupo geral\n",
        "int num_procs, rank;\n",
        "\n",
        "MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n",
        "MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "\n",
        "int i;\n",
        "int val;    // vai ser usado para transmissão pelo rank 0 e para recepção pelos demais processos\n",
        "...\n",
        "if (rank == 0) {\n",
        "    val = set_val();   // função hipotética que produz o valor de interesse\n",
        "    // envia mensagem para todos os demais processos, 1 a rank-1\n",
        "    // int MPI_Send(const void *buf, int count, MPI_Datatype datatype,\n",
        "    //              int dest, int tag, MPI_Comm comm)\n",
        "    for(i=1; i < num_procs; i++)\n",
        "      MPI_Send(&val, 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n",
        "\n",
        "} else if (rank != 0) { // processos rank > 0 recebem mensagem de rank 0\n",
        "    // int MPI_Recv(void *buf, int count, MPI_Datatype datatype,\n",
        "    //              int source, int tag, MPI_Comm comm, MPI_Status *status)\n",
        "    MPI_Recv(&val, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
        "    printf(\"Processo %d recebeu valor %d do processo rank 0\\n\", rank, val);\n",
        "}\n",
        "```\n",
        "\n",
        "Alguns aspectos a notar neste trecho de código:\n",
        "\n",
        "* o **mesmo programa** está sendo executado nos processos de todos os *ranks*\n",
        "* no envio, veja que o buffer de transmissão é o endereço de onde se iniciam os dados. Neste caso, é o endereço da variável;\n",
        "* no envio ainda, o contador é o número de elementos, do tipo definido, que serão copiados para transmissão a partir do endereço de início do *buffer*;\n",
        "* no envio, o valor de ***i*** está sendo usado para indicar o *ranK* de cada receptor a que se destina a mensagem;\n",
        "* veja que, como o nó de *rank* ***0*** está transmitindo, o ***for*** varia de ***i=1 a (num_procs-1)***. Ou seja, 0 envia para os demais;\n",
        "* nessas transmissões, todas as mensagens têm o ***tag*** 0, tanto no envio quanto no recebimento; assim, não há seleção de mensagens pelos *tags* neste caso;\n",
        "* no recebimento, passa-se o endereço de onde os dados serão colocados, o tipo dos dados e o número de ocorrências;\n",
        "* no recebimento, todos os nós recebem do nó de *rank* ***0***;\n",
        "* ainda no recebimento, veja que o valor inteiro sendo recebido é copiado para a variável ***val***. É claro que isso não vai sobrepor o valor da variável val do emissor, já que essa operação de recebimento vai estar sendo executada só nos nós receptores (*rank* > 0)!\n",
        "\n",
        "Um outro aspecto a ressaltar aqui e que talvez esteja confuso, é o fato que embora estejamos examinando um código único, vai haver várias instâncias de processos executando esse mesmo código, provavelmente em nós (computadores) distintos!\n",
        "\n",
        "Em uma das cópias, no nó de *rank* ***0***, aquele de iniciou a execução, vai ser executada a primeira parte do ***if***. Nos demais, o código do ***else***.\n",
        "\n",
        "Faz sentido?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4kAk0w6SW1P"
      },
      "source": [
        "# Transmissões não bloqueantes\n",
        "\n",
        "Em MPI, e na programação com passagem de mensagem de maneira geral,  as transmissões de mensagens podem também servir para algum tipo de **sincronização** entre os processos. O uso de *buffers*, explicitamente pela aplicação, ou pela biblioteca MPI, introduz ainda um outro aspecto às transmissões.\n",
        "\n",
        "Comumente, a bibliotevca MPI utiliza *buffers* para transmissão, alocados no espaço de endereçamento do processo, mas de forma **não visível pela aplicação**. Assim, quando há uma chamada ***MPI_Send***, os dados a serem transmitidos são copiados para um *buffer* da biblioteca MPI e a chamada retorna imediatamente. Caso não haja espaço nesse *buffer*, devido a transmissões anteriores ainda pendentes, a tarefa que emitiu a chamada é bloqueada.\n",
        "\n",
        "Já quando a primitiva ***MPI_Isend*** for usada, a chamada retorna imediatamente, mesmo que os dados a transmitir não tenham sido copiados para o *buffer* da biblioteca. Esse é um comportamento **não bloqueante**. Um parâmetro extra presente nesta chamada, ***request***, pode ser usado posteriormente para verficiar o estado desta operação.\n",
        "\n",
        "O recebimento de mensagens também pode ser não bloqueante, usando-se ***MPI_Irecv***.\n",
        "\n",
        "* **MPI_Send** (buffer,count,type,dest,tag,comm): envio bloqueante\n",
        "* **MPI_Isend** (buffer,count,type,dest,tag,comm,request): envio não-bloqueante\n",
        "* **MPI_Recv** (buffer,count,type,source,tag,comm,status): recebimento bloqueante\n",
        "* **MPI_Irecv** (buffer,count,type,source,tag,comm,request): recebimento não bloqueante\n",
        "\n",
        "\n",
        "Parâmetros:\n",
        "\n",
        "* ***Buffer***: endereço de memória da localização dos dados; geralmente é o endereço de uma variável.\n",
        "* ***Data Count***: número de elementos de dados do tipo especificado a serem enviados.\n",
        "* ***Data Type***: tipos pré-definidos ou definidos pelo usuário.\n",
        "* ***Destination***: indica o rank do processo a quem se destina a msg.\n",
        "* ***Source***: especifica o rank do processo emissor. MPI_ANY_SOURCE permite receber de qualquer tarefa.\n",
        "* ***Tag***: identificador atribuído (0..32767) pelo programador para identificar uma mensagem. Permite especificar a mensagem a receber. MPI_ANY_TAG permite receber qualquer mensagem.\n",
        "* ***Communicator***: indica o conjunto de processos a quem se destina a mensagem. Normalmente usa-se MPI_COMM_WORLD.\n",
        "* ***Status***: em C, é um ponteiro para uma estrutura MPI_Status. Ex. stat.MPI_SOURCE, stat.MPI_TAG, MPI_Get_count routine (núm. Bytes recebidos)\n",
        "* ***Request***: usado em operações não-bloqueantes, retorna um \"request number\", que pode ser usado posteriormente (em operações do tipo WAIT) para determinar o estado da operação.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0BQ4IsZrHYm"
      },
      "source": [
        "O exemplo de código a seguir ilustra o uso de primitivas para envio e recebimento de mensagens. Neste exemplo, o processo de rank 0 envia mensagens individuais para cada um dos demais processos. Cada um deles recebe a mensagem do rank0 e envia uma resposta.\n",
        "\n",
        "Como há apenas 1 mensagem de cada origem para cada destino, o campo ***tag*** pode ser o mesmo em todas as trasmissões."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MP-6QUvfhbg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b981ef4-0425-4b60-db1c-7dba21f8574f"
      },
      "source": [
        "%%writefile sr.c\n",
        "\n",
        "#include <sys/types.h>\n",
        "#include <unistd.h>\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "#include <mpi.h>\n",
        "\n",
        "// #define LEN 256\n",
        "#define LEN (MPI_MAX_PROCESSOR_NAME + 128)\n",
        "\n",
        "int\n",
        "main( int argc, char *argv[])\n",
        "{\n",
        "\tint i, rank, result, numtasks, namelen, msgtag, pid, pid_0;\n",
        "\tchar processor_name[MPI_MAX_PROCESSOR_NAME];\n",
        "\tchar tx_buf[LEN], rx_buf[LEN];\n",
        "\tMPI_Status status;\n",
        "\n",
        "\tresult = MPI_Init(&argc,&argv);\n",
        "\n",
        "\tif (result != MPI_SUCCESS) {\n",
        "\t\tprintf (\"Erro iniciando programa MPI.\\n\");\n",
        "\t\tMPI_Abort(MPI_COMM_WORLD, result);\n",
        "\t}\n",
        "\n",
        "\t// Determina número de processos em execução na aplicação\n",
        "\tMPI_Comm_size(MPI_COMM_WORLD,&numtasks);\n",
        "\n",
        "\t// Determina ranking desse processo no grupo\n",
        "\tMPI_Comm_rank(MPI_COMM_WORLD,&rank);\n",
        "\n",
        "\t// Determina nome do host local\n",
        "\tMPI_Get_processor_name(processor_name,&namelen);\n",
        "\n",
        "\tpid=getpid();\n",
        "\n",
        "\tmsgtag=1;\n",
        "\n",
        "\tif(rank==0) { // master node envia msg para todos os demais: 1..N-1\n",
        "\n",
        "\t\tfor(i=1; i < numtasks; i++) {\n",
        "\n",
        "\t\t\t// int MPI_Send(void *buf, int count, MPI_Datatype dtype, int dest, int tag, MPI_Comm comm)\n",
        "\t\t\tMPI_Send (&pid, 1, MPI_INT,i,msgtag,MPI_COMM_WORLD);\n",
        "\t\t\t// printf(\"%s enviou: %d para processo %d\\n\",processor_name,pid, i);\n",
        "\t\t}\n",
        "\n",
        "\t\t// rank 0 aguarda resposta individual de cada um dos demais nós: 1..N-1\n",
        "\n",
        "\t\tfor(i=1; i < numtasks; i++) {\n",
        "\t\t\t// rank 0 recebe dos demais (MPI_Comm_size -1)\n",
        "\n",
        "\t\t\t// int MPI_Recv(void *buf, int count, MPI_Datatype dtype,\n",
        "\t\t\t//              int src, int tag, MPI_Comm comm, MPI_Status *stat)\n",
        "\t\t\tMPI_Recv(rx_buf,LEN,MPI_CHAR, i, msgtag, MPI_COMM_WORLD,&status);\n",
        "\n",
        "\t\t\tprintf(\"%s: msg de resposta recebida do processo %d: %s\\n\", processor_name, i,rx_buf);\n",
        "\t\t}\n",
        "\n",
        "\t} else {   // rank != 0: worker nodes: todos recebem de rank 0 e retornam\n",
        "\n",
        "\t\t// int MPI_Recv(void* buf,int count,MPI_Datatype datatype,\n",
        "\t\t//              int source, int tag,MPI_Comm comm,MPI_Status *status);\n",
        "\t\tMPI_Recv(&pid_0,1,MPI_INT,0,msgtag,MPI_COMM_WORLD,&status);\n",
        "\t\t// printf(\"%s recebeu: %d\\n\",processor_name,pid_0);\n",
        "\n",
        "\t\t// Ranks != 0 enviam msg para rank 0\n",
        "    // Monta mensagem de texto para resposta\n",
        "\t\tsprintf(tx_buf,\"%s: rank=%d, pid 0=%d\",processor_name,rank,pid_0);\n",
        "\n",
        "\t\t// int MPI_Send(void *buf, int count, MPI_Datatype dtype, int dest,\n",
        "\t\t//              int tag, MPI_Comm comm)\n",
        "\t\tMPI_Send(tx_buf,strlen(tx_buf)+1,MPI_CHAR,0,msgtag,MPI_COMM_WORLD);\n",
        "\t}\n",
        "\n",
        "\tMPI_Finalize();\n",
        "\n",
        "\treturn(0);\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing sr.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vPK4zi4giB9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1b1e046-f8e4-4fd4-8da8-8390b8d747c2"
      },
      "source": [
        "!mpicc sr.c -o sr && mpirun --allow-run-as-root -n 4 -host localhost:4 sr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3c1f42ea217e: msg de resposta recebida do processo 1: 3c1f42ea217e: rank=1, pid 0=33042\n",
            "3c1f42ea217e: msg de resposta recebida do processo 2: 3c1f42ea217e: rank=2, pid 0=33042\n",
            "3c1f42ea217e: msg de resposta recebida do processo 3: 3c1f42ea217e: rank=3, pid 0=33042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhDygKaQe2pH"
      },
      "source": [
        "O exemplo a seguir, baseado em https://mpitutorial.com/tutorials/mpi-send-and-receive/, ilustra um modelo de comunicação circular entre os processos da aplicação, também usando as primitivas MPI_Send e MPI_Recv."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5oKx93wVzip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "801cc7f7-bf6a-45dd-8e5f-03cc24bd2ddd"
      },
      "source": [
        "%%writefile pipeline.c\n",
        "\n",
        "#include <mpi.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <unistd.h>\n",
        "\n",
        "int\n",
        "main(int argc, char** argv)\n",
        "{\n",
        "  int world_rank;\n",
        "  int world_size;\n",
        "  int token, prox, ant;\n",
        "\n",
        "  MPI_Init(NULL, NULL);\n",
        "\n",
        "  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n",
        "  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n",
        "\n",
        "  // Recebe mensagem do processo de rank anterior e envia para o posterior\n",
        "  // Atenção com o primeiro e o último...\n",
        "  // Todas as mensagens são enviadas com Tag=0\n",
        "\n",
        "  prox = (world_rank +1) % world_size;\n",
        "  ant = (world_rank + world_size -1) % world_size;\n",
        "\n",
        "  // Quem começa?\n",
        "  if (world_rank == 0) {\n",
        "    token = 0;\n",
        "    // envia token: 1 valor do tipo MPI_INT\n",
        "    MPI_Send(&token, 1, MPI_INT, prox, 0, MPI_COMM_WORLD);\n",
        "  }\n",
        "  // todos agora, inclusive o 0...\n",
        "  do {\n",
        "    // espera token...\n",
        "    MPI_Recv(&token, 1, MPI_INT, ant, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
        "    printf(\"Rank %d recebeu token %d do rank %d\\n\", world_rank, token, ant);\n",
        "\n",
        "    if(token < 3 * world_size -1) {\n",
        "      // Já que tem o token, poderia usar o recurso agora!\n",
        "      // Token, na verdade, poderia ser dados que serão manipulados localmente e\n",
        "      // encaminhados para mais processamentos no próximo nó do pipeline...\n",
        "      sleep(rand()%3);\n",
        "\n",
        "      token++;\n",
        "    }\n",
        "    // libera o token, passando-o para o próximo no anel\n",
        "    MPI_Send(&token, 1, MPI_INT, prox, 0, MPI_COMM_WORLD);\n",
        "\n",
        "  } while (token < 3 * world_size -1);\n",
        "\n",
        "  printf(\"Rank %d terminando...\\n\",world_rank);\n",
        "\n",
        "  MPI_Finalize();\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing pipeline.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI7gBVC2Cuxx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e25a07-24fa-495a-b0ea-17983ecfb382"
      },
      "source": [
        "# Aqui, testamos com 4 processos no mesmo nó. Se houver mais nós bastaria configurá-los no hostfile, ou especificar em linha de comando\n",
        "! mpicc -Wall pipeline.c -o pipeline && mpirun --allow-run-as-root -n 4 -host localhost:4 pipeline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank 1 recebeu token 0 do rank 0\n",
            "Rank 2 recebeu token 1 do rank 1\n",
            "Rank 3 recebeu token 2 do rank 2\n",
            "Rank 0 recebeu token 3 do rank 3\n",
            "Rank 1 recebeu token 4 do rank 0\n",
            "Rank 2 recebeu token 5 do rank 1\n",
            "Rank 3 recebeu token 6 do rank 2\n",
            "Rank 0 recebeu token 7 do rank 3\n",
            "Rank 1 recebeu token 8 do rank 0\n",
            "Rank 2 recebeu token 9 do rank 1\n",
            "Rank 3 recebeu token 10 do rank 2\n",
            "Rank 3 terminando...\n",
            "Rank 0 recebeu token 11 do rank 3\n",
            "Rank 0 terminando...\n",
            "Rank 1 recebeu token 11 do rank 0\n",
            "Rank 1 terminando...\n",
            "Rank 2 recebeu token 11 do rank 1\n",
            "Rank 2 terminando...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZGqIadihpNp"
      },
      "source": [
        "# MPI_ANY_SOURCE e MPI_ANY_TAG\n",
        "\n",
        "No exemplo a seguir, o processo de rank 0 envia e recebe mensagens para/de todos os demais processos da aplicação (MPI_COMM_WORLD).\n",
        "\n",
        "Neste caso, contudo, o recebimento pode ser fora de ordem; ou seja, o identificador do processo que enviou a próxima mensagem na fila não é conhecido previamente. Assim, uma opção é usar-se a constante **MPI_ANY_SOURCE** para identificar a origem da mensagem esperada. Como o **tag** pode variar também, usa-se a constante **MPI_ANY_TAG** para especificar a mensagem.\n",
        "\n",
        "Também pode ocorrer de o número de itens recebidos na mensagem não ser conhecido previamente.\n",
        "\n",
        "Vale observar que na operação de recebimento há um parâmetro a mais que no envio, que é um ponteiro para uma estrutura **MPI_Status**. No retorno da chamada, esssa variável vai estar preenchida com informações sobre a mensagem recebida.\n",
        "\n",
        "```c\n",
        "MPI_Status {\n",
        "\tint MPI_SOURCE;\n",
        "\tint MPI_TAG;\n",
        "\tint MPI_ERROR;\n",
        "\tint st_length;  // message length // tamanho da mensagem recebida\n",
        "};\n",
        "```\n",
        "Os campos MPI_SOURCE, MPI_TAG e MPI_ERROR podem ser usados diretamente. Entretanto, para saber o número de itens recebidos na mensagem é preciso usar a função [MPI_Get_count](https://www.open-mpi.org/doc/v3.1/man3/MPI_Get_count.3.php), indicando o tipo dos itens contidos na mensagem:\n",
        "\n",
        "```c\n",
        "int MPI_Get_count(const MPI_Status *status, MPI_Datatype datatype, int *count);\n",
        "```\n",
        "O exemplo a seguir ilustra essa forma de recebimento, com identificação do emissor feita posteriormente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBoMQ9wRCOxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d6306a2-17e9-4687-c464-5456a84b8586"
      },
      "source": [
        "%%writefile sr-any.c\n",
        "\n",
        "#include <sys/types.h>\n",
        "#include <unistd.h>\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "#include <mpi.h>\n",
        "\n",
        "#define LEN 300\n",
        "\n",
        "int\n",
        "main( int argc, char *argv[])\n",
        "{\n",
        "\tint i, rank, count, result, numtasks, namelen, msgtag, pid, pid_0;\n",
        "\tchar processor_name[MPI_MAX_PROCESSOR_NAME];\n",
        "\tchar tx_buf[LEN], rx_buf[LEN];\n",
        "\tMPI_Status status;\n",
        "\n",
        "\tresult = MPI_Init(&argc,&argv);\n",
        "\n",
        "\tif (result != MPI_SUCCESS) {\n",
        "\t\tprintf (\"Erro iniciando programa MPI.\\n\");\n",
        "\t\tMPI_Abort(MPI_COMM_WORLD, result);\n",
        "\t}\n",
        "\n",
        "\t// Determina número de processos em execução na aplicação\n",
        "\tMPI_Comm_size(MPI_COMM_WORLD,&numtasks);\n",
        "\n",
        "\t// Determina ranking desse processo no grupo\n",
        "\tMPI_Comm_rank(MPI_COMM_WORLD,&rank);\n",
        "\n",
        "\t// Determina nome do host local\n",
        "\tMPI_Get_processor_name(processor_name,&namelen);\n",
        "\n",
        "\tpid=getpid();\n",
        "\n",
        "\tmsgtag=1;\n",
        "\n",
        "\t// todos os processos executaram o mesmo código até aqui!\n",
        "\n",
        "\tif(rank==0) { // master node\n",
        "\n",
        "\t\t// rank 0 envia valor de seu PID para todos os demais: 1..N-1\n",
        "\t\tfor(i=1; i < numtasks; i++) {\n",
        "\t\t\t// int MPI_Send(void *buf, int count, MPI_Datatype dtype, int dest, int tag, MPI_Comm comm)\n",
        "\t\t\tMPI_Send(&pid,1,MPI_INT,i,msgtag,MPI_COMM_WORLD);\n",
        "\t\t\t// printf(\"%s enviou: %d\\n\",processor_name,pid);\n",
        "\t\t}\n",
        "\n",
        "\t\t// rank 0 aguarda resposta individual de cada um dos demais nós: 1..N-1\n",
        "\n",
        "\t\tfor(i=1;i<numtasks;i++) {\n",
        "\t\t\t// rank 0 recebe dos demais (MPI_Comm_size -1)\n",
        "\n",
        "\t\t\t// Uso de MPI_ANY_SOURCE e MPI_ANY_TAG: não se sabe a ordem de envio\n",
        "\t\t\t// int MPI_Recv(void *buf, int count, MPI_Datatype dtype,\n",
        "\t\t\t//              int src, int tag, MPI_Comm comm, MPI_Status *stat)\n",
        "\t\t\tMPI_Recv(rx_buf,LEN,MPI_CHAR,MPI_ANY_SOURCE,MPI_ANY_TAG,MPI_COMM_WORLD,&status);\n",
        "\n",
        "      // Neste caso, como os valores MPI_ANY_SOURCE e MPI_ANY_TAG foram usados,\n",
        "      // se for preciso identificar o emissor, é preciso verificar as informações\n",
        "      // retornadas na estrutura status.\n",
        "\n",
        "\t\t\t// Campos de MPI_Status. Tamanho da mensagem recebida pode ser consultado via MPI_Get_count\n",
        "\t\t\t// MPI_Status {\n",
        "\t\t\t//    int MPI_SOURCE;\n",
        "\t\t\t//    int MPI_TAG;\n",
        "\t\t\t//    int MPI_ERROR;\n",
        "\t\t\t//    int st_length;  // message length\n",
        "\t\t\t// };\n",
        "\n",
        "\t\t\t// int MPI_Get_count(const MPI_Status *status, MPI_Datatype datatype, int *count);\n",
        "\t\t\t// Retorna o número de itens recebidos\n",
        "\t\t\tresult = MPI_Get_count(&status, MPI_CHAR, &count);\n",
        "\n",
        "\t\t\tprintf(\"%d @ %s recebeu %d itens do processo %d (%d): %s\\n\",\n",
        "          rank, processor_name, count, status.MPI_SOURCE, status.MPI_TAG, rx_buf);\n",
        "\t\t}\n",
        "\n",
        "\t} else {   // worker nodes\n",
        "\n",
        "\t\t// todos recebem de rank 0\n",
        "\t\t// int MPI_Recv(void* buf,int count,MPI_Datatype datatype,\n",
        "\t\t//              int source, int tag,MPI_Comm comm,MPI_Status *status);\n",
        "\t\tMPI_Recv(&pid_0,1,MPI_INT,0,msgtag,MPI_COMM_WORLD,&status);\n",
        "\t\t// printf(\"%s recebeu: %d\\n\",processor_name,pid_0);\n",
        "\n",
        "\t\t// Ranks != 0 enviam msg para rank 0\n",
        "\t\tsprintf(tx_buf,\"%s: rank=%d, pid 0=%d\",processor_name,rank,pid_0);\n",
        "\n",
        "\t\t// int MPI_Send(void *buf, int count, MPI_Datatype dtype, int dest,\n",
        "\t\t//              int tag, MPI_Comm comm)\n",
        "\t\tmsgtag=pid;\n",
        "\t\tMPI_Send(tx_buf,strlen(tx_buf)+1,MPI_CHAR,0,msgtag,MPI_COMM_WORLD);\n",
        "\t\t// MPI_Send(&pid,1,MPI_INT,0,msgtag,MPI_COMM_WORLD);\n",
        "\n",
        "\t}\n",
        "\n",
        "\tMPI_Finalize();\n",
        "\n",
        "\treturn(0);\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting sr-any.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KjHhDG9Tj-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ff2fd37-f8cb-4147-e480-5d66261281f6"
      },
      "source": [
        "!mpicc -Wall sr-any.c -o sr-any && mpirun --allow-run-as-root -n 4 -host localhost:4 sr-any"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 @ ee13bfcc7386 recebeu 33 itens do processo 3 (4871): ee13bfcc7386: rank=3, pid 0=4866\n",
            "0 @ ee13bfcc7386 recebeu 33 itens do processo 1 (4867): ee13bfcc7386: rank=1, pid 0=4866\n",
            "0 @ ee13bfcc7386 recebeu 33 itens do processo 2 (4868): ee13bfcc7386: rank=2, pid 0=4866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ_6GIUSm0aT"
      },
      "source": [
        "# MPI_Probe\n",
        "\n",
        "(baseado em https://mpitutorial.com/tutorials/dynamic-receiving-with-mpi-probe-and-mpi-status/)\n",
        "\n",
        "\n",
        "Em algumas aplicações distribuídas, além de o nó receptor não saber previamente o emissor das mensagens que irá receber, o que o obriga a usar a origem MPI_ANY_SOURCE, é comum que mesmo o tamanho das mensagens não seja conhecido.\n",
        "\n",
        "Neste caso, é possível alocar um *buffer* com o tamanho da maior mensagem prevista.\n",
        "\n",
        "Entretanto, como as mensagens transmitidas via *socket* pela implementação MPI são armazenadas previamente em *buffers* locais, é possível à aplicação saber previamente o tamanho de uma messagem já recebida pela API, antes chamar a operação MPI_Recv.\n",
        "\n",
        "Essa consulta pode ser feita com a chamada MPI_Probe:\n",
        "```c\n",
        "MPI_Probe(int source, int tag, MPI_Comm comm, MPI_Status* status)\n",
        "```\n",
        "De maneira equivalente a MPI_Recv, MPI_Probe bloqueia à espera do recebimento de uma mensagem específica pela implementação da API, sem que o conteúdo seja copiado para algum endereço especificado pela aplicação, contudo.\n",
        "\n",
        "Quando a chamada retorna, a estrutura *status* pode ser usada com a função MPI_Get_count para saber o número de itens recebidos. É possível então alocar um buffer de tamanho apropriado para o recebimento.\n",
        "\n",
        "```c\n",
        "int number_amount;\n",
        "\n",
        "if (world_rank == 0) {\n",
        "    const int MAX_NUMBERS = 100;\n",
        "    int numbers[MAX_NUMBERS];\n",
        "\n",
        "    // Pick a random amount of integers to send to process one\n",
        "    srand(time(NULL));\n",
        "    number_amount = (rand() / (float)RAND_MAX) * MAX_NUMBERS;\n",
        "\n",
        "    // Send the random amount of integers to process one\n",
        "    MPI_Send(numbers, number_amount, MPI_INT, 1, 0, MPI_COMM_WORLD);\n",
        "    printf(\"0 sent %d numbers to 1\\n\", number_amount);\n",
        "\n",
        "} else if (world_rank == 1) {\n",
        "    MPI_Status status;\n",
        "    // Probe for an incoming message from process zero\n",
        "    MPI_Probe(0, 0, MPI_COMM_WORLD, &status);\n",
        "\n",
        "    // When probe returns, the status object has the size and other\n",
        "    // attributes of the incoming message. Get the message size\n",
        "    MPI_Get_count(&status, MPI_INT, &number_amount);\n",
        "\n",
        "    // Allocate a buffer to hold the incoming numbers\n",
        "    int* number_buf = (int*)malloc(sizeof(int) * number_amount);\n",
        "\n",
        "    // Now receive the message with the allocated buffer\n",
        "    MPI_Recv(number_buf,number_amount,MPI_INT,0,0,MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
        "    printf(\"1 dynamically received %d numbers from 0.\\n\", number_amount);\n",
        "    free(number_buf);\n",
        "}\n",
        "```\n",
        "\n",
        "Se a locação e a liberação de memória forem constantes no código, contudo, talvez valha a pena usar um buffer de tamanho máximo :-)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJCyZ2twdp-G",
        "outputId": "1abfed24-4a5b-43bb-a163-97585df006bc"
      },
      "source": [
        "%%writefile sr-any.c\n",
        "\n",
        "#include <sys/types.h>\n",
        "#include <unistd.h>\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "#include <unistd.h>\n",
        "#include <stdlib.h>\n",
        "#include <mpi.h>\n",
        "\n",
        "#define LEN 256\n",
        "\n",
        "int\n",
        "main( int argc, char *argv[])\n",
        "{\n",
        "\tint i, rank, result, numtasks, namelen, msgtag;\n",
        "  double local_time, remote_time, mean_time, time_diff;\n",
        "\tchar processor_name[MPI_MAX_PROCESSOR_NAME];\n",
        "\n",
        "\tMPI_Status status;\n",
        "\n",
        "\tresult = MPI_Init(&argc,&argv);\n",
        "\n",
        "\tif (result != MPI_SUCCESS) {\n",
        "\t\tprintf (\"Erro iniciando programa MPI.\\n\");\n",
        "\t\tMPI_Abort(MPI_COMM_WORLD, result);\n",
        "\t}\n",
        "\n",
        "\t// Determina número de processos em execução na aplicação\n",
        "\tMPI_Comm_size(MPI_COMM_WORLD,&numtasks);\n",
        "\n",
        "\t// Determina ranking desse processo no grupo\n",
        "\tMPI_Comm_rank(MPI_COMM_WORLD,&rank);\n",
        "\n",
        "\t// Determina nome do host local\n",
        "\tMPI_Get_processor_name(processor_name,&namelen);\n",
        "\n",
        "\tmsgtag=1;\n",
        "\n",
        "\tif(rank==0) { // master node\n",
        "\n",
        "\t\t// rank 0 envia o instante local para todos os demais: 1..N-1\n",
        "\t\tfor(i=1; i < numtasks; i++) {\n",
        "\n",
        "      // determina instante atual\n",
        "      local_time = MPI_Wtime();\n",
        "\n",
        "\t\t\t// int MPI_Send(void *buf, int count, MPI_Datatype dtype, int dest, int tag, MPI_Comm comm)\n",
        "\t\t\tMPI_Send(&local_time, 1, MPI_DOUBLE, i, msgtag, MPI_COMM_WORLD);\n",
        "\n",
        "\t\t\t// printf(\"%s enviou: %f\\n\",processor_name, local_time);\n",
        "\t\t}\n",
        "\n",
        "\t\t// rank 0 aguarda resposta individual de cada um dos demais nós: 1..N-1\n",
        "\n",
        "    mean_time = 0.0;\n",
        "\n",
        "\t\tfor(i=1; i < numtasks;i++) {\n",
        "\n",
        "\t\t\t// rank 0 recebe dos demais (MPI_Comm_size -1)\n",
        "\n",
        "\t\t\t// Uso de MPI_ANY_SOURCE e MPI_ANY_TAG: não se sabe a ordem de envio\n",
        "\t\t\t// int MPI_Recv(void *buf, int count, MPI_Datatype dtype,\n",
        "\t\t\t//              int src, int tag, MPI_Comm comm, MPI_Status *stat)\n",
        "\t\t\tMPI_Recv(&remote_time,1,MPI_DOUBLE, MPI_ANY_SOURCE,MPI_ANY_TAG,MPI_COMM_WORLD,&status);\n",
        "\n",
        "      mean_time += remote_time;\n",
        "\n",
        "\t\t\tprintf(\"%d @ %s recebeu time_diff do processo %d: %f\\n\",\n",
        "          rank, processor_name, status.MPI_SOURCE, remote_time);\n",
        "\t\t}\n",
        "    mean_time /= numtasks;\n",
        "\n",
        "    printf(\"Atraso médio de propagaçao: %f\\n\", mean_time);\n",
        "\n",
        "\n",
        "\t} else {   // worker nodes\n",
        "\n",
        "\t\t// todos recebem de rank 0\n",
        "\t\t// int MPI_Recv(void* buf,int count,MPI_Datatype datatype,\n",
        "\t\t//              int source, int tag,MPI_Comm comm,MPI_Status *status);\n",
        "\t\tMPI_Recv(&remote_time,1,MPI_DOUBLE,0,msgtag,MPI_COMM_WORLD,&status);\n",
        "\n",
        "\t\t// printf(\"%s recebeu: %f\\n\",processor_name,remote_time);\n",
        "\n",
        "    local_time = MPI_Wtime();\n",
        "    time_diff = local_time - remote_time;\n",
        "\n",
        "    // dorme um pouquinho, para gerar envio em ordem aleatória...\n",
        "    usleep(rand()%100);\n",
        "\n",
        "\t\t// int MPI_Send(void *buf, int count, MPI_Datatype dtype, int dest,\n",
        "\t\t//              int tag, MPI_Comm comm)\n",
        "\t\tMPI_Send(&time_diff, 1, MPI_DOUBLE,0,msgtag,MPI_COMM_WORLD);\n",
        "\t}\n",
        "\n",
        "\tMPI_Finalize();\n",
        "\n",
        "\treturn(0);\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting sr-any.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEQcY60C5WRE"
      },
      "source": [
        "# Envio não bloqueante (imediato)\n",
        "\n",
        "O exemplo a seguir ilustra a comunicação não bloqueante com as primitivas [MPI_Isnd](https://www.open-mpi.org/doc/v4.0/man3/MPI_Isend.3.php) e [MPI_Irecv](https://www.open-mpi.org/doc/v4.0/man3/MPI_Irecv.3.php). Essas chamadas liberam a biblioteca MPI para escrever nos *buffers* internos associados às transmissões. Posteriormente, é possível   bloquear o prosseguimento do programa até que as operações tenham sido concluídas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSRD029k5W4U",
        "outputId": "504e9d91-826b-4c4d-fc76-deb6e8578fe6"
      },
      "source": [
        "%%writefile isnd.c\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <unistd.h>\n",
        "\n",
        "#include \"mpi.h\"\n",
        "\n",
        "int\n",
        "main(int argc,char *argv[])\n",
        "{\n",
        "  int numtasks, rank, next, prev, buf[2], tag1=1, tag2=2;\n",
        "  MPI_Request reqs[4];   // required variable for non-blocking calls\n",
        "  MPI_Status stats[4];   // required variable for Waitall routine\n",
        "\n",
        "  char hostname[MPI_MAX_PROCESSOR_NAME];\n",
        "  int namelen;\n",
        "\n",
        "\n",
        "  MPI_Init(&argc,&argv);\n",
        "  MPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n",
        "  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "\n",
        "  // Determina nome do host local\n",
        "  MPI_Get_processor_name(hostname,&namelen);\n",
        "\n",
        "  // determina nós vizinhos à esquerda e à direita\n",
        "  prev = rank-1;\n",
        "  if (rank == 0)\n",
        "    prev = numtasks - 1;\n",
        "\n",
        "  next = rank+1;\n",
        "  if (rank == (numtasks - 1))\n",
        "    next = 0;\n",
        "\n",
        "  // int MPI_Irecv(void *buf, int count, MPI_Datatype datatype,\n",
        "  //               int source, int tag, MPI_Comm comm, MPI_Request *request);\n",
        "  //\n",
        "  // Nonblocking calls allocate a communication request object and associate it\n",
        "  // with the request handle (the argument request). The request can be used\n",
        "  // later to query the status of the communication or wait for its completion.\n",
        "  //\n",
        "  // A nonblocking receive call indicates that the system may start writing data\n",
        "  // into the receive buffer. The receiver should not access any part of the\n",
        "  // receive buffer after a nonblocking receive operation is called, until the\n",
        "  // receive completes.\n",
        "  //\n",
        "  // A receive request can be determined being completed by calling the MPI_Wait,\n",
        "  // MPI_Waitany, MPI_Test, or MPI_Testany with request returned by this function.\n",
        "\n",
        "  // post non-blocking receives and sends for neighbors\n",
        "  MPI_Irecv(&buf[0], 1, MPI_INT, prev, tag1, MPI_COMM_WORLD, &reqs[0]);\n",
        "  MPI_Irecv(&buf[1], 1, MPI_INT, next, tag2, MPI_COMM_WORLD, &reqs[1]);\n",
        "\n",
        "  // int MPI_Isend(const void* buf, int count, MPI_Datatype datatype, int dest,\n",
        "  //               int tag, MPI_Comm comm, MPI_Request *request);\n",
        "  //\n",
        "  // MPI_Isend starts a standard-mode, nonblocking send. Nonblocking calls\n",
        "  // allocate a communication request object and associate it with the request\n",
        "  // handle (the argument request). The request can be used later to query the\n",
        "  // status of the communication or wait for its completion.\n",
        "  //\n",
        "  // A nonblocking send call indicates that the system may start copying data\n",
        "  // out of the send buffer. The sender should not modify any part of the send\n",
        "  // buffer after a nonblocking send operation is called, until the send completes.\n",
        "  //\n",
        "  // A send request can be determined being completed by calling the MPI_Wait,\n",
        "  // MPI_Waitany, MPI_Test, or MPI_Testany with request returned by this function.\n",
        "\n",
        "  MPI_Isend(&rank, 1, MPI_INT, prev, tag2, MPI_COMM_WORLD, &reqs[2]);\n",
        "  MPI_Isend(&rank, 1, MPI_INT, next, tag1, MPI_COMM_WORLD, &reqs[3]);\n",
        "\n",
        "\n",
        "  // aqui, devereia haver algo últil a fazer, ao invés de parar à espera das mensagens...\n",
        "\n",
        "\n",
        "  // int MPI_Waitall(int count, MPI_Request array_of_requests[],\n",
        "  //                 MPI_Status *array_of_statuses)\n",
        "  //\n",
        "  // Blocks until all communication operations associated with active handles in\n",
        "  // the list complete, and returns the status of all these operations. Both\n",
        "  // arrays have the same number of valid entries. The ith entry in array_of_statuses\n",
        "  // is set to the return status of the ith operation. Requests that were created by\n",
        "  // nonblocking communication operations are deallocated, and the corresponding\n",
        "  // handles in the array are set to MPI_REQUEST_NULL.\n",
        "  //\n",
        "  // When one or more of the communications completed by a call to MPI_Waitall\n",
        "  // fail, it is desirable to return specific information on each communication.\n",
        "\n",
        "  MPI_Waitall(4, reqs, stats);\n",
        "\n",
        "  printf(\"%s (%d): buf[0]: %d, buf[1]: %d\\n\",hostname,rank,buf[0],buf[1]);\n",
        "\n",
        "  MPI_Finalize();\n",
        "\n",
        "  return(0);\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing isnd.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNJ14ALn5YKl",
        "outputId": "e4e85de0-914b-4193-9a1d-001369a6521a"
      },
      "source": [
        "!mpicc -Wall isnd.c -o isnd && mpirun --allow-run-as-root -n 4 -host localhost:4 isnd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ee13bfcc7386 (1): buf[0]: 0, buf[1]: 2\n",
            "ee13bfcc7386 (3): buf[0]: 2, buf[1]: 0\n",
            "ee13bfcc7386 (2): buf[0]: 1, buf[1]: 3\n",
            "ee13bfcc7386 (0): buf[0]: 3, buf[1]: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4Otl3Lk-3cE"
      },
      "source": [
        "Ainda sobre o exemplo anterior, já que cada processo realiza operações e transmissão e recebimento, seria possível ainda substituir essas 2 operações por uma uma chamada a MPI_Sendrecv:\n",
        "\n",
        "```c\n",
        "int MPI_Sendrecv(const void *sendbuf, int sendcount, MPI_Datatype sendtype,\n",
        "    int dest, int sendtag, void *recvbuf, int recvcount,\n",
        "    MPI_Datatype recvtype, int source, int recvtag,\n",
        "    MPI_Comm comm, MPI_Status *status)\n",
        "```\n",
        "Esta chamada permite enviar e receber mensagens de nós distintos, com tags, tipos e tamanhos distintos também.\n",
        "\n",
        "Bem, há várias primitivas de comunicação com MPI. Espero que os exemplos tratados sirvam para entender os mecanismos utilizados e para explorar outras funcionalidades!"
      ]
    }
  ]
}