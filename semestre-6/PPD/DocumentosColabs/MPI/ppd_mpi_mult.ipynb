{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu7CnSM6WTQa"
      },
      "source": [
        "# PPD: Programação Paralela e Distribuída\n",
        "\n",
        "Hélio Crestana Guardia - DC / UFSCar - 2023\n",
        "\n",
        "**Programa**: multiplicação de matrizes\n",
        "\n",
        "$AB_{ij} = ∑_{k = 1}^{m}(A_{ik}*B_{kj})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoUr5VSI1a3t"
      },
      "source": [
        "De que maneira o programa da multiplicação de matrizes poderia ser paralelizado com MPI?\n",
        "\n",
        "O programa a seguir apresenta o esqueleto da uma solução para este problema. Como estamos tratando de computação paralela usando computadores distintos, sem áreas de memória compartilhada para as comunicações, precisamos tratar explicitamente de toda comunicação e sincronização entre os processos executando nos diversos nós.\n",
        "\n",
        "O problema começa com a decisão de como será feito o particionamento e de quais dados precisam ser passados a quais nós para as manipulações.\n",
        "\n",
        "Algumas questões a considerar neste problema:\n",
        "\n",
        "* Quem lê os dados das matrizes ou gera os valores dos elementos? rank 0?\n",
        "* Como dividir os cálculos entre os diversos processos? Cada um calcula um grupo de linhas da matriz C?\n",
        "* Como informar a cada processo quantas (e quais?) linhas (ou conjuntos de elementos) ele irá calcular?\n",
        "* Supondo a divisão do trabalho com o cálculo de linhas, o que o nó de rank 0 precisa enviar para cada nó? Matriz B inteira e apenas as linhas específicas de A, ou a matriz A é enviada inteira a todos?\n",
        "* E possível usar MPI_Scatter para enviar a matriz A aos demais processos? Será que se o número de elementos da matriz A não for múltiplo do número de processos, a operação de Scatter poderia gerar erros na distribuição das linhas?\n",
        "* É possível usar MPI_Gather para receber os dados da matriz C calculados pelos nós?\n",
        "* É possível usar MPI_Bcast para enviar a todos? O que seria enviado desta forma?\n",
        "* O nó de rank 0 deve realizar cálculos ou apenas atuar como coordenador?\n",
        "* Como receber os resultados (linhas de C) dos nós? As mensagens podem chegar fora de ordem?\n",
        "* É necessária alguma sincronização?\n",
        "\n",
        "Como o modelo de execução de MPI é comumente SPMD, o mesmo código é executado por todos os processos. Como se vê no programa a seguir, contudo, é possível diferenciar atividades dentro do mesmo código, comumente em função dos *ranks* dos processos, para que processos distintos realizem operações distintas.\n",
        "\n",
        "Usando o Colab mesmo, é possível editar essa versão do programa e executar aqui.\n",
        "\n",
        "O exemplo de execução apresentado depois do código realiza a execução e a medição do tempo de execução."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K1uIiQ8hDTDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kD9l8frY6yA0",
        "outputId": "9b3ca422-1480-466a-8db0-c9cefd751fc8"
      },
      "source": [
        "%%writefile mult.c\n",
        "\n",
        "/*\n",
        "** Universidade Federal de Sao Carlos\n",
        "** PPD: Programação Paralela e Distribuída\n",
        "** Hélio Crestana Guardia\n",
        "*/\n",
        "\n",
        "/*\n",
        "** Programa : multiplicacao de matrizes\n",
        "** Objetivo: paralelizacao com MPI\n",
        "*/\n",
        "\n",
        "#include <math.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "#include <stdio.h>\n",
        "#include <unistd.h>\n",
        "#include <time.h>\n",
        "#include \"mpi.h\"\n",
        "\n",
        "float *A, *B, *C;\n",
        "\n",
        "// Informações a serem enviadas pelo rank 0 aos demais processos\n",
        "struct info {\n",
        "  int lin_a;    // número de linhas da matriz A\n",
        "  int col_a;    // número de colunas da matriz A (= lin_b)\n",
        "  int lin_b;    // número de linhas da matriz B (= col_a)\n",
        "  int col_b;    // número de colunas da matriz B\n",
        "  int inic;     // número da linha inicial que o processo irá calcular\n",
        "  int numlin;   // número de linhas que serão calculadas pelo processo\n",
        "} s_info;\n",
        "\n",
        "int\n",
        "main(int argc, char *argv[])\n",
        "{\n",
        "\tint lin_a,col_a,lin_b,col_b,lin_c,col_c;\n",
        "\tint i,j,k, t;\n",
        "\tint numtasks, rank;\n",
        "\tint result, sum;\n",
        "\tint numlin, resto;\n",
        "\n",
        "\t// Todos os processos iniciam a biblioteca e determinam seus ranks na aplicação\n",
        "\n",
        "\tresult = MPI_Init(&argc,&argv);\n",
        "\tif (result != MPI_SUCCESS) {\n",
        "\t\tfprintf(stderr,\"Erro iniciando MPI: %d\\n\",result);\n",
        "\t\tMPI_Abort(MPI_COMM_WORLD, result);\n",
        "\t}\n",
        "\tMPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n",
        "\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "\n",
        "\n",
        "/*\n",
        "  Atividades do processo com rank 0:\n",
        "  ---------------------------------\n",
        "  - determinar dimensões das matrizes\n",
        "  - alocar espaço e carregar os dados das matrizes na memória local\n",
        "  - determinar como será a divisão do trabalho entre os processos\n",
        "  - enviar a cada processo as informações sobre as matrizes e sobre os cálculos que este irá realizar\n",
        "  - enviar a cada processo as informações das matrizes pertinentes para os cálculos\n",
        "  - receber resultados dos cálculos\n",
        "\n",
        "  Atividades dos processos com rank > 0:\n",
        "  ---------------------------------\n",
        "  - Aguardar (e receber) informações sobre as matrizes e a divisão do trabalho\n",
        "\t- Alocar espaço em memória para armazenar os dados das matrizes que irão receber\n",
        "  - Receber os dados pertinentes das matrizes, posicionando-os em memória para os cálculos\n",
        "  - Realizar os cálculos locais\n",
        "  - Enviar os valores processados de volta ao processo de rank 0\n",
        "*/\n",
        "\n",
        "\tif(rank==0) {\n",
        "\n",
        "    /********** Atividades do processo com rank 0 *************************/\n",
        "\n",
        "    // Nó com rank 0 faz a leitura das dimensõe das matrizes\n",
        "\n",
        "\t\tsetbuf(stdout,NULL); // para forçar a exibição imediata dos textos no terminal\n",
        "\n",
        "\t\tprintf(\"Linhas A: \");\n",
        "\t\tscanf(\"%d\",&lin_a);\n",
        "\t\tprintf(\"Colunas A / Linhas B: \");\n",
        "\t\tscanf(\"%d\",&col_a);\n",
        "\t\tlin_b = col_a;\n",
        "\t\tprintf(\"Colunas B: \");\n",
        "\t\tscanf(\"%d\",&col_b);\n",
        "\t\tprintf(\"\\n\");\n",
        "\t\tlin_c = lin_a;\n",
        "\t\tcol_c = col_b;\n",
        "\n",
        "  \t// Nó rank 0 aloca espaço para as matrizes e as preenche de forma aleatória.\n",
        "    // Numa aplicação efetiva, provavelmente leria dados de arquivos\n",
        "\n",
        "\t\t// Alocacao dinâmica das matrizes, com linhas em sequência\n",
        "\t\tA=(float *)malloc(lin_a*col_a*sizeof(float));\n",
        "\t\tB=(float *)malloc(lin_b*col_b*sizeof(float));\n",
        "\t\tC=(float *)malloc(lin_c*col_c*sizeof(float));\n",
        "\n",
        "\t\t// Inicia gerador de números aleatórios. Comentar comando a seguir se quiser\n",
        "\t\t// gerar sempre os mesmos valores para uniformidade nos cálculos.\n",
        "\t\tsrandom(time(NULL));\n",
        "\n",
        "\t\tfor(i=0; i < lin_a * col_a; i++)\n",
        "\t\t\tA[i]=(float)rand() / (float)RAND_MAX;\n",
        "\n",
        "\t\tfor(i=0; i < lin_b * col_b; i++)\n",
        "\t\t\tB[i]=(float)rand() / (float)RAND_MAX;\n",
        "\n",
        "\t  // Envio das matrizes para os processos.\n",
        "    // O que enviar depende de como os cálculos serão divididos.\n",
        "    // Aqui, considerando divisão das linhas de C\n",
        "\n",
        "\t\t// preenche informações sobre as matrizes\n",
        "\t\ts_info.lin_a = lin_a;\n",
        "\t\ts_info.col_a = col_a;\n",
        "\t\ts_info.lin_b = lin_b;\n",
        "\t\ts_info.col_b = col_b;\n",
        "\n",
        "\t\t// informações sobre a linha inicial e o número de linhas dependem do rank\n",
        "\n",
        "\t\t// Determina quantas e quais linhas cada processo (rank) vai calcular\n",
        "\t\t// rank 0 não irá participar dos cálculos...\n",
        "\n",
        "\t\tnumlin = lin_a / (numtasks -1);\n",
        "\t\tresto = lin_a % (numtasks -1);\n",
        "\n",
        "\t\t// Determina linha inicial e número de linhas para cada processo rank > 0\n",
        "    // e lhe envia linhas apropriadas\n",
        "\n",
        "\t\tfor(t=1; t < numtasks; t++) {\n",
        "\n",
        "\t\t\t// Determina informações sobre linhas a calcular pelo processo\n",
        "\t\t\ts_info.numlin = numlin;\n",
        "\t\t\tif(t <= resto)\n",
        "\t\t\t\ts_info.numlin += 1;  // resto primeiros processos recebem 1 linha a mais\n",
        "\t\t\ts_info.inic = (t-1) * numlin;\n",
        "\t\t\tif(resto) {\n",
        "\t\t\t\tif(t<=resto)\n",
        "\t\t\t\t\ts_info.inic += t-1;    // resto primeiros processos recebem 1 linha a mais\n",
        "\t\t\t\telse\n",
        "\t\t\t\t\ts_info.inic += resto;   // resto primeiros processos recebem 1 linha a mais\n",
        "\t\t\t}\n",
        "      // Envia informações de controle para demais processos rank > 0\n",
        "\n",
        "\t\t\t// MPI_Send( &s_info, sizeof(s_info), MPI_INT, t, 1, MPI_COMM_WORLD);\n",
        "\t\t\tMPI_Send( &s_info, sizeof(s_info), MPI_CHAR, t, 1, MPI_COMM_WORLD);\n",
        "\n",
        "\t\t\t// Envia linhas de A aos processos rank = t (>0)\n",
        "\t\t\t// Enviar 1 linha de cada vez ou todas em sequência?\n",
        "      // Como linhas estão contíguas na memória, poderia usar MPI_Scatter?\n",
        "\t\t\t// Fazer o recebimento correspondende nos demais ranks\n",
        "\n",
        "\t\t\tfor (i=s_info.inic; i < s_info.inic + s_info.numlin; i++)\n",
        "\t\t\t\tMPI_Send( &A[i*lin_a], col_a, MPI_INT, t, 1, MPI_COMM_WORLD);\n",
        "    }\n",
        "\n",
        "  \t// Nessa estratégia de particionamento, todos precisam da matriz B inteira.\n",
        "\t \t// Como enviá-la, replicando ou em Bcast?\n",
        "\t  // Bcast da matriz inteira ou linha por linha?\n",
        "\t\t// Observar que operação de Bcast, coletiva, deve ser realizada por todos os processos\n",
        "\t  for (i=0; i < lin_b; i++)\n",
        "\t\t  MPI_Bcast (&B[i*col_b], col_b, MPI_INT, 0, MPI_COMM_WORLD);\n",
        "\n",
        "    // Recebe resultados finais. Rank 0 recebe linhas de C\n",
        "\t  // É possível receber as linhas de C fora de ordem?\n",
        "\n",
        "  \tfor(t=1; t < numtasks; t++) {\n",
        "\n",
        "\t  \t// Determina informações sobre linhas que foram calculadas por cada processo rank > 0\n",
        "\t\t \t// Poderia ter salvo essas infos, já calculadas no envio, num vetor de parâmetros...\n",
        "\n",
        "\t  \ts_info.numlin = numlin;\n",
        "\t  \tif(t <= resto)\n",
        "\t  \t\ts_info.numlin += 1;  // resto primeiros processos recebem 1 linha a mais\n",
        "\t  \ts_info.inic = (t-1) * numlin;\n",
        "\t  \tif(resto) {\n",
        "\t  \t\tif(t<=resto)\n",
        "\t  \t\t\ts_info.inic += t-1;    // resto primeiros processos recebem 1 linha a mais\n",
        "\t  \t\telse\n",
        "\t  \t\t\ts_info.inic += resto;   // resto primeiros processos recebem 1 linha a mais\n",
        "\t  \t}\n",
        "\n",
        "\t  \t// Recebe as linhas de C calculadas em cada processo rank > 0\n",
        "      // Podeira fazer o recebimento fora de ordem?\n",
        "      // Ideia: usar MPI_ANY_SOURCE... requer recebimento em buffer e cópia para\n",
        "      // posição efetiva...\n",
        "\t  \tfor (i=s_info.inic; i < s_info.inic + s_info.numlin; i++)\n",
        "\t  \t\tMPI_Recv( &C[i*col_c], col_c, MPI_INT, t, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
        "    }\n",
        "\n",
        "\t} else {\n",
        "\n",
        "    /********** Atividades dos processos com rank > 0 *************************/\n",
        "\n",
        "  \t// Recebimento das matrizes e parâmetros para os cálculos\n",
        "\n",
        "\t\t// Recebem informações sobre as matrizes\n",
        "\t\t// MPI_Recv (&s_info, sizeof(s_info), MPI_INT, 0, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
        "\t\tMPI_Recv (&s_info, sizeof(s_info), MPI_CHAR, 0, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
        "\n",
        "\n",
        "\t\tprintf(\"%d recebeu: A[%d,%d], B[%d,%d], inic: %d, numlin: %d\\n\",\n",
        "\t\t\trank, s_info.lin_a, s_info.col_a, s_info.lin_b, s_info.col_b, s_info.inic, s_info.numlin);\n",
        "\n",
        "\t\t// Alocacam espaços para as matrizes.\n",
        "\t\t// Alocam todo o espaço ou apenas para conter as linhas que irão manipular?\n",
        "\t\t// Matrizes A e C precisam de espaço apenas para as linhas que serão manipuladas pelo processo\n",
        "\n",
        "\t\t// Alocacao dinâmica das matrizes, com linhas em sequência\n",
        "\t\tB = (float *)malloc( s_info.lin_b * s_info.col_b * sizeof(float));\n",
        "\t\t// A = (float *)malloc( s_info.lin_a * s_info.col_a * sizeof(float));\n",
        "\t\tA = (float *)malloc( s_info.numlin * s_info.col_a * sizeof(float));\n",
        "\t\t// C = (float *)malloc( s_info.lin_a * s_info.col_b * sizeof(float));\n",
        "\t\tC = (float *)malloc( s_info.numlin * s_info.col_b * sizeof(float));\n",
        "\n",
        "\t\t// Recebe linhas de A (s_info.numlin), posicionando-as na matriz alocada\n",
        "\t\tfor (i=0; i < s_info.numlin; i++)\n",
        "\t\t\tMPI_Recv( &A[i*s_info.col_a], s_info.col_a, MPI_INT, 0, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
        "\n",
        "\t\t// Recebe as linhas de B enviadas em Bcast\n",
        "\t\tfor (i=0; i < s_info.lin_b; i++)\n",
        "\t\t\tMPI_Bcast (&B[i*s_info.col_b], s_info.col_b, MPI_INT, 0, MPI_COMM_WORLD);\n",
        "\n",
        "  \t// cálculo da multiplicacao, feito pelos processos de rank > 0\n",
        "\n",
        "\t\t// Cada processo calcula s_info.numlin linhas\n",
        "\n",
        "\t\tfor ( i=0; i < s_info.numlin; i++)\n",
        "\t\t\tfor ( j=0; j < s_info.col_b; j++) {\n",
        "\t\t\t\t// C[ i * s_info.col_b +j ] = 0;\n",
        "\t\t\t\tsum = 0;\n",
        "\t\t\t\tfor ( k=0; k < s_info.col_a; k++)\n",
        "\t\t\t\t\t// C[ i * s_info.col_b +j ] += A[ i * s_info.col_a +k ] * B[ k * s_info.col_b +j ];\n",
        "\t\t\t\t\tsum += A[ i * s_info.col_a +k ] * B[ k * s_info.col_b +j ];\n",
        "\t\t\t\tC[ i * s_info.col_b +j ] = sum;\n",
        "\t\t\t}\n",
        "\n",
        "\t\t// Envia linhas da matriz C calculadas localmente ao process rank 0\n",
        "\n",
        "\t\tfor (i=0; i < s_info.numlin; i++)\n",
        "\t\t\tMPI_Send( &C[i*s_info.lin_a], s_info.col_b, MPI_INT, 0, 1, MPI_COMM_WORLD);\n",
        "\t}\n",
        "\n",
        "/* É possível comparar os resultados calculados paralelamente com os resultados\n",
        " * calculados de forma sequencial?\n",
        " * Hum... se os dados forem float, a ordem das operações pode gerar resultados\n",
        " * diferentes...\n",
        " */\n",
        "\n",
        "  // Se quiser testar os resultados produzidos de forma paralela\n",
        "// #define DEBUG\n",
        "\n",
        "#ifdef DEBUG\n",
        "  if(rank==0) {\n",
        "   // Cálculo sequencial para comparações\n",
        "  \tfloat *AB=(float *)malloc(lin_c*col_c*sizeof(float));\n",
        "  \tfor(i=0; i < lin_c; i++)\n",
        "  \t\tfor(j=0; j < col_c; j++) {\n",
        "  \t\t\t// AB[i*col_c+j]=0;\n",
        "\t\t\t\tsum = 0;\n",
        "  \t\t\tfor(k=0; k < col_a; k++)\n",
        "  \t\t\t\t// AB[i*col_c+j] = AB[i*col_c+j] + A[i*col_a+k] * B[k*col_b+j];\n",
        "  \t\t\t\tsum += A[i*col_a+k] * B[k*col_b+j];\n",
        "\t\t\t\tAB[i*col_c+j] = sum;\n",
        "  \t\t}\n",
        "   // Comparação da matriz calculada sequencialmente com a matriz calculada em paralelo\n",
        "  \tfor(i=0;i<lin_c;i++)\n",
        "  \t\tfor(j=0;j<col_c;j++)\n",
        "  \t\t\tif(C[i*col_c+j] != AB[i*col_c+j])\n",
        "  \t\t\t\tprintf(\"Erro em %d,%d\\n\",i,j);\n",
        "  }\n",
        "#endif\n",
        "\n",
        "\t// Todos os processos\n",
        "\n",
        "  // Libera áreas de memória\n",
        "  free(A); free(B); free(C);\n",
        "\n",
        "\tMPI_Finalize();\n",
        "\n",
        "\treturn(0);\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mult.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mtkY88VV3I_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cb7c3d2-4ed3-49fb-e250-8fe74ffb472f"
      },
      "source": [
        "%%writefile dados\n",
        "1024\n",
        "1024\n",
        "1024"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dados\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkMXaSafS7l2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bd307a4-e21c-467c-b000-adf11f989c1d"
      },
      "source": [
        "! if [ ! mult -nt mult.c ]; then mpicc mult.c -o mult -O3 ; fi\n",
        "# Execução com medição de tempo pelo comando time. Observe o tempo total decorrido (real)\n",
        "! time mpirun --allow-run-as-root -n 5 -host localhost:5 mult < dados"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kmult.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kmain\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kmult.c:81:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kscanf\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "   81 |   \u001b[01;35m\u001b[Kscanf(\"%d\",&lin_a)\u001b[m\u001b[K;\n",
            "      |   \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kmult.c:83:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kscanf\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "   83 |   \u001b[01;35m\u001b[Kscanf(\"%d\",&col_a)\u001b[m\u001b[K;\n",
            "      |   \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kmult.c:86:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kscanf\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "   86 |   \u001b[01;35m\u001b[Kscanf(\"%d\",&col_b)\u001b[m\u001b[K;\n",
            "      |   \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "Linhas A: Colunas A / Linhas B: Colunas B: \n",
            "1 recebeu: A[1024,1024], B[1024,1024], inic: 0, numlin: 256\n",
            "2 recebeu: A[1024,1024], B[1024,1024], inic: 256, numlin: 256\n",
            "3 recebeu: A[1024,1024], B[1024,1024], inic: 512, numlin: 256\n",
            "4 recebeu: A[1024,1024], B[1024,1024], inic: 768, numlin: 256\n",
            "\n",
            "real\t0m46.052s\n",
            "user\t1m28.451s\n",
            "sys\t0m0.238s\n"
          ]
        }
      ]
    }
  ]
}