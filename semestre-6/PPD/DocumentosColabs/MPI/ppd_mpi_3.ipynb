{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_vqFo-wFgJn"
      },
      "source": [
        "# PPD: MPI e programação com passagem de mensagem\n",
        "\n",
        "Hélio - DC/UFSCar - 2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqLeez1BbscH"
      },
      "source": [
        "# Comunicações coletivas\n",
        "\n",
        "Algumas formas de comunicação oferecidas por MPI são operações coletivas (*Collective communications*) e envolvem **todos** os processos pertencentes a um grupo (*communicator*) de uma só vez.\n",
        "\n",
        "Nessas operações, a mesma chamada deve ser emitida por todos os processos do grupo envolvido, sendo que um ou mais processos irão enviar dados e um ou mais irão recebê-los. Cabe ao programador, contudo, garantir que todos os processos do grupo emitam as chamadas apropriadas.\n",
        "\n",
        "Tipos de operações coletivas (*Collective Operations*):\n",
        "\n",
        "* ***Synchronization***: operações coletivas deste tipo servem para que processos esperem até que todos os membros do grupo atinjam um ponto de sincronização;\n",
        "* ***Data Movement***: permitem a transferência de dados, em diferentes modelos (*broadcast, scatter/gather, all to all, ...*);\n",
        "* ***Collective Computation (reductions)***: são operações coletivas em que um membro do grupo coleta os dados e realiza uma operação sobre eles, como nas operações de redução (min, max, add, multiply, etc.)\n",
        "\n",
        "Considerações:\n",
        "\n",
        "* Como envolvem todos os processos em um grupo (*communicator*) operações coletivas são bloqueantes e têm o efeito de sincronizar todos os processos do grupo.\n",
        "* Não há *tags* nas mensagens envolvidas nessas operações.\n",
        "* Para realizar operações coletivas usando apenas sub-conjuntos dos processos da aplicação, é preciso antes criar os sub-grupos (*communicators*) apropriados.\n",
        "* Apenas tipos pré-definidos por MPI podem ser usados nas comunicações coletivas.\n",
        "\n",
        "Primitivas para comunicações coletivas:\n",
        "\n",
        "* [MPI_Barrier](https://www.open-mpi.org/doc/v4.1/man3/MPI_Barrier.3.php) (comm): Cria uma barreira de sincronização para os processo de um grupo. Todos os processos que realizam a chamada são bloqueados até que o último processo do grupo faça esta mesma chamada.\n",
        "* **MPI_Bcast** (&buffer,count,datatype,root,comm): o processo cujo rank for especificado como *root* na chamada difunde (*broadcasts*) uma mensagem para todos os demais do grupo.\n",
        "* [MPI_Scatter](https://www.open-mpi.org/doc/v4.1/man3/MPI_Scatter.3.php) (&sendbuf,sendcnt,sendtype,&recvbuf,...... recvcnt,recvtype,root,comm): o processo identificado como *root* na chamada distribui partes distintas da mensagem para os demais processos do grupo.\n",
        "* **MPI_Scatterv** ( ):\n",
        "* [MPI_Gather](https://www.open-mpi.org/doc/v4.1/man3/MPI_Gather.3.php) (&sendbuf,sendcnt,sendtype,&recvbuf, ...... recvcount,recvtype,root,comm): de forma contrária ao que ocorre em *MPI_Scatter*, nesta chamada, o processo identificado como *root* coleta (*gathers*) mensagens distintas de todos os demais nós do grupo, concatenando-as em posições distintas do buffer de recepção, de acordo com o número lógico dos processos emissores.\n",
        "* **MPI_Gatherv**( ):\n",
        "* **MPI_Allgather** (&sendbuf,sendcount,sendtype,&recvbuf, ...... recvcount,recvtype,comm): nessa operaçao de concatenação, todos os processos do grupo obtêm os dados transmitidos por todos no grupo.\n",
        "\n",
        "Nas operações coletivas de **redução**, além de coletar os dados enviados por todos os processos do grupo, o processo *root* realiza uma operação de agregação sobre os dados.\n",
        "\n",
        "* [MPI_Reduce](https://www.open-mpi.org/doc/v4.1/man3/MPI_Reduce.3.php) (&sendbuf,&recvbuf,count,datatype,op,root,comm): processo *root* coleta todos os dados e aplica uma operação de redução.\n",
        "\n",
        "* **MPI_Allreduce** (const void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm): também realiza a redução, mas todos os processos do grupo têm o resultado.\n",
        "\n",
        "* **MPI_Reduce_scatter** (&sendbuf,&recvbuf,recvcount,datatype, ...... op,comm): *First does an element-wise reduction on a vector across all tasks in the group. Next, the result vector is split into disjoint segments and distributed across the tasks. This is equivalent to an MPI_Reduce followed by an MPI_Scatter operation*.\n",
        "\n",
        "* **MPI_Alltoall** (&sendbuf,sendcount,sendtype,&recvbuf, ...... recvcnt,recvtype,comm): *Each task in a group performs a scatter operation, sending a distinct message to all the tasks in the group in order by index*.\n",
        "\n",
        "* **[MPI_Scan](https://www.open-mpi.org/doc/v3.1/man3/MPI_Scan.3.php)**(&sendbuf,&recvbuf,count,datatype,op,comm): *Performs a scan operation with respect to a reduction operation across a task group*.\n",
        "\n",
        "```\n",
        "MPI Reduction   Operation                 C Data Types\n",
        "MPI_MAX         maximum                   integer, float\n",
        "MPI_MIN         minimum                   integer, float\n",
        "MPI_SUM         sum                       integer, float\n",
        "MPI_PROD        product                   integer, float\n",
        "MPI_LAND        logical AND               integer\n",
        "MPI_BAND        bit-wise AND              integer\n",
        "MPI_LOR         logical OR                integer\n",
        "MPI_BOR         bit-wise OR               integer, MPI_BYTE\n",
        "MPI_LXOR        logical XOR               integer\n",
        "MPI_BXOR        bit-wise XOR              integer, MPI_BYTE\n",
        "MPI_MAXLOC      max value and location    float, double and long double\n",
        "MPI_MINLOC      min value and location    float, double and long double\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb9WsW6XVBeC"
      },
      "source": [
        "# Broadcast\n",
        "\n",
        "Ao criar aplicações que se comunicam via MPI, independentemente da tecnologia da rede física e dos protocolos usados na interligação dos computadores,  não é preciso preocupar-se com detalhes das transmissões. Não é preciso ficar cuidando de quais são os processos que compõem a aplicação, de quais são os endereços IP dos computadores em que esses processos estão sendo executados, dos números de porta utilizados, de detalhes de *buffers* e retransmissões. Tudo isso é feito pela implementação MPI.\n",
        "\n",
        " Por exemplo, nos casos em que é preciso **enviar a mesma informação** para **todos** os processos da aplicação (que estão no mesmo MPI_COMM_WORLD), há 2 formas: replicando envios individuais, ou usando [MPI_Bcast](https://www.open-mpi.org/doc/v4.1/man3/MPI_Bcast.3.php).\n",
        "\n",
        "```c\n",
        "1. // Broadcast com envios individuais\n",
        "  int root = 0;\n",
        "  ...\n",
        "  MPI_Comm_size(MPI_COMM_WORLD,&num_tasks);\n",
        "\n",
        "  if (rank == root)\n",
        "    for (i = 1; i < num_taks; i++) {\n",
        "      // int MPI_Send(void *buf, int count, MPI_Datatype dtype, int dest,\n",
        "      //              int tag, MPI_Comm comm)\n",
        "      MPI_Send(tx_buf, count, data_type, i, tag, MPI_COMM_WORLD);\n",
        "  else\n",
        "    // int MPI_Recv(void *buf, int count, MPI_Datatype dtype, int src,\n",
        "    //              int tag, MPI_Comm comm, MPI_Status *stat)\n",
        "    MPI_Recv(rx_buf, count, data_type, root, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
        "```\n",
        "No caso 1, a aplicação identificou quantos processos estão sendo usados nesta execução e realizou envios individuais, replicando a mesma mensagem para todos. Nas transmissões, o índice do comando de iteração (***i***) foi usado para identificar o destino de cada mensagem, variando de ***1 a num_taks -1***.\n",
        "\n",
        "Todos os demais processos, com ***rank*** **> 0**, executaram uma operação de recebimento correspondente às mensagens lhes enviadas pelo proceso de *rank* **0** .\n",
        "\n",
        "<br>\n",
        "\n",
        "A segunda forma de enviar um mesmo conteúdo para todos os processos de um grupo é o uso da operação coletiva de ***broadcast***, ilustrado a seguir.\n",
        "\n",
        "```c\n",
        "2.  // TODOS os processos do grupo (MPI_COMM_WORLD, neste caso),\n",
        "    // independentemente de seus ranks, têm que fazer a chamada.\n",
        "\n",
        "    // O processo root tem os dados no buf.\n",
        "    // Nos demais, buf é o endereço onde os dados recebidos serão copiados.\n",
        "\n",
        "    // int MPI_Bcast(void *buffer, int count, MPI_Datatype datatype, int root, MPI_Comm comm)\n",
        "    MPI_Bcast( buf, count, data_type, root, MPI_COMM_WORLD);\n",
        "```\n",
        "No caso 2, a aplicação realiza a difusão do conteúdo do *buffer* usando uma única chamada de *broadcast*. Nesta operação, **um** dos processos do grupo vai fazer a **transmissão** e **todos os demais** membros deste grupo vão **recebê-la**.\n",
        "\n",
        "Se o grupo especificado na chamada for MPI_COMM_WOLRD, isso significa que todos os processos da aplicação vão ter que executar a mesma chamada.\n",
        "\n",
        "Nesta chamada, o processo emissor é identificado pelo ***rank*** informado no parâmetro ***root***.  \n",
        "\n",
        "<br>\n",
        "\n",
        "Como todos os processos do grupo realizam a mesma chamada, é preciso atentar para o endereço passado no parâmetro *buffer*. Para o nó *root*, que é o emissor, o *buffer* aponta para o endereço de onde os dados a serem transmitidos estão posicionados na memória deste processo. Para os demais processos, passa-se o endereço da posição de memória **onde eles serão copiados**.\n",
        "\n",
        "Se o modelo de execução SMPD for usado na ativação do programa MPI, em que o mesmo código é executado em todos os processos, é preciso diferenciar os papeis que os processos com os diferentes *ranks* irão realizar.\n",
        "\n",
        "<br>\n",
        "\n",
        "Será que há alguma vantagem em usar o caso 2? Isso depende da tecnologia de transmissão usada na rede (Ethernet, e.g.).\n",
        "\n",
        "É claro que, nesta forma de transmissão, cabe à **implementação MPI** fazer com que a mensagem seja efetivamente enviada a todos os processos do grupo. Se isso será feito com UDP ou TCP, usando *Ethernet broadcast*, ou mensagens replicadas, (felizmente :-) não é problema da aplicação.\n",
        "\n",
        "É claro, contudo, que nem sempre o melhor desempenho pode obtido.\n",
        "\n",
        "<br>\n",
        "\n",
        "Vejamos um exemplo a seguir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBoMQ9wRCOxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98ed5f3b-b30e-4e05-b110-ee8c7ed97d25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing bcast.c\n"
          ]
        }
      ],
      "source": [
        "%%writefile bcast.c\n",
        "\n",
        "#include <sys/types.h>\n",
        "#include <unistd.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "#include <mpi.h>\n",
        "\n",
        "#define N 10\n",
        "\n",
        "#define root 0\n",
        "\n",
        "int A[N], B[N], C[N];\n",
        "\n",
        "int\n",
        "main( int argc, char *argv[])\n",
        "{\n",
        "\tint i, rank, result, numtasks;\n",
        "\n",
        "\tresult = MPI_Init(&argc,&argv);\n",
        "\n",
        "\tif (result != MPI_SUCCESS) {\n",
        "\t\tprintf (\"Erro iniciando programa MPI.\\n\");\n",
        "\t\tMPI_Abort(MPI_COMM_WORLD, result);\n",
        "\t}\n",
        "\n",
        "\tMPI_Comm_size(MPI_COMM_WORLD,&numtasks);\n",
        "\tMPI_Comm_rank(MPI_COMM_WORLD,&rank);\n",
        "\n",
        "  // inicia semente do gerador de números aleatórios\n",
        "\t// srand(time(NULL));\n",
        "  srand(getpid());\n",
        "\n",
        "\t// Todos iniciam vetor A\n",
        "\tfor (i=0; i < N; i++)\n",
        "\t\tA[i]=rand() % 10;\n",
        "\n",
        "\t// Processo com rank 0 foi escolhido para gerar vetor B e propagá-lo aos demais\n",
        "\tif(rank==root)\n",
        "\t\tfor (i=0; i < N; i++)\n",
        "\t\t\tB[i]=rand() % 10;\n",
        "\n",
        "\t// Todos os processos participam do broacast.\n",
        "\t// E claro que essa chamada poderia estar em trechos distintos do codigo,\n",
        "  // usando buffers distintos.\n",
        "\t// O que importa é que todos facam a chamada e, exceto pelo root,\n",
        "  // tenham espaco para armazenar os dados.\n",
        "\t// O processo root, 0 neste caso, tem o vetor cujo conteúdo sejá enviado\n",
        "\n",
        "\tMPI_Bcast( B, N, MPI_INT, root, MPI_COMM_WORLD);\n",
        "\n",
        "  printf(\"(%d) B: \",rank);\n",
        "\tfor(i=0; i < N; i++)\n",
        "\t\tprintf(\"%d \",B[i]);\n",
        "\tprintf(\"\\n\"); fflush(stdout);\n",
        "\n",
        "\t// todos fazem a operacao usando o mesmo vetor B\n",
        "\tfor(i=0;i<N;i++)\n",
        "\t\tC[i]=A[i]+B[i];\n",
        "\t// ...\n",
        "\n",
        "\t// todos imprimem o vetor C.\n",
        "\t// Na ativação dos processos remotos com mpirun, stdout é redirecionado para\n",
        "\t// o nó de origem. Assim, todas as impressões vão aparecer no terminal.\n",
        "  printf(\"(%d) C: \",rank);\n",
        "\tfor(i=0; i < N; i++)\n",
        "\t\tprintf(\"%d \",C[i]);\n",
        "\tprintf(\"\\n\"); fflush(stdout);\n",
        "\n",
        "\tMPI_Finalize();\n",
        "\n",
        "\treturn(0);\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KjHhDG9Tj-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dd59ce1-713d-413b-a7c9-4930cf41963d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0) B: 8 9 9 3 6 5 1 3 4 3 \n",
            "(0) C: 13 18 12 12 6 10 10 7 9 5 \n",
            "(2) B: 8 9 9 3 6 5 1 3 4 3 \n",
            "(2) C: 17 13 16 10 7 9 6 12 13 4 \n",
            "(1) B: 8 9 9 3 6 5 1 3 4 3 \n",
            "(3) B: 8 9 9 3 6 5 1 3 4 3 \n",
            "(3) C: 17 14 15 6 12 14 9 5 9 11 \n",
            "(1) C: 8 17 11 6 13 11 10 4 9 6 \n"
          ]
        }
      ],
      "source": [
        "! if [ ! bcast -nt bcast.c ]; then mpicc -Wall bcast.c -o bcast; fi && mpirun --allow-run-as-root -n 4 -host localhost:4 bcast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhDygKaQe2pH"
      },
      "source": [
        "# Scatter & Gatther\n",
        "\n",
        "Dentre as operações MPI para comunicação coletiva, a primitiva [MPI_Scatter](https://www.open-mpi.org/doc/v4.1/man3/MPI_Scatter.3.php) causa o **envio particionado** de uma mensagem a **todos** os processos de um grupo (*communicator*).\n",
        "\n",
        "```c\n",
        "int MPI_Scatter(const void *sendbuf, int sendcount, MPI_Datatype sendtype,\n",
        "                void *recvbuf, int recvcount, MPI_Datatype recvtype,\n",
        "                int root, MPI_Comm comm);\n",
        "int MPI_Iscatter(const void *sendbuf, int sendcount, MPI_Datatype sendtype,\n",
        "                 void *recvbuf, int recvcount, MPI_Datatype recvtype,\n",
        "                 int root, MPI_Comm comm, MPI_Request *request);\n",
        "```\n",
        "O efeito dessa primitiva, que faz o oposto de MPI_Gather, é como se uma série de transmissões fossem feitas com MPI_Send, cada uma enviando uma parte sequencial do conteúdo do *buffer* de transmissão para um processo separado:\n",
        "\n",
        "```c\n",
        "MPI_Send(sendbuf + i * sendcount * sizeof(sendtype), sendcount, sendtype, i, ...);\n",
        "```\n",
        "com cada processo (incluse o emissor) executando uma operação de recebimento equivalente:\n",
        "```c\n",
        "MPI_Recv(recvbuf, recvcount, recvtype, i, ...).\n",
        "```\n",
        "\n",
        "O resultado é que cada processo do grupo vai **receber uma fração** da mensagem original, contendo um número de elementos **proporcional** à divisão do número total pelo número de processos do grupo. É como se a mensagem original fosse particionada e o **segemento** ***i*** fosse enviado ao processo ***i***.\n",
        "\n",
        "Examinando os campos desta primitiva, vê-se que há um *buffer* de envio e que há também indicação de um processo (***rank***) que será o ***root***, ou seja, o emissor nesta transmissão. No processamento da chamada, o *buffer* de envio (***sendbuf***) é ignorado em todos os nós cujo *rank* for diferente do ***root***.\n",
        "\n",
        "Outroa campos da primitiva incluem um ponteiro para o *buffer* de recebimento. Esse campo deverá ser válido **em todos** os processos (*ranks*), pois todos irão receber parte dos dados. O nó emissor também recebe sua fração dos dados.\n",
        "\n",
        "É claro que os campos ***root*** e ***comm*** devem conter os mesmos valores em todos\n",
        "os nós.\n",
        "\n",
        "\n",
        "\n",
        "# MPI_Gather\n",
        "\n",
        "[MPI_Gather](https://www.open-mpi.org/doc/v3.1/man3/MPI_Gather.3.php) tem o efeito contrário da operação *scatter*. Assim, o resultado é que o nó *root* vai receber uma mensagem de cada processo do grupo, inclusive uma sua, e vai colocar essas mensagens em sequência no buffer de recepção, em ordem correspondente aos identificadores dos nós no grupo.\n",
        "\n",
        "```c\n",
        "int MPI_Gather(const void *sendbuf, int sendcount, MPI_Datatype sendtype,\n",
        "    void *recvbuf, int recvcount, MPI_Datatype recvtype, int root,\n",
        "    MPI_Comm comm)\n",
        "```\n",
        "\n",
        "O programa exemplo a seguir ilustra o uso das operações de ***scatter*** e ***gather***."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5oKx93wVzip",
        "outputId": "62ef5e22-c7dd-4f7e-8b8f-c50fba88d56e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing scatter.c\n"
          ]
        }
      ],
      "source": [
        "%%writefile scatter.c\n",
        "\n",
        "#include <mpi.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define SIZE 10\n",
        "\n",
        "int\n",
        "main(int argc, char *argv[])\n",
        "{\n",
        "\tint numtasks, rank, i, j;\n",
        "\n",
        "\tint recbuf[SIZE];\n",
        "\tint *fulbuf;\n",
        "\n",
        "\tchar hostname[MPI_MAX_PROCESSOR_NAME];\n",
        "\tint namelen;\n",
        "\n",
        "\tMPI_Init(&argc,&argv);\n",
        "\n",
        "\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "\tMPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n",
        "\n",
        "\tMPI_Get_processor_name(hostname, &namelen);\n",
        "\n",
        "\t// root node\n",
        "\tif(rank == 0) {\n",
        "\n",
        "\t\tfulbuf = (int *)malloc(numtasks * SIZE * sizeof(int));\n",
        "\n",
        "\t\t// preenche vetor para envio\n",
        "\t\tfor(i=0; i < numtasks; i++)\n",
        "\t\t\tfor(j=0; j < SIZE; j++)\n",
        "\t\t\t\tfulbuf [i*SIZE + j] = i;\n",
        "\t}\n",
        "\t// Todos os processos, independentemente de seus ranks, devem emitir essa\n",
        "  // chamada de operaçào coletiva. Os dados do processo root é que serão\n",
        "  // distribuídos, de forma fracionada, entre todos.\n",
        "\n",
        "\t// int MPI_Scatter(const void *sendbuf, int sendcount, MPI_Datatype sendtype,\n",
        "\t//                 void *recvbuf, int recvcount, MPI_Datatype recvtype, int root,\n",
        "\t//                 MPI_Comm comm);\n",
        "\n",
        "\tMPI_Scatter(fulbuf, SIZE, MPI_INT, recbuf, SIZE, MPI_INT, 0, MPI_COMM_WORLD);\n",
        "\n",
        "\t// todos recebem, inclusive o processo root\n",
        "\tprintf(\"%s (%d): \", hostname, rank);\n",
        "\tfor(i=0;i<SIZE;i++) {\n",
        "\t\tprintf(\"%d \",recbuf[i]);\n",
        "\t\trecbuf[i] += 10 * rank;\n",
        "\t}\n",
        "\tprintf(\"\\n\"); fflush(stdout);\n",
        "\n",
        "\t// int MPI_Gather(const void *sendbuf, int sendcount, MPI_Datatype sendtype,\n",
        "\t//                void *recvbuf, int recvcount, MPI_Datatype recvtype, int root,\n",
        "\t//                MPI_Comm comm);\n",
        "\tMPI_Gather(recbuf, SIZE, MPI_INT, fulbuf, SIZE, MPI_INT, 0, MPI_COMM_WORLD);\n",
        "\n",
        "\tif(rank == 0) {\n",
        "\t\tprintf(\"\\n\");\n",
        "\t\tfor(i=0; i < numtasks; i++) {\n",
        "\t\t\tfor(j=0; j<SIZE; j++)\n",
        "\t\t\t\tprintf(\"%2d \",fulbuf[i*SIZE + j]);\n",
        "\t\t\tprintf(\"\\n\");\n",
        "\t\t}\n",
        "\t}\n",
        "\n",
        "\tMPI_Finalize();\n",
        "\n",
        "\treturn(0);\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aI7gBVC2Cuxx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "371fe85c-c2f5-4a87-c47c-5518a34082fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c109399169a1 (0): 0 0 0 0 0 0 0 0 0 0 \n",
            "c109399169a1 (2): 2 2 2 2 2 2 2 2 2 2 \n",
            "c109399169a1 (1): 1 1 1 1 1 1 1 1 1 1 \n",
            "c109399169a1 (3): 3 3 3 3 3 3 3 3 3 3 \n",
            "\n",
            " 0  0  0  0  0  0  0  0  0  0 \n",
            "11 11 11 11 11 11 11 11 11 11 \n",
            "22 22 22 22 22 22 22 22 22 22 \n",
            "33 33 33 33 33 33 33 33 33 33 \n"
          ]
        }
      ],
      "source": [
        "# Aqui, testamos com 4 processos no mesmo nó. Se houver mais nós, basta configurá-los em um hostfile, ou especificar em linha de comando\n",
        "!mpicc -Wall scatter.c -o scatter && mpirun --allow-run-as-root -n 4 -host localhost:4 scatter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5jTzfBreoZY"
      },
      "source": [
        "# MPI_Reduce\n",
        "\n",
        "As operações globais de redução ([MPI_Reduce](https://www.open-mpi.org/doc/v4.1/man3/MPI_Gather.3.php), MPI_Op_create, MPI_Op_free,  MPI_Allreduce, MPI_Reduce_scatter, MPI_Scan) realizam a **sumarização global** de algum valor provido por todos os processos de um grupo.\n",
        "\n",
        "A operação de sumarização pode ser alguma entre as operações pré-definidas (*), como a soma ou a multiplicação dos valores, a identificação do maior ou do menor valor, ou novas operações definidas pela aplicação .\n",
        "\n",
        "```\n",
        "(*) Name                Meaning\n",
        "   ---------           --------------------\n",
        "    MPI_MAX             maximum\n",
        "    MPI_MIN             minimum\n",
        "    MPI_SUM             sum\n",
        "    MPI_PROD            product\n",
        "    MPI_LAND            logical and\n",
        "    MPI_BAND            bit-wise and\n",
        "    MPI_LOR             logical or\n",
        "    MPI_BOR             bit-wise or\n",
        "    MPI_LXOR            logical xor\n",
        "    MPI_BXOR            bit-wise xor\n",
        "    MPI_MAXLOC          max value and location\n",
        "    MPI_MINLOC          min value and location\n",
        "```\n",
        "Enquanto a operação MPI_Reduce produz o valor da redução em apenas um processo definido, [MPI_Allreduce](https://www.open-mpi.org/doc/v4.1/man3/MPI_Allreduce.3.php) produz o mesmo resultado nos *buffers* de todos os processos do grupo.\n",
        "\n",
        "Já a chamada [MPI_Reduce_scatter](https://www.open-mpi.org/doc/v4.1/man3/MPI_Reduce_scatter.3.php) combina operação de redução com a distribuição, operando sobre um vetor de resultados combinados.\n",
        "\n",
        "```c\n",
        "int MPI_Reduce(const void *sendbuf, void *recvbuf, int count,\n",
        "               MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm comm);\n",
        "\n",
        "int MPI_Ireduce(const void *sendbuf, void *recvbuf, int count,\n",
        "                MPI_Datatype datatype, MPI_Op op, int root,\n",
        "                MPI_Comm comm, MPI_Request *request);\n",
        "```\n",
        "\n",
        "Entre os campos desta chamada, como era de se esperar, aparecm o endereço do ***buffer*** **de envio**, que vale para todos os nós, a indicação do processo que será o ***root*** desta operação, ou seja o ***rank*** daquele que vai receber todos os dados e sumarizá-los, a operação que será aplicada para a redução, além do tipo e a contagem dos dados.\n",
        "\n",
        "Há algumas primitivas diferetes para redução, como descritas a seguir.\n",
        "\n",
        "* **MPI_Reduce**(&sendbuf,&recvbuf,count,datatype,op,root,comm)\n",
        "Applies a reduction operation on all tasks in the group and places the result in one task.\n",
        "* **MPI_Allreduce**(const void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm).  \n",
        "Same as MPI_Reduce except that the result appears in the receive buffer of all the group members. Applies a reduction operation and places the result in all tasks in the group. This is equivalent to an MPI_Reduce followed by an MPI_Bcast.\n",
        "* **MPI_Reduce_scatter**(&sendbuf,&recvbuf,recvcount,datatype, ...... op,comm).\n",
        "First does an element-wise reduction on a vector across all tasks in the group. Next, the result vector is split into disjoint segments and distributed across the tasks. This is equivalent to an MPI_Reduce followed by an MPI_Scatter operation.\n",
        "* **MPI_Scan**(&sendbuf,&recvbuf,count,datatype,op,comm).\n",
        "Performs a scan operation with respect to a reduction operation across a task group.\n",
        "\n",
        "Resumidamente, a chamada MPI_Reduce, aplica uma operação de redução sobre dados recebidos de todos os processos de um grupo.\n",
        "\n",
        "O exemplo a seguir ilustra o uso da primitiva MPI_Reduce.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRq4RHq3etgh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f654d9c-e764-41e4-c986-113315ab6b90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reduce.c\n"
          ]
        }
      ],
      "source": [
        "%%writefile reduce.c\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <unistd.h>\n",
        "#include <time.h>\n",
        "#include \"mpi.h\"\n",
        "\n",
        "\n",
        "int main(int argc, char *argv[])\n",
        "{\n",
        "\tint rank, numtasks;\n",
        "  int num, redutor = 0;\n",
        "\n",
        "\tMPI_Init(&argc,&argv);\n",
        "\n",
        "\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "\tMPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n",
        "\n",
        "  // srand(time(NULL));\n",
        "  srand(getpid());\n",
        "  num = rand() % 100;\n",
        "\n",
        "\t// collective communications\n",
        "\n",
        "\t// int MPI_Reduce (void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype,\n",
        "\t//                 MPI_Op op, int root, MPI_Comm comm);\n",
        "\tMPI_Reduce(&num, &redutor, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n",
        "\n",
        "  // Apenas o processo de rank 0, neste caso, deve imprimir um valor consistente\n",
        "  // já que ele foi o 'root' desta operação\n",
        "\tprintf(\"Rank %d: num: %d  sum: %d\\n\", rank, num, redutor);\n",
        "\n",
        "\t// int MPI_Allreduce (void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype,\n",
        "\t//                    MPI_Op op, MPI_Comm comm )\n",
        "\tMPI_Allreduce(&num, &redutor, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);\n",
        "\n",
        "  // Todos os processos devem ter recebido uma cópia do valor gerado com a redução agora\n",
        "\tprintf(\"Rank %d: num: %d  max: %d\\n\", rank, num, redutor);\n",
        "\n",
        "\tMPI_Finalize();\n",
        "\n",
        "\treturn(0);\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_KNKHp-ew3W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e1f6588-85cb-4257-fb97-4e690c86fda9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank 2: num: 19  sum: 0\n",
            "Rank 3: num: 44  sum: 0\n",
            "Rank 1: num: 74  sum: 0\n",
            "Rank 0: num: 67  sum: 204\n",
            "Rank 0: num: 67  max: 74\n",
            "Rank 2: num: 19  max: 74\n",
            "Rank 1: num: 74  max: 74\n",
            "Rank 3: num: 44  max: 74\n"
          ]
        }
      ],
      "source": [
        "! if [ ! reduce -nt reduce.c ]; then mpicc -Wall reduce.c -o reduce; fi && mpirun --allow-run-as-root -n 4 -host localhost:4 reduce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb8edXRnOckg"
      },
      "source": [
        "Vejamos mais um exemplo de operação de redução.\n",
        "\n",
        "Como se pode ver nos parâmetros da primitiva MPI_Reduce, os *buffers* de envio e recebimento podem ser um vetor, já que os demais parâmetros indicam o tipo dos dados e o número de elementos. Assim, a operação de redução pode ser aplicada a cada um dos elementos deste vetor!\n",
        "\n",
        "Do ponto de vista de desempenho, contudo, talvez esse tipo de operação não seja muito eficiente. Embora a implementação da biblioteca MPI trate de todos os detalhes desta operação, é possível que uma solução mais eficiente seja obtida caso a redução seja feita localmente, por exemplo, usando algum algoritmo em *log_n* etapas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkA8KOQIjD9X",
        "outputId": "d609cd4c-a66f-4ce8-ad7f-956f718e3ff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing red-vet.c\n"
          ]
        }
      ],
      "source": [
        "%%writefile red-vet.c\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <unistd.h>\n",
        "#include <time.h>\n",
        "#include <mpi.h>\n",
        "#include <string.h>\n",
        "\n",
        "#define NELEM 10\n",
        "\n",
        "int main(int argc, char *argv[])\n",
        "{\n",
        "\tint i, rank, numtasks;\n",
        "  int num[NELEM], redutor[NELEM];\n",
        "\n",
        "\tMPI_Init(&argc,&argv);\n",
        "\n",
        "\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "\tMPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n",
        "0\n",
        "\t// int MPI_Allreduce (void *sendbuf, void *recvbuf0, int count, MPI_Datatype datatype,\n",
        "\t//                    MPI_Op op, MPI_Comm comm )\n",
        "\tMPI_Allreduce (num, redutor, NELEM, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n",
        "\n",
        "  // Todos devem ter os valores da redução agora\n",
        "  printf(\"%d: \", rank);\n",
        "\tfor (i=0; i < NELEM; i++)\n",
        "    printf(\"%d \", redutor[i]);\n",
        "  printf(\"\\n\");\n",
        "\n",
        "\tMPI_Finalize();\n",
        "\n",
        "\treturn(0);\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlllqJJFjOAo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4a456fd-2f14-468f-b850-66c94e90e579"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2: 6 2 6 1 9 0 3 9 3 9 \n",
            "2: 0 0 0 0 0 0 0 0 0 0 \n",
            "3: 2 4 1 1 1 0 9 8 1 3 \n",
            "3: 0 0 0 0 0 0 0 0 0 0 \n",
            "0: 3 8 2 1 6 7 1 1 3 0 \n",
            "1: 7 5 6 7 0 3 9 2 0 3 \n",
            "1: 0 0 0 0 0 0 0 0 0 0 \n",
            "0: 7 8 6 7 9 7 9 9 3 9 \n",
            "0: 18 19 15 10 16 10 22 20 7 15 \n",
            "1: 18 19 15 10 16 10 22 20 7 15 \n",
            "2: 18 19 15 10 16 10 22 20 7 15 \n",
            "3: 18 19 15 10 16 10 22 20 7 15 \n"
          ]
        }
      ],
      "source": [
        "! if [ ! red-vet -nt red-vet.c ]; then mpicc -Wall red-vet.c -o red-vet; fi && mpirun --allow-run-as-root -n 4 -host localhost:4 red-vet"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Gb9WsW6XVBeC",
        "nhDygKaQe2pH"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}